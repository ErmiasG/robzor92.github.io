{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1 { font-size: 0em; } Enterprise Data Feature Engineering Frameworks Pandas Flink Spark SQL Storage connectors JDBC BigQuery Object Store Snowflake RedShift Enterprise Feature Store Govern & Monitor Serve Share & Re-use Create .layer_02{pointer-events: none; } .round-frame{ pointer-events: initial; } Project Based Collaboration Write API Feature Groups External Feature Groups Read API Feature Views Training Dataset Feature Vectors Search, Versioning, Statistics, Code Provenance & Lineage Azure AWS Google Cloud On-premise Enterprise AI MLOps Experiments & Model Training Model Registry Model Deployment Operational ML Analytical ML BI Tools Vector DB OpenSearch Hopsworks is a data platform for ML with a Python-centric Feature Store and MLOps capabilities. Hopsworks is a modular platform. You can use it as a standalone Feature Store, you can use it manage, govern, and serve your models, and you can even use it to develop and operate feature pipelines and training pipelines. Hopsworks brings collaboration for ML teams, providing a secure, governed platform for developing, managing, and sharing ML assets - features, models, training data, batch scoring data, logs, and more. Python-Centric Feature Store # Hopsworks is widely used as a standalone Feature Store. Hopsworks breaks the monolithic model development pipeline into separate feature and training pipelines, enabling both feature reuse and better tested ML assets. You can develop features by building feature pipelines in any Python (or Spark or Flink) environment, either inside or outside Hopsworks. You can use the Python frameworks you are familiar with to build production feature pipelines. You can compute aggregations in Pandas, validate feature data with Great Expectations, reduce your data dimensionality with embeddings and PCA, test your feature logic and features end-to-end with PyTest, and transform your categorical and numerical features with Scikit-Learn, TensorFlow, and PyTorch. You can orchestrate your feature pipelines with your Python framework of choice, including Hopsworks' own Airflow support. The Widest Feature Store Capabilities # Hopsworks Feature Store also supports feature pipelines in PySpark, Spark, Flink, and SQL. Offline features can either be stored in Hopsworks, as Hudi tables on object storage, or in external data lakehouses (Snowflake, Databricks, Redshift, BigQuery, any JDBC-enabled platform) via External Feature Groups. Online features are served by RonDB , developed by Hopsworks as the lowest latency, highest throughput, highest availability data store for your features. MLOps on Hops # Hopsworks provides model serving capabilities through KServe, with additional support for feature/prediction logging to Kafka (also part of Hopsworks), and secure, low-latency model deployments via Istio. Hopsworks also has a Model Registry for KServe, with support for versioning both models and model assets (such as KServe transformers). Hopsworks also includes a vector database to provide similarity search capabilities for embeddings, based on OpenSearch . Project-based Multi-Tenancy and Team Collaboration # Hopsworks provides projects as a secure sandbox in which teams can collaborate and share ML assets. Hopsworks' unique multi-tenant project model even enables sensitive data to be stored in a shared cluster, while still providing fine-grained sharing capabilities for ML assets across project boundaries. Projects can be used to structure teams so that they have end-to-end responsibilty from raw data to managed features and models. Projects can also be used to create development, staging, and production environments for data teamss. All ML assets support versioning, lineage, and provenance provide all hopsworks users with a complete view of the MLOps life cycle, from feature engineering through model serving. Development and Operations # Hopsworks provides development tools for Data Science, including conda environments for Python, Jupyter notebooks, jobs, or even notebooks as jobs. You can build production pipelines with the bundled Airflow, and even run ML training pipelines with GPUs in notebooks on Airflow. You can train models on as many GPUs as are installed in a Hopsworks cluster and easily share them among users. You can also run Spark, Spark Streaming, or Flink programs on Hopsworks, with support for elastic workers in the cloud (add/remove workers dynamically). Available on any Platform # Hopsworks is available as a both managed platform in the cloud on AWS, Azure, and GCP, and can be installed on any Linux-based virtual machines (Ubuntu/Redhat compatible), even in air-gapped data centers. Hopsworks is also available as a serverless platform that manages and serves both your features and models. Join the community # Ask questions and give us feedback in the Hopsworks Community Follow us on Twitter Check out all our latest product releases Contribute # We are building the most complete and modular ML platform available in the market and we count on your support to continuously improve Hopsworks. Feel free to give us suggestions , report bugs and add features to our library anytime. Open-Source # Hopsworks is available under the AGPL-V3 license. In plain English this means that you are free to use Hopsworks and even build paid services on it, but if you modify the source code, you should also release back your changes and any systems built around it as AGPL-V3. We're the best at what we do, and we strive to keep the same standard for our community! Our many thanks to the contributors of Hopsworks.","title":"Home"},{"location":"#python-centric-feature-store","text":"Hopsworks is widely used as a standalone Feature Store. Hopsworks breaks the monolithic model development pipeline into separate feature and training pipelines, enabling both feature reuse and better tested ML assets. You can develop features by building feature pipelines in any Python (or Spark or Flink) environment, either inside or outside Hopsworks. You can use the Python frameworks you are familiar with to build production feature pipelines. You can compute aggregations in Pandas, validate feature data with Great Expectations, reduce your data dimensionality with embeddings and PCA, test your feature logic and features end-to-end with PyTest, and transform your categorical and numerical features with Scikit-Learn, TensorFlow, and PyTorch. You can orchestrate your feature pipelines with your Python framework of choice, including Hopsworks' own Airflow support.","title":"Python-Centric Feature Store"},{"location":"#the-widest-feature-store-capabilities","text":"Hopsworks Feature Store also supports feature pipelines in PySpark, Spark, Flink, and SQL. Offline features can either be stored in Hopsworks, as Hudi tables on object storage, or in external data lakehouses (Snowflake, Databricks, Redshift, BigQuery, any JDBC-enabled platform) via External Feature Groups. Online features are served by RonDB , developed by Hopsworks as the lowest latency, highest throughput, highest availability data store for your features.","title":"The Widest Feature Store Capabilities"},{"location":"#mlops-on-hops","text":"Hopsworks provides model serving capabilities through KServe, with additional support for feature/prediction logging to Kafka (also part of Hopsworks), and secure, low-latency model deployments via Istio. Hopsworks also has a Model Registry for KServe, with support for versioning both models and model assets (such as KServe transformers). Hopsworks also includes a vector database to provide similarity search capabilities for embeddings, based on OpenSearch .","title":"MLOps on Hops"},{"location":"#project-based-multi-tenancy-and-team-collaboration","text":"Hopsworks provides projects as a secure sandbox in which teams can collaborate and share ML assets. Hopsworks' unique multi-tenant project model even enables sensitive data to be stored in a shared cluster, while still providing fine-grained sharing capabilities for ML assets across project boundaries. Projects can be used to structure teams so that they have end-to-end responsibilty from raw data to managed features and models. Projects can also be used to create development, staging, and production environments for data teamss. All ML assets support versioning, lineage, and provenance provide all hopsworks users with a complete view of the MLOps life cycle, from feature engineering through model serving.","title":"Project-based Multi-Tenancy and Team Collaboration"},{"location":"#development-and-operations","text":"Hopsworks provides development tools for Data Science, including conda environments for Python, Jupyter notebooks, jobs, or even notebooks as jobs. You can build production pipelines with the bundled Airflow, and even run ML training pipelines with GPUs in notebooks on Airflow. You can train models on as many GPUs as are installed in a Hopsworks cluster and easily share them among users. You can also run Spark, Spark Streaming, or Flink programs on Hopsworks, with support for elastic workers in the cloud (add/remove workers dynamically).","title":"Development and Operations"},{"location":"#available-on-any-platform","text":"Hopsworks is available as a both managed platform in the cloud on AWS, Azure, and GCP, and can be installed on any Linux-based virtual machines (Ubuntu/Redhat compatible), even in air-gapped data centers. Hopsworks is also available as a serverless platform that manages and serves both your features and models.","title":"Available on any Platform"},{"location":"#join-the-community","text":"Ask questions and give us feedback in the Hopsworks Community Follow us on Twitter Check out all our latest product releases","title":"Join the community"},{"location":"#contribute","text":"We are building the most complete and modular ML platform available in the market and we count on your support to continuously improve Hopsworks. Feel free to give us suggestions , report bugs and add features to our library anytime.","title":"Contribute"},{"location":"#open-source","text":"Hopsworks is available under the AGPL-V3 license. In plain English this means that you are free to use Hopsworks and even build paid services on it, but if you modify the source code, you should also release back your changes and any systems built around it as AGPL-V3. We're the best at what we do, and we strive to keep the same standard for our community! Our many thanks to the contributors of Hopsworks.","title":"Open-Source"},{"location":"admin/","text":"Cluster Administration # Hopsworks has a cluster management page that allows you, the administrator, to perform management actions, monitor and control Hopsworks. To access the cluster management page you should log in into Hopsworks using your administrator account. In the top right corner, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.","title":"Introduction"},{"location":"admin/#cluster-administration","text":"Hopsworks has a cluster management page that allows you, the administrator, to perform management actions, monitor and control Hopsworks. To access the cluster management page you should log in into Hopsworks using your administrator account. In the top right corner, click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu.","title":"Cluster Administration"},{"location":"admin/alert/","text":"Configure Alerts # Alerts are sent from Hopsworks using Prometheus' Alert manager . In order to send alerts we first need to configure the Alert manager . To do that click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' Alerts tab you can configure the alert manager to send alerts via email, slack or pagerduty. Configure alerts 1. Email Alerts # To send alerts via email you need to configure an SMTP server. Click on the Configure button on the left side of the email row and fill out the form that pops up. Configure Email Alerts Default from : the address used as sender in the alert email. SMTP smarthost : the Simple Mail Transfer Protocol (SMTP) host through which emails are sent. Default hostname (optional) : hostname to identify to the SMTP server. Authentication method : how to authenticate to the SMTP server. CRAM-MD5, LOGIN or PLAIN. Optionally cluster wide Email alert receivers can be added in Default receiver emails . These receivers will be available to all users when they create event triggered alerts . 2. Slack Alerts # Alert can also be sent via Slack message. To be able to send Slack messages you first need to configure a Slack webhook. Click on the Configure button on the left side of the slack row and past in your Slack webhook in Webhook . Configure slack Alerts Optionally cluster wide Slack alert receivers can be added in Slack channel/user . These receivers will be available to all users when they create event triggered alerts . 3. Pagerduty # Pagerduty is another way you can send alerts from Hopsworks. Click on the Configure button on the left side of the pagerduty row and fill out the form that pops up. Configure Pagerduty Alerts Fill in Pagerduty URL: the URL to send API requests to. Optionally cluster wide Pagerduty alert receivers can be added in Service key/Routing key . By first choosing the PagerDuty integration type: global event routing (routing_key) : when using PagerDuty integration type Events API v2 . service (service_key) : when using PagerDuty integration type Prometheus . Then adding the Service key/Routing key of the receiver(s). PagerDuty provides documentation on how to integrate with Prometheus' Alert manager. Advanced configuration # If you are familiar with Prometheus' Alert manager you can also configure alerts by editing the yaml/json file directly. Advanced configuration Example: Adding the yaml snippet shown below in the global section of the alert manager configuration will have the same effect as creating the SMTP configuration as shown in section 1 above. global : smtp_smarthost : smtp.gmail.com:587 smtp_from : hopsworks@gmail.com smtp_auth_username : hopsworks@gmail.com smtp_auth_password : XXXXXXXXX smtp_auth_identity : hopsworks@gmail.com ... To test the alerts by creating triggers from Jobs and Feature group validations see Alerts .","title":"Configure Alerts"},{"location":"admin/alert/#configure-alerts","text":"Alerts are sent from Hopsworks using Prometheus' Alert manager . In order to send alerts we first need to configure the Alert manager . To do that click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' Alerts tab you can configure the alert manager to send alerts via email, slack or pagerduty. Configure alerts","title":"Configure Alerts"},{"location":"admin/alert/#1-email-alerts","text":"To send alerts via email you need to configure an SMTP server. Click on the Configure button on the left side of the email row and fill out the form that pops up. Configure Email Alerts Default from : the address used as sender in the alert email. SMTP smarthost : the Simple Mail Transfer Protocol (SMTP) host through which emails are sent. Default hostname (optional) : hostname to identify to the SMTP server. Authentication method : how to authenticate to the SMTP server. CRAM-MD5, LOGIN or PLAIN. Optionally cluster wide Email alert receivers can be added in Default receiver emails . These receivers will be available to all users when they create event triggered alerts .","title":"1. Email Alerts"},{"location":"admin/alert/#2-slack-alerts","text":"Alert can also be sent via Slack message. To be able to send Slack messages you first need to configure a Slack webhook. Click on the Configure button on the left side of the slack row and past in your Slack webhook in Webhook . Configure slack Alerts Optionally cluster wide Slack alert receivers can be added in Slack channel/user . These receivers will be available to all users when they create event triggered alerts .","title":"2. Slack Alerts"},{"location":"admin/alert/#3-pagerduty","text":"Pagerduty is another way you can send alerts from Hopsworks. Click on the Configure button on the left side of the pagerduty row and fill out the form that pops up. Configure Pagerduty Alerts Fill in Pagerduty URL: the URL to send API requests to. Optionally cluster wide Pagerduty alert receivers can be added in Service key/Routing key . By first choosing the PagerDuty integration type: global event routing (routing_key) : when using PagerDuty integration type Events API v2 . service (service_key) : when using PagerDuty integration type Prometheus . Then adding the Service key/Routing key of the receiver(s). PagerDuty provides documentation on how to integrate with Prometheus' Alert manager.","title":"3. Pagerduty"},{"location":"admin/alert/#advanced-configuration","text":"If you are familiar with Prometheus' Alert manager you can also configure alerts by editing the yaml/json file directly. Advanced configuration Example: Adding the yaml snippet shown below in the global section of the alert manager configuration will have the same effect as creating the SMTP configuration as shown in section 1 above. global : smtp_smarthost : smtp.gmail.com:587 smtp_from : hopsworks@gmail.com smtp_auth_username : hopsworks@gmail.com smtp_auth_password : XXXXXXXXX smtp_auth_identity : hopsworks@gmail.com ... To test the alerts by creating triggers from Jobs and Feature group validations see Alerts .","title":"Advanced configuration"},{"location":"admin/auth/","text":"Authentication Methods # To configure Authentication methods click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings Authentication tab you can configure how users authenticate. TOTP Two-factor Authentication : can be disabled , optional or mandatory . If set to mandatory all users are required to set up two-factor authentication when registering. Note If two-factor is set to mandatory on a cluster with preexisting users all users will need to go through lost device recovery step to enable two-factor. So consider setting it to optional first and allow users to enable it before setting it to mandatory. OAuth2 : if your organization already have an identity management system compatible with OpenID Connect (OIDC) you can configure Hopsworks to use your identity provider by enabling OAuth as shown in the figure below. After enabling OAuth you can register your identity provider by clicking on Add Identity Provider button. See Create client for details. LDAP/Kerberos : if your organization is using LDAP or Kerberos to manage users and services you can configure Hopsworks to use it as the user management system. You can enable LDAP/Kerberos by clicking on the checkbox, as shown in the figure below, and choosing LDAP or Kerberos. For more information on how to configure LDAP and Kerberos see Configure LDAP and Configure Kerberos . Setup Authentication Methods In the figure above we see a cluster with Two-factor authentication disabled, OAuth enabled with one registered identity provider and LDAP authentication enabled.","title":"Configure Authentication"},{"location":"admin/auth/#authentication-methods","text":"To configure Authentication methods click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings Authentication tab you can configure how users authenticate. TOTP Two-factor Authentication : can be disabled , optional or mandatory . If set to mandatory all users are required to set up two-factor authentication when registering. Note If two-factor is set to mandatory on a cluster with preexisting users all users will need to go through lost device recovery step to enable two-factor. So consider setting it to optional first and allow users to enable it before setting it to mandatory. OAuth2 : if your organization already have an identity management system compatible with OpenID Connect (OIDC) you can configure Hopsworks to use your identity provider by enabling OAuth as shown in the figure below. After enabling OAuth you can register your identity provider by clicking on Add Identity Provider button. See Create client for details. LDAP/Kerberos : if your organization is using LDAP or Kerberos to manage users and services you can configure Hopsworks to use it as the user management system. You can enable LDAP/Kerberos by clicking on the checkbox, as shown in the figure below, and choosing LDAP or Kerberos. For more information on how to configure LDAP and Kerberos see Configure LDAP and Configure Kerberos . Setup Authentication Methods In the figure above we see a cluster with Two-factor authentication disabled, OAuth enabled with one registered identity provider and LDAP authentication enabled.","title":"Authentication Methods"},{"location":"admin/dummy/","text":"","title":"Dummy"},{"location":"admin/installation/","text":"Installation notes #","title":"Installation notes"},{"location":"admin/installation/#installation-notes","text":"","title":"Installation notes"},{"location":"admin/project/","text":"Admin Project Guide #","title":"Admin Project Guide"},{"location":"admin/project/#admin-project-guide","text":"","title":"Admin Project Guide"},{"location":"admin/roleChaining/","text":"AWS IAM Role Chaining # Introduction # Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS: Prerequisites # Before you begin this guide you'll need the following: A Hopsworks cluster running on EC2. Step 1: Create an instance profile role # Create an instance profile role with policies that will allow it to assume all resource roles that we can assume from the Hopsworks cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles. Step 2: Create the resource roles # Create the resource roles and edit trust relationship and add policy document that will allow the instance profile to assume this role. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document. Step 3: Create mappings # Role chaining allows the instance profile to assume any role in the policy attached in step 1. To limit access to iam roles we can create a per-project mapping from the admin page in Hopsworks. Role Chaining Click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' IAM Role Chaining tab you can configure the mappings between projects and IAM roles. You can add mappings by entering the project name, which roles in that project can access the cloud role and the role ARN. Optionally you can set a role mapping as default by marking the default checkbox. The default roles can be changed from the project setting by a Data owner in that project. Create Role Chaining Any member of a project can then go to the Project Settings -> Assuming IAM Roles page to see which roles they can assume. Conclusion # In this guide you learned how to map AWS IAM roles to project roles in Hopsworks.","title":"IAM Role Chaining"},{"location":"admin/roleChaining/#aws-iam-role-chaining","text":"","title":"AWS IAM Role Chaining"},{"location":"admin/roleChaining/#introduction","text":"Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . To use Role chaining, you need to first setup IAM roles in AWS:","title":"Introduction"},{"location":"admin/roleChaining/#prerequisites","text":"Before you begin this guide you'll need the following: A Hopsworks cluster running on EC2.","title":"Prerequisites"},{"location":"admin/roleChaining/#step-1-create-an-instance-profile-role","text":"Create an instance profile role with policies that will allow it to assume all resource roles that we can assume from the Hopsworks cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AssumeDataRoles\" , \"Effect\" : \"Allow\" , \"Action\" : \"sts:AssumeRole\" , \"Resource\" : [ \"arn:aws:iam::123456789011:role/test-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/dev-s3-role\" , \"arn:aws:iam::xxxxxxxxxxxx:role/redshift\" ] } ] } Example policy for assuming four roles.","title":"Step 1: Create an instance profile role"},{"location":"admin/roleChaining/#step-2-create-the-resource-roles","text":"Create the resource roles and edit trust relationship and add policy document that will allow the instance profile to assume this role. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::xxxxxxxxxxxx:role/instance-profile\" }, \"Action\" : \"sts:AssumeRole\" } ] } Example policy document.","title":"Step 2: Create the resource roles"},{"location":"admin/roleChaining/#step-3-create-mappings","text":"Role chaining allows the instance profile to assume any role in the policy attached in step 1. To limit access to iam roles we can create a per-project mapping from the admin page in Hopsworks. Role Chaining Click on your name in the top right corner of the navigation bar and choose Cluster Settings from the dropdown menu. In the Cluster Settings' IAM Role Chaining tab you can configure the mappings between projects and IAM roles. You can add mappings by entering the project name, which roles in that project can access the cloud role and the role ARN. Optionally you can set a role mapping as default by marking the default checkbox. The default roles can be changed from the project setting by a Data owner in that project. Create Role Chaining Any member of a project can then go to the Project Settings -> Assuming IAM Roles page to see which roles they can assume.","title":"Step 3: Create mappings"},{"location":"admin/roleChaining/#conclusion","text":"In this guide you learned how to map AWS IAM roles to project roles in Hopsworks.","title":"Conclusion"},{"location":"admin/services/","text":"Manage Services # Hopsworks provides administrators with a view of the status/health of the cluster. This information is provided through the Services page. You can find the Services page by clicking on your name, in the top right corner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Services tab. Services page This page give administrators an overview of which services are running on the cluster. It provides information about their status as reported by agents that monitor the status of the different Systemd units. Columns in the services table represent machines in your cluster. Each service running on a machine will have a status running (green) or stopped (red). If a service is not installed on a machine it will have a status not installed (gray). Services are divided into groups, and you can search for a service by its name or group. You can also search for machines by their host name. Services After you find the correct service you will be able to start , stop or restart it, by clicking on its status. Start, Stop and Restart a service Note Stopping some services like the web server (glassfish_domain1) is not recommended. If you stop it you will have to access the machine running the service and start it with systemctl start glassfish_domain1 .","title":"Manage Services"},{"location":"admin/services/#manage-services","text":"Hopsworks provides administrators with a view of the status/health of the cluster. This information is provided through the Services page. You can find the Services page by clicking on your name, in the top right corner of the navigation bar, and choosing Cluster Settings from the dropdown menu and going to the Services tab. Services page This page give administrators an overview of which services are running on the cluster. It provides information about their status as reported by agents that monitor the status of the different Systemd units. Columns in the services table represent machines in your cluster. Each service running on a machine will have a status running (green) or stopped (red). If a service is not installed on a machine it will have a status not installed (gray). Services are divided into groups, and you can search for a service by its name or group. You can also search for machines by their host name. Services After you find the correct service you will be able to start , stop or restart it, by clicking on its status. Start, Stop and Restart a service Note Stopping some services like the web server (glassfish_domain1) is not recommended. If you stop it you will have to access the machine running the service and start it with systemctl start glassfish_domain1 .","title":"Manage Services"},{"location":"admin/user/","text":"User Management # Whether you run Hopsworks on-premise, or on the cloud using hopsworks.ai , you have a Hopsworks cluster which contains all users and projects. Cluster users # All the users of your Hopsworks instance have access to your cluster with different access rights. You can find them by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu and going to the Users tab (You need to have Admin role to get access to the Cluster Settings page). Active Users Cluster roles # Roles let you manage the access rights of a user to the cluster. User: users with this role are only allowed to use the cluster by creating a limited number of projects. Admin: users with this role are allowed to manage the cluster. This includes accepting new users to the cluster or blocking them, managing user quota, configure alerts and setting up authentication methods . Validating and blocking users # By default, a user who register on Hopsworks using their own credentials are not granted access to the cluster. First, a user with an admin role needs to validate their account. By clicking on the Review Requests button you can open a user request review popup as shown in the image below. Review user request On the user request review popup you can activate or block users. Users with a validated email address will have a check mark on their email. Similarly, if a user is no longer allowed access to the cluster you can block them. To keep consistency with the history of your datasets, a user can not be deleted but only blocked. If necessary a user can be deleted manually in the cluster using the command line. You can block a user by clicking on the block icon on the right side of the user in the list. Blocked Users Blocked users will appear on the lower section of the page. Click on display blocked users to show all the blocked users in your cluster. If a user is blocked by mistake you can reactivate it by clicking on the check mark icon that corresponds to that user in the blocked users list. You can also change the role of a user by clicking on the select dropdown that shows the current role of the user. If there are too many users in your cluster, use the search box (available for blocked users too) to filter users by name or email. It is also possible to filter activated users by role. For example to see all administrators in you cluster click on the select dropdown to the right of the search box and choose Admin . Create users # If you want to allow users to login without registering you can pre-create them by clicking on New user . Create new user After setting the user's name and email chose the type of user you want to create (Hopsworks, Kerberos or LDAP). To create a Kerberos or LDAP user you need to get the users UUID from the Kerberos or LDAP server. Hopsworks user can also be assigned a Role . Kerberos and LDAP users on the other hand can only be assigned a role through group mapping. A temporary password will be generated and displayed when you click on Create new user . Copy the password and pass it securely to the user. Copy temporary password Reset user password # In the case where a user loses her/his password and can not recover it with the password recovery , an administrator can reset it for them. On the bottom of the Users page click on the Reset a user password link. A popup window with a dropdown for searching users by name or email will open. Find the user and click on Reset new password . Reset user password A temporary password will be displayed. Copy the password and pass it to the user securely. Copy temporary password A user with a temporary password will see a warning message when going to User settings Authentication tab. Change password Note A temporary password should be changed as soon as possible.","title":"User Management"},{"location":"admin/user/#user-management","text":"Whether you run Hopsworks on-premise, or on the cloud using hopsworks.ai , you have a Hopsworks cluster which contains all users and projects.","title":"User Management"},{"location":"admin/user/#cluster-users","text":"All the users of your Hopsworks instance have access to your cluster with different access rights. You can find them by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu and going to the Users tab (You need to have Admin role to get access to the Cluster Settings page). Active Users","title":"Cluster users"},{"location":"admin/user/#cluster-roles","text":"Roles let you manage the access rights of a user to the cluster. User: users with this role are only allowed to use the cluster by creating a limited number of projects. Admin: users with this role are allowed to manage the cluster. This includes accepting new users to the cluster or blocking them, managing user quota, configure alerts and setting up authentication methods .","title":"Cluster roles"},{"location":"admin/user/#validating-and-blocking-users","text":"By default, a user who register on Hopsworks using their own credentials are not granted access to the cluster. First, a user with an admin role needs to validate their account. By clicking on the Review Requests button you can open a user request review popup as shown in the image below. Review user request On the user request review popup you can activate or block users. Users with a validated email address will have a check mark on their email. Similarly, if a user is no longer allowed access to the cluster you can block them. To keep consistency with the history of your datasets, a user can not be deleted but only blocked. If necessary a user can be deleted manually in the cluster using the command line. You can block a user by clicking on the block icon on the right side of the user in the list. Blocked Users Blocked users will appear on the lower section of the page. Click on display blocked users to show all the blocked users in your cluster. If a user is blocked by mistake you can reactivate it by clicking on the check mark icon that corresponds to that user in the blocked users list. You can also change the role of a user by clicking on the select dropdown that shows the current role of the user. If there are too many users in your cluster, use the search box (available for blocked users too) to filter users by name or email. It is also possible to filter activated users by role. For example to see all administrators in you cluster click on the select dropdown to the right of the search box and choose Admin .","title":"Validating and blocking users"},{"location":"admin/user/#create-users","text":"If you want to allow users to login without registering you can pre-create them by clicking on New user . Create new user After setting the user's name and email chose the type of user you want to create (Hopsworks, Kerberos or LDAP). To create a Kerberos or LDAP user you need to get the users UUID from the Kerberos or LDAP server. Hopsworks user can also be assigned a Role . Kerberos and LDAP users on the other hand can only be assigned a role through group mapping. A temporary password will be generated and displayed when you click on Create new user . Copy the password and pass it securely to the user. Copy temporary password","title":"Create users"},{"location":"admin/user/#reset-user-password","text":"In the case where a user loses her/his password and can not recover it with the password recovery , an administrator can reset it for them. On the bottom of the Users page click on the Reset a user password link. A popup window with a dropdown for searching users by name or email will open. Find the user and click on Reset new password . Reset user password A temporary password will be displayed. Copy the password and pass it to the user securely. Copy temporary password A user with a temporary password will see a warning message when going to User settings Authentication tab. Change password Note A temporary password should be changed as soon as possible.","title":"Reset user password"},{"location":"admin/ldap/configure-krb/","text":"Configure Kerberos # Kerberos need some server configuration before you can enable it from the UI. For instruction on how to configure your hopsworks server see Server Configuration for Kerberos After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable Kerberos by clicking on the Kerberos checkbox. If LDAP/Kerberos checkbox is not checked, make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Finally, click on edit configuration and fill in the attributes. Configure Kerberos Account status: the status a user will be assigned when logging in for the first time. If a user is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Principal search filter: the search filter for principal name. Default krbPrincipalName=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use Kerberos for authentication. Log in using Kerberos Note Make sure that you have Kerberos properly configured on your computer and you are logged in. Kerberos support must also be configured on the browser to use Kerberos for authentication.","title":"Configure Kerberos"},{"location":"admin/ldap/configure-krb/#configure-kerberos","text":"Kerberos need some server configuration before you can enable it from the UI. For instruction on how to configure your hopsworks server see Server Configuration for Kerberos After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable Kerberos by clicking on the Kerberos checkbox. If LDAP/Kerberos checkbox is not checked, make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Finally, click on edit configuration and fill in the attributes. Configure Kerberos Account status: the status a user will be assigned when logging in for the first time. If a user is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Principal search filter: the search filter for principal name. Default krbPrincipalName=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use Kerberos for authentication. Log in using Kerberos Note Make sure that you have Kerberos properly configured on your computer and you are logged in. Kerberos support must also be configured on the browser to use Kerberos for authentication.","title":"Configure Kerberos"},{"location":"admin/ldap/configure-ldap/","text":"Configure LDAP/Kerberos # LDAP need some server configuration before you can enable it from the UI. For instruction on how to configure your hopsworks server see Server Configuration for LDAP After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable LDAP by clicking on the LDAP checkbox. If LDAP/Kerberos checkbox is not checked make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Finally, click on edit configuration and fill in the attributes. Configure LDAP Account status: the status a user will be assigned when logging in for the first time. If a use is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use LDAP for authentication. Log in using LDAP","title":"Configure LDAP"},{"location":"admin/ldap/configure-ldap/#configure-ldapkerberos","text":"LDAP need some server configuration before you can enable it from the UI. For instruction on how to configure your hopsworks server see Server Configuration for LDAP After configuring the server you can configure Authentication methods by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Authentication tab you can find in Cluster Settings , you can enable LDAP by clicking on the LDAP checkbox. If LDAP/Kerberos checkbox is not checked make sure that you configured your application server and enable it by clicking on the checkbox. Setup Authentication Methods Finally, click on edit configuration and fill in the attributes. Configure LDAP Account status: the status a user will be assigned when logging in for the first time. If a use is assigned a status different from Activated an admin needs to manually activate each user from the User management . Group mapping: allows you to specify a mapping between LDAP groups and Hopsworks groups. The mapping is a semicolon separated string in the form Directory Administrators->HOPS_ADMIN;IT People-> HOPS_USER . Default is empty. If no mapping is specified, users need to be assigned a role by an admin before they can log in. User id: the id field in LDAP with a string placeholder. Default uid=%s . User given name: the given name field in LDAP. Default givenName . User surname: the surname field in LDAP. Default sn . User email: the email field in LDAP. Default mail . User search filter: the search filter for user. Default uid=%s . Group search filter: the search filter for groups. Default member=%d . Group target: the target to search for groups in the LDAP directory tree. Default cn . Dynamic group target: the target to search for dynamic groups in the LDAP directory tree. Default memberOf . User dn: specify the distinguished name (DN) of the container or base point where the users are stored. Default is empty. Group dn: specify the DN of the container or base point where the groups are stored. Default is empty. All defaults are taken from OpenLDAP . The login page will now have the choice to use LDAP for authentication. Log in using LDAP","title":"Configure LDAP/Kerberos"},{"location":"admin/ldap/configure-server/","text":"Configure Server for LDAP and Kerberos # LDAP and Kerberos integration need some configuration in the Karamel cluster definition used to deploy your hopsworks cluster. Server Configuration for LDAP # The LDAP attributes below are used to configure JNDI external resource in Payara. The JNDI resource will communicate with your LDAP server to perform the authentication. ldap : enabled : true jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"entryUUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" jndilookupname: should contain the LDAP domain. attr_binary_val: is the binary unique identifier that will be used in subsequent logins to identify the user. security_auth: how to authenticate to the LDAP server. security_principal: contains the username of the user that will be used to query LDAP. security_credentials: contains the password of the user that will be used to query LDAP. referral: whether to follow or ignore an alternate location in which an LDAP Request may be processed. Server Configuration for Kerberos # The Kerberos attributes are used to configure SPNEGO . SPNEGO is used to establish a secure context between the requester and the application server when using Kerberos authentication. kerberos : enabled : true krb_conf_path : \"/etc/krb5.conf\" krb_server_key_tab_path : \"/etc/security/keytabs/service.keytab\" krb_server_key_tab_name : \"service.keytab\" spnego_server_conf : '\\nuseKeyTab=true\\nprincipal=\\\"HTTP/server.hopsworks.ai@HOPSWORKS.AI\\\"\\nstoreKey=true\\nisInitiator=false' ldap : jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"objectGUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" Both Kerberos and LDAP attributes need to be specified to configure Kerberos. The LDAP attributes are explained above. krb_conf_path: contains the path to the krb5.conf used by SPNEGO to get information about the default domain and the location of the Kerberos KDC. The file is copied by the recipe in to /srv/hops/domains/domain1/config. krb_server_key_tab_path: contains the path to the Kerberos service keytab. The keytab is copied by the recipe in to /srv/hops/domains/domain/config with the name set in the krb_server_key_tab_name attribute. spnego_server_conf: contains the configuration that will be appended to Payara's (application serve used to host hopsworks) login.conf. In particular, it should contain useKeyTab=true, and the principal name to be used in the authentication phase. Initiator should be set to false.","title":"Configure server for LDAP and Kerberos"},{"location":"admin/ldap/configure-server/#configure-server-for-ldap-and-kerberos","text":"LDAP and Kerberos integration need some configuration in the Karamel cluster definition used to deploy your hopsworks cluster.","title":"Configure Server for LDAP and Kerberos"},{"location":"admin/ldap/configure-server/#server-configuration-for-ldap","text":"The LDAP attributes below are used to configure JNDI external resource in Payara. The JNDI resource will communicate with your LDAP server to perform the authentication. ldap : enabled : true jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"entryUUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" jndilookupname: should contain the LDAP domain. attr_binary_val: is the binary unique identifier that will be used in subsequent logins to identify the user. security_auth: how to authenticate to the LDAP server. security_principal: contains the username of the user that will be used to query LDAP. security_credentials: contains the password of the user that will be used to query LDAP. referral: whether to follow or ignore an alternate location in which an LDAP Request may be processed.","title":"Server Configuration for LDAP"},{"location":"admin/ldap/configure-server/#server-configuration-for-kerberos","text":"The Kerberos attributes are used to configure SPNEGO . SPNEGO is used to establish a secure context between the requester and the application server when using Kerberos authentication. kerberos : enabled : true krb_conf_path : \"/etc/krb5.conf\" krb_server_key_tab_path : \"/etc/security/keytabs/service.keytab\" krb_server_key_tab_name : \"service.keytab\" spnego_server_conf : '\\nuseKeyTab=true\\nprincipal=\\\"HTTP/server.hopsworks.ai@HOPSWORKS.AI\\\"\\nstoreKey=true\\nisInitiator=false' ldap : jndilookupname : \"dc=hopsworks,dc=ai\" provider_url : \"ldap://193.10.66.104:1389\" attr_binary_val : \"objectGUID\" security_auth : \"none\" security_principal : \"\" security_credentials : \"\" referral : \"ignore\" additional_props : \"\" Both Kerberos and LDAP attributes need to be specified to configure Kerberos. The LDAP attributes are explained above. krb_conf_path: contains the path to the krb5.conf used by SPNEGO to get information about the default domain and the location of the Kerberos KDC. The file is copied by the recipe in to /srv/hops/domains/domain1/config. krb_server_key_tab_path: contains the path to the Kerberos service keytab. The keytab is copied by the recipe in to /srv/hops/domains/domain/config with the name set in the krb_server_key_tab_name attribute. spnego_server_conf: contains the configuration that will be appended to Payara's (application serve used to host hopsworks) login.conf. In particular, it should contain useKeyTab=true, and the principal name to be used in the authentication phase. Initiator should be set to false.","title":"Server Configuration for Kerberos"},{"location":"admin/oauth2/create-azure-client/","text":"Create An Application in Azure Active Directory. # This example uses Azure Active Directory as the identity provider, but the same can be done with any identity provider supporting OAuth2. Configure your identity provider. # To use OAuth2 in hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers. Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory . Click on App Registrations . Click on New Registration . Create application Enter a name for the client such as hopsworks_oauth_client . Verify the Supported account type is set to Accounts in this organizational directory only . And Click Register. Name application In the Overview section, copy the Application (client) ID field . We will use it in Identity Provider registration under the name Client id . Copy client ID Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part. We will use it in Identity Provider registration under the name Connection URL . Endpoint Click on Certificates & secrets , then Click on New client secret . New client secret Add a description of the secret. Select an expiration period. And, Click Add . Client secret creation Copy the secret. This will be used in Identity Provider registration under the name Client Secret . Client secret creation Click on Authentication . Then click on Add a platform Add a platform In Configure platforms click on Web . Configure platform: Web Enter the Redirect URI and click on Configure . The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your hopsworks cluster. Configure platform: Redirect Note If your hopsworks cluster is created on the cloud (hopsworks.ai), you can find your HOPSWORKS-URI by going to the hopsworks.ai dashboard in the General tab of your cluster and copying the URI.","title":"Create Azure Client"},{"location":"admin/oauth2/create-azure-client/#create-an-application-in-azure-active-directory","text":"This example uses Azure Active Directory as the identity provider, but the same can be done with any identity provider supporting OAuth2.","title":"Create An Application in Azure Active Directory."},{"location":"admin/oauth2/create-azure-client/#configure-your-identity-provider","text":"To use OAuth2 in hopsworks you first need to create and configure an OAuth client in your identity provider. We will take the example of Azure AD for the remaining of this documentation, but equivalent steps can be taken on other identity providers. Navigate to the Microsoft Azure Portal and authenticate. Navigate to Azure Active Directory . Click on App Registrations . Click on New Registration . Create application Enter a name for the client such as hopsworks_oauth_client . Verify the Supported account type is set to Accounts in this organizational directory only . And Click Register. Name application In the Overview section, copy the Application (client) ID field . We will use it in Identity Provider registration under the name Client id . Copy client ID Click on Endpoints and copy the OpenId Connect metadata document endpoint excluding the .well-known/openid-configuration part. We will use it in Identity Provider registration under the name Connection URL . Endpoint Click on Certificates & secrets , then Click on New client secret . New client secret Add a description of the secret. Select an expiration period. And, Click Add . Client secret creation Copy the secret. This will be used in Identity Provider registration under the name Client Secret . Client secret creation Click on Authentication . Then click on Add a platform Add a platform In Configure platforms click on Web . Configure platform: Web Enter the Redirect URI and click on Configure . The redirect URI is HOPSWORKS-URI/callback with HOPSWORKS-URI the URI of your hopsworks cluster. Configure platform: Redirect Note If your hopsworks cluster is created on the cloud (hopsworks.ai), you can find your HOPSWORKS-URI by going to the hopsworks.ai dashboard in the General tab of your cluster and copying the URI.","title":"Configure your identity provider."},{"location":"admin/oauth2/create-client/","text":"Register Identity Provider in Hopsworks # Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . An example on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively. After acquiring the client id and client secret create the client in Hopsworks by enabling OAuth2 and clicking on add another identity provider in the Authentication configuration page . Then set base uri of your identity provider in Connection URL give a name to your identity provider (the name will be used in the login page as an alternative login method) and set the client id and client secret in their respective fields, as shown in the figure below. Application overview Connection URL : (provider Uri) is the base uri of the identity provider's API (URI should contain scheme http:// or https://). Additional configuration can be set here: Verify email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Code challenge : if your identity provider requires code challenge for authorization request check the code challenge check box. This will allow you to choose code challenge method that can be either plain or S256 . Logo URL : optionally a logo URL to an image can be added. The logo will be shown on the login page with the name as shown in the figure below. Group mapping # Optionally you can add a group mapping from your identity provider to hopsworks groups, by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Cluster Settings Configuration tab search for oauth_group_mapping and click on the edit button. Set Configuration variables Note Setting oauth_group_mapping to ANY_GROUP->HOPS_USER will assign the role user to any user from any group in your identity provider when they log into hopsworks with OAuth for the first time. You can replace ANY_GROUP with the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the users of that group to be admins in hopsworks. You can do several mappings by separating them with a semicolon. Users will now see a new button on the login page. The button has the name you set above for Name and will redirect to your identity provider. Login with OAuth2 Note When creating a client make sure you can access the provider metadata by making a GET request on the well known endpoint of the provider. The well-known URL, will typically be the Connection URL plus .well-known/openid-configuration . For the above client it would be https://dev-86723251.okta.com/.well-known/openid-configuration .","title":"Register an Identity Provider"},{"location":"admin/oauth2/create-client/#register-identity-provider-in-hopsworks","text":"Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . An example on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively. After acquiring the client id and client secret create the client in Hopsworks by enabling OAuth2 and clicking on add another identity provider in the Authentication configuration page . Then set base uri of your identity provider in Connection URL give a name to your identity provider (the name will be used in the login page as an alternative login method) and set the client id and client secret in their respective fields, as shown in the figure below. Application overview Connection URL : (provider Uri) is the base uri of the identity provider's API (URI should contain scheme http:// or https://). Additional configuration can be set here: Verify email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Code challenge : if your identity provider requires code challenge for authorization request check the code challenge check box. This will allow you to choose code challenge method that can be either plain or S256 . Logo URL : optionally a logo URL to an image can be added. The logo will be shown on the login page with the name as shown in the figure below.","title":"Register Identity Provider in Hopsworks"},{"location":"admin/oauth2/create-client/#group-mapping","text":"Optionally you can add a group mapping from your identity provider to hopsworks groups, by clicking on your name in the top right corner of the navigation bar and choosing Cluster Settings from the dropdown menu. In the Cluster Settings Configuration tab search for oauth_group_mapping and click on the edit button. Set Configuration variables Note Setting oauth_group_mapping to ANY_GROUP->HOPS_USER will assign the role user to any user from any group in your identity provider when they log into hopsworks with OAuth for the first time. You can replace ANY_GROUP with the group of your choice in the identity provider. You can replace HOPS_USER by HOPS_ADMIN if you want the users of that group to be admins in hopsworks. You can do several mappings by separating them with a semicolon. Users will now see a new button on the login page. The button has the name you set above for Name and will redirect to your identity provider. Login with OAuth2 Note When creating a client make sure you can access the provider metadata by making a GET request on the well known endpoint of the provider. The well-known URL, will typically be the Connection URL plus .well-known/openid-configuration . For the above client it would be https://dev-86723251.okta.com/.well-known/openid-configuration .","title":"Group mapping"},{"location":"admin/oauth2/create-okta-client/","text":"Create An Application in Okta # This example uses an Okta development account to create an application that will represent a Hopsworks client in the identity provider. To create a developer account go to Okta developer . After creating a developer account register a client by going to Applications and click on Create App Integration . Okta Applications This will open a popup as shown in the figure below. Select OIDC as Sign-in-method and Web Application as Application type and click next. Create new Application Give your application a name and select Client credential as Grant Type . Then add a Sign-in redirect URI that is your Hopsworks cluster domain name (including the port number if needed) with path /callback , and a Sign-out redirect URI that is Hopsworks cluster domain name (including the port number if needed) with no path. New Application If you want to limit who can access your Hopsworks cluster select Limit access to selected groups and select group(s) you want to give access to. Here we will allow everyone in the organization to access the cluster. Group assignment Group mapping # You can also create mappings from groups in Okta to groups in Hopsworks. To achieve this you need to configure Okta to send Groups with user information. To do this go to Applications and select your application name. In the Sign On tab click edit OpenID Connect ID Token and select Filter for Groups claim type , then for Groups claim filter add groups as the claim name, select Match Regex from the dropdown and .* (dot star) as Regex to match all groups. See Group mapping on how to do the mapping in Hopsworks. Group claim After the application is created go back to Applications and click on the application you just created. Use the Okta domain ( Connection URL ), client id and client secret generated for your app in the Identity Provider registration in Hopsworks. Application overview Note When copying the domain in the figure above make sure to add the url scheme (http:// or https://) when using it in the Connection URL in the Identity Provider registration form .","title":"Create Okta Client"},{"location":"admin/oauth2/create-okta-client/#create-an-application-in-okta","text":"This example uses an Okta development account to create an application that will represent a Hopsworks client in the identity provider. To create a developer account go to Okta developer . After creating a developer account register a client by going to Applications and click on Create App Integration . Okta Applications This will open a popup as shown in the figure below. Select OIDC as Sign-in-method and Web Application as Application type and click next. Create new Application Give your application a name and select Client credential as Grant Type . Then add a Sign-in redirect URI that is your Hopsworks cluster domain name (including the port number if needed) with path /callback , and a Sign-out redirect URI that is Hopsworks cluster domain name (including the port number if needed) with no path. New Application If you want to limit who can access your Hopsworks cluster select Limit access to selected groups and select group(s) you want to give access to. Here we will allow everyone in the organization to access the cluster. Group assignment","title":"Create An Application in Okta"},{"location":"admin/oauth2/create-okta-client/#group-mapping","text":"You can also create mappings from groups in Okta to groups in Hopsworks. To achieve this you need to configure Okta to send Groups with user information. To do this go to Applications and select your application name. In the Sign On tab click edit OpenID Connect ID Token and select Filter for Groups claim type , then for Groups claim filter add groups as the claim name, select Match Regex from the dropdown and .* (dot star) as Regex to match all groups. See Group mapping on how to do the mapping in Hopsworks. Group claim After the application is created go back to Applications and click on the application you just created. Use the Okta domain ( Connection URL ), client id and client secret generated for your app in the Identity Provider registration in Hopsworks. Application overview Note When copying the domain in the figure above make sure to add the url scheme (http:// or https://) when using it in the Connection URL in the Identity Provider registration form .","title":"Group mapping"},{"location":"concepts/hopsworks/","text":"Hopsworks is a modular MLOps platform with: a feature store (available as standalone) model registry and model serving based on KServe vector database based on OpenSearch a data science and data engineering platform Standalone Feature Store # Hopsworks was the first open-source and first enterprise feature store for ML. You can use Hopsworks as a standalone feature store with the HSFS API. Model Management # Hopsworks includes support for model management, with model deployments using the KServe framework and a model registry designed for KServe. Hopsworks logs all inference requests to Kafka to enable easy monitoring of deployed models, and provides model metrics with grafana/prometheus. Vector DB # Hopsworks provides a vector database (or embedding store) based on OpenSearch kNN ( FAISS and nmslib ). Hopsworks Vector DB includes out-of-the-box support for authentication, access control, filtering, backup-and-restore, and horizontal scalability. Hopsworks' Feature Store and vector DB are often used together to build scalable recommender systems, such as ranking-and-retrieval for real-time recommendations. Governance # Hopsworks provides a data-mesh architecture for managing ML assets and teams, with multi-tenant projects. Not unlike a github repository, a project is a sandbox containing team members, data, and ML assets. In Hopsworks, all ML assets (features, models, training data) are versioned, taggable, lineage-tracked, and support free-text search. Data can be also be securely shared between projects. Data Science Platform # You can develop feature engineering pipelines and training pipelines in Hopsworks. There is support for version control (github, gitlab, bitbucket), Jupyter notebooks, a shared distriubted file system, per project conda environments for managing python dependencies without needing to write Dockerfiles, jobs (Python, Spark, Flink), and workflow orchestration with Airflow.","title":"Hopsworks Platform"},{"location":"concepts/hopsworks/#standalone-feature-store","text":"Hopsworks was the first open-source and first enterprise feature store for ML. You can use Hopsworks as a standalone feature store with the HSFS API.","title":"Standalone Feature Store"},{"location":"concepts/hopsworks/#model-management","text":"Hopsworks includes support for model management, with model deployments using the KServe framework and a model registry designed for KServe. Hopsworks logs all inference requests to Kafka to enable easy monitoring of deployed models, and provides model metrics with grafana/prometheus.","title":"Model Management"},{"location":"concepts/hopsworks/#vector-db","text":"Hopsworks provides a vector database (or embedding store) based on OpenSearch kNN ( FAISS and nmslib ). Hopsworks Vector DB includes out-of-the-box support for authentication, access control, filtering, backup-and-restore, and horizontal scalability. Hopsworks' Feature Store and vector DB are often used together to build scalable recommender systems, such as ranking-and-retrieval for real-time recommendations.","title":"Vector DB"},{"location":"concepts/hopsworks/#governance","text":"Hopsworks provides a data-mesh architecture for managing ML assets and teams, with multi-tenant projects. Not unlike a github repository, a project is a sandbox containing team members, data, and ML assets. In Hopsworks, all ML assets (features, models, training data) are versioned, taggable, lineage-tracked, and support free-text search. Data can be also be securely shared between projects.","title":"Governance"},{"location":"concepts/hopsworks/#data-science-platform","text":"You can develop feature engineering pipelines and training pipelines in Hopsworks. There is support for version control (github, gitlab, bitbucket), Jupyter notebooks, a shared distriubted file system, per project conda environments for managing python dependencies without needing to write Dockerfiles, jobs (Python, Spark, Flink), and workflow orchestration with Airflow.","title":"Data Science Platform"},{"location":"concepts/dev/inside/","text":"Hopsworks provides a complete self-service development environment for feature engineering and model training. You can develop programs as Jupyer noteooks or jobs, you can manage the Python libaries in a project using its conda environment, you can manage your source code in Git or BitBucket, and you can orchestrate jobs with Airflow. Jupyter Notebooks # Hopsworks provides a Jupyter notebook development environment for programs written in Python, Spark, Flink, and SparkSQL. You can also develop in your IDE (PyCharm, IntelliJ, etc), test locally, and then run your programs as Jobs in Hopsworks. Jupyter notebooks can also be run as Jobs. Source Code Control # Hopsworks provides source code control support using Git (Github, Gitlab) or Bitbucket. You can securely checkout code into your project and commit and push updates to your code to your source code repository. Conda Environment per Project # Hopsworks supports the self-service installation of Python libraries using PyPi, Conda, Wheel files, or Github URLs. The Python libraries are installed in a Conda environment linked with your project. Each project has a base Docker image and its custom conda environment. Jobs are run as Docker images, but they are compiled transparently for you when you update your Conda enviornment. That is, there is no need to write a Dockerfile, users install Python libraries in their project. You can setup custom development and production environments by creating new projects, each with their own conda environment. Jobs # In Hopsworks, a Job is a schedulable program that is allocated compute and memory resources. You can run a Job in Hopsworks: from the UI; programmatically with the Hopsworks SDK (Python, Java) or REST API; from Airflow programs (either inside our outside Hopsworks); from your IDE using a plugin ( PyCharm/IntelliJ plugin ); Orchestration # Airflow comes out-of-the box with Hopsworks, but you can also use an external Airflow cluster (with the Hopsworks Job operator) if you have one. Airflow can be used to schedule the execution of Jobs, individually or as part of Airflow DAGs.","title":"Inside Hopsworks"},{"location":"concepts/dev/inside/#jupyter-notebooks","text":"Hopsworks provides a Jupyter notebook development environment for programs written in Python, Spark, Flink, and SparkSQL. You can also develop in your IDE (PyCharm, IntelliJ, etc), test locally, and then run your programs as Jobs in Hopsworks. Jupyter notebooks can also be run as Jobs.","title":"Jupyter Notebooks"},{"location":"concepts/dev/inside/#source-code-control","text":"Hopsworks provides source code control support using Git (Github, Gitlab) or Bitbucket. You can securely checkout code into your project and commit and push updates to your code to your source code repository.","title":"Source Code Control"},{"location":"concepts/dev/inside/#conda-environment-per-project","text":"Hopsworks supports the self-service installation of Python libraries using PyPi, Conda, Wheel files, or Github URLs. The Python libraries are installed in a Conda environment linked with your project. Each project has a base Docker image and its custom conda environment. Jobs are run as Docker images, but they are compiled transparently for you when you update your Conda enviornment. That is, there is no need to write a Dockerfile, users install Python libraries in their project. You can setup custom development and production environments by creating new projects, each with their own conda environment.","title":"Conda Environment per Project"},{"location":"concepts/dev/inside/#jobs","text":"In Hopsworks, a Job is a schedulable program that is allocated compute and memory resources. You can run a Job in Hopsworks: from the UI; programmatically with the Hopsworks SDK (Python, Java) or REST API; from Airflow programs (either inside our outside Hopsworks); from your IDE using a plugin ( PyCharm/IntelliJ plugin );","title":"Jobs"},{"location":"concepts/dev/inside/#orchestration","text":"Airflow comes out-of-the box with Hopsworks, but you can also use an external Airflow cluster (with the Hopsworks Job operator) if you have one. Airflow can be used to schedule the execution of Jobs, individually or as part of Airflow DAGs.","title":"Orchestration"},{"location":"concepts/dev/outside/","text":"You can write programs that use Hopsworks in any Python, Spark, PySpark, or Flink environment. Hopsworks also running SQL queries to compute features in external data warehouses. The Feature Store can also be queried with SQL. There is REST API for Hopsworks that can be used with a valid API key, generated in Hopsworks. However, it is often easier to develop your programs against SDKs available in Python and Java/Scala for HSFS, in Python for HSML, and in Python for the Hopsworks API.","title":"Outside Hopsworks"},{"location":"concepts/fs/","text":"What is Hopsworks Feature Store? # Hopsworks and its Feature Store are an open source data-intensive AI platform used for the development and operation of machine learning models at scale. The Hopsworks Feature Store provides the HSFS API to enable clients to write features to feature groups in the feature store, and to read features from feature views - either through a low latency Online API to retrieve pre-computed features for operational models or through a high throughput, latency insensitive Offline API, used to create training data and to retrieve batch data for scoring. HSFS API # The HSFS (HopsworkS Feature Store) API is how you, as a developer, will use the feature store. The HSFS API helps simplify some of the problems that feature stores address including: consistent features for training and serving centralized, secure access to features point-in-time JOINs of features to create training data with no data leakage easier connection and backfilling of features from external data sources use of external tables as features transparent computation of statistics and usage data for features. Write to feature groups, read from feature views. # You write to feature groups with a feature pipeline program. The program can be written in Python, Spark, Flink, or SQL. You read from views on top of the feature groups, called feature views. That is, a feature view does not store feature data, but is a logical grouping of features. Typically, you define a feature view because you want to train/deploy a model with exactly those features in the feature view. Feature views enable the reuse of feature data from different feature groups across different models.","title":"Architecture"},{"location":"concepts/fs/#what-is-hopsworks-feature-store","text":"Hopsworks and its Feature Store are an open source data-intensive AI platform used for the development and operation of machine learning models at scale. The Hopsworks Feature Store provides the HSFS API to enable clients to write features to feature groups in the feature store, and to read features from feature views - either through a low latency Online API to retrieve pre-computed features for operational models or through a high throughput, latency insensitive Offline API, used to create training data and to retrieve batch data for scoring.","title":"What is Hopsworks Feature Store?"},{"location":"concepts/fs/#hsfs-api","text":"The HSFS (HopsworkS Feature Store) API is how you, as a developer, will use the feature store. The HSFS API helps simplify some of the problems that feature stores address including: consistent features for training and serving centralized, secure access to features point-in-time JOINs of features to create training data with no data leakage easier connection and backfilling of features from external data sources use of external tables as features transparent computation of statistics and usage data for features.","title":"HSFS API"},{"location":"concepts/fs/#write-to-feature-groups-read-from-feature-views","text":"You write to feature groups with a feature pipeline program. The program can be written in Python, Spark, Flink, or SQL. You read from views on top of the feature groups, called feature views. That is, a feature view does not store feature data, but is a logical grouping of features. Typically, you define a feature view because you want to train/deploy a model with exactly those features in the feature view. Feature views enable the reuse of feature data from different feature groups across different models.","title":"Write to feature groups, read from feature views."},{"location":"concepts/fs/feature_group/external_fg/","text":"External feature groups are offline feature groups where their data is stored in an external table. An external table requires a storage connector, defined with the Connector API (or more typically in the user interface), to enable HSFS to retrieve data from the external table. An external table includes a user-defined SQL string for retrieving data, but you also perform SQL operations, including projections, aggregations, and so on. The SQL query is executed on-demand when HSFS retrieves data from the external Feature Group, for example, when creating training data using features in the external table. In the image below, we can see that HSFS currently supports a large number of data sources, including any JDBC-enabled source, Snowflake, Data Lake, Redshift, BigQuery, S3, ADLS, GCS, and Kafka","title":"External Feature Groups"},{"location":"concepts/fs/feature_group/feature_pipelines/","text":"A feature pipeline is a program that orchestrates the execution of a dataflow graph of data validation, aggregation, dimensionality reduction, transformation, and other feature engineering steps on input data to create and/or update feature data. With HSFS, you can write feature pipelines in different languages as shown in the figure below. Data Sources # Your feature pipeline needs to connect to some (external) data source to read the data to be processed. Python, Spark, and Flink have connectors to a huge number of different data sources, while SQL feature pipelines are often restricted to a single data source (for example, your connector to SnowFlake only runs SQL on SnowFlake). SparkSQL, in contrast, can be used over tables that originate in different data sources. Data Validation # In order to be able to train and serve models that you can rely on, you need clean, high quality features. Data validation operations include removing bad data, removing or imputing missing values, and identifying problems such as feature shift. HSFS supports Great Expectations to specify data validation rules that are executed in the client before features are written to the Feature Store. The validation results are collected and shown in Hopsworks. Aggregations # Aggregations are used to summarize large datasets into more concise, signal-rich features. Popular aggregations include count(), sum(), mean(), median(), stddev(), min(), and max(). These aggregations produce a single number (a numerical feature) that captures information about a potentially large dataset. Both numerical and categorical features are often transformed before being used to train or serve models. Dimensionality Reduction # If input data is impractically large or if it has a signnificant amount of redundancy, it can often be transformed into a reduced set of features with dimensionality reduction (often called feature extraction). Popular dimensionality algorithms include embedding algorithms, PCA, and TSNE. Transformations # Transformations are covered in more detail in training/inference pipelines , as transformations typically happen after the feature store. If you store transformed features in feature groups, the feature data is no longer useful for EDA (as it near to impossible for Data Scientists to understand the transformed values). It also makes it impossible for inference pipelines to log untransformed feature values and predictions for an operational model. There is one use case for storing transformed features in feature groups - when you need to have ultra low latency when reading precomputed features (and online transformations when reading features add too much latency for your use case). The figure below shows to include transformations in your feature pipelines. Feature Engineering in Python # Python is the most widely used framework for feature engineering due to its extensive library support for aggregations (Pandas), data validation (Great Expectations), and dimensionality reduction (embeddings, PCA), and transformations (in Scikit-Learn, TensorFlow, PyTorch). Python also supports open-source feature engineering frameworks used for automated feature engineering, such as featuretools that supports relational and temporal sources. Feature Engineering in Spark/PySpark # Spark is popular as a feature engineering framework as it can scale to process larger volumes of data than Python, and provides native support for aggregations, and it supports many of the same data validation (Great Expectations), and dimensionality reduction algorithms (embeddings, PCA) as Python. Spark also has native support for transformations, which are useful for analytical models (batch scoring), but less useful for operational models, where online transformations are required, and Spark environments are less common. Online model serving environments typically only support online transformations in Python. Feature Engineering in SQL # SQL has grown in popularity for performing heavy lifting in feature pipelines - computing aggregates on data - when the input data already resides in a data warehouse. Data warehouses also support data validation, for example, through Great Expectations in DBT. However, SQL is not mature as a platform for transformations and dimensionality reductions, where UDFs are applied row-wise. You can do aggregation in SQL for data in your data warehouse or database. Feature Engineering in Flink # Flink is used for feature engineering when you need very fresh features computed in real-time. Flink pipelines are often written in Java, and provide native support for aggregations, with dimensionality reduction algorithms and transformations also possible in Java.","title":"Feature Pipelines"},{"location":"concepts/fs/feature_group/feature_pipelines/#data-sources","text":"Your feature pipeline needs to connect to some (external) data source to read the data to be processed. Python, Spark, and Flink have connectors to a huge number of different data sources, while SQL feature pipelines are often restricted to a single data source (for example, your connector to SnowFlake only runs SQL on SnowFlake). SparkSQL, in contrast, can be used over tables that originate in different data sources.","title":"Data Sources"},{"location":"concepts/fs/feature_group/feature_pipelines/#data-validation","text":"In order to be able to train and serve models that you can rely on, you need clean, high quality features. Data validation operations include removing bad data, removing or imputing missing values, and identifying problems such as feature shift. HSFS supports Great Expectations to specify data validation rules that are executed in the client before features are written to the Feature Store. The validation results are collected and shown in Hopsworks.","title":"Data Validation"},{"location":"concepts/fs/feature_group/feature_pipelines/#aggregations","text":"Aggregations are used to summarize large datasets into more concise, signal-rich features. Popular aggregations include count(), sum(), mean(), median(), stddev(), min(), and max(). These aggregations produce a single number (a numerical feature) that captures information about a potentially large dataset. Both numerical and categorical features are often transformed before being used to train or serve models.","title":"Aggregations"},{"location":"concepts/fs/feature_group/feature_pipelines/#dimensionality-reduction","text":"If input data is impractically large or if it has a signnificant amount of redundancy, it can often be transformed into a reduced set of features with dimensionality reduction (often called feature extraction). Popular dimensionality algorithms include embedding algorithms, PCA, and TSNE.","title":"Dimensionality Reduction"},{"location":"concepts/fs/feature_group/feature_pipelines/#transformations","text":"Transformations are covered in more detail in training/inference pipelines , as transformations typically happen after the feature store. If you store transformed features in feature groups, the feature data is no longer useful for EDA (as it near to impossible for Data Scientists to understand the transformed values). It also makes it impossible for inference pipelines to log untransformed feature values and predictions for an operational model. There is one use case for storing transformed features in feature groups - when you need to have ultra low latency when reading precomputed features (and online transformations when reading features add too much latency for your use case). The figure below shows to include transformations in your feature pipelines.","title":"Transformations"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-python","text":"Python is the most widely used framework for feature engineering due to its extensive library support for aggregations (Pandas), data validation (Great Expectations), and dimensionality reduction (embeddings, PCA), and transformations (in Scikit-Learn, TensorFlow, PyTorch). Python also supports open-source feature engineering frameworks used for automated feature engineering, such as featuretools that supports relational and temporal sources.","title":"Feature Engineering in Python"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-sparkpyspark","text":"Spark is popular as a feature engineering framework as it can scale to process larger volumes of data than Python, and provides native support for aggregations, and it supports many of the same data validation (Great Expectations), and dimensionality reduction algorithms (embeddings, PCA) as Python. Spark also has native support for transformations, which are useful for analytical models (batch scoring), but less useful for operational models, where online transformations are required, and Spark environments are less common. Online model serving environments typically only support online transformations in Python.","title":"Feature Engineering in Spark/PySpark"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-sql","text":"SQL has grown in popularity for performing heavy lifting in feature pipelines - computing aggregates on data - when the input data already resides in a data warehouse. Data warehouses also support data validation, for example, through Great Expectations in DBT. However, SQL is not mature as a platform for transformations and dimensionality reductions, where UDFs are applied row-wise. You can do aggregation in SQL for data in your data warehouse or database.","title":"Feature Engineering in SQL"},{"location":"concepts/fs/feature_group/feature_pipelines/#feature-engineering-in-flink","text":"Flink is used for feature engineering when you need very fresh features computed in real-time. Flink pipelines are often written in Java, and provide native support for aggregations, with dimensionality reduction algorithms and transformations also possible in Java.","title":"Feature Engineering in Flink"},{"location":"concepts/fs/feature_group/fg_overview/","text":"As a programmer, you can consider a feature, in machine learning, to be a variable associated with some entity that contains a value that is useful for helping train a model to solve a prediction problem. That is, the feature is just a variable with predictive power for a machine learning problem, or task. A feature group is a table of features, where each feature group has a primary key, and optionally an event_time column (indicating when the features in that row were observed), and a partition key. Collectively, they are referred to as columns. The partition key determines how to layout the feature group rows on disk such that you can efficiently query the data using queries with the partition key. For example, if your partition key is the day and you have hundreds of days worth of data, with a partition key, you can query the day for only a given day or a range of days, and only the data for those days will be read from disk. Online and offline Storage # Feature groups can be stored in a low-latency \"online\" database and/or in low cost, high throughput \"offline\" storage, typically a data lake or data warehouse. The online store stores only the latest values of features for a feature group. It is used to serve pre-computed features to models at runtime. The offline store stores the historical values of features for a feature group, so it may store many times more data than the online store. Offline feature groups are used, typically, to create training data for models, but also to retrieve data for batch scoring of models:","title":"Overview"},{"location":"concepts/fs/feature_group/fg_overview/#online-and-offline-storage","text":"Feature groups can be stored in a low-latency \"online\" database and/or in low cost, high throughput \"offline\" storage, typically a data lake or data warehouse. The online store stores only the latest values of features for a feature group. It is used to serve pre-computed features to models at runtime. The offline store stores the historical values of features for a feature group, so it may store many times more data than the online store. Offline feature groups are used, typically, to create training data for models, but also to retrieve data for batch scoring of models:","title":"Online and offline Storage"},{"location":"concepts/fs/feature_group/fg_statistics/","text":"HSFS supports monitoring, validation, and alerting for features: transparently compute statistics over features on writing to a feature group; validation of data written to feature groups using Great Expectations alerting users when there was a problem writing or update features. Statistics # When you create a Feature Group in HSFS, you can configure it to compute statistics over the features inserted into the fFeature Group by setting the statistics_config dict parameter, see Feature Group Statistics for details. Every time you write to the Feature Group, new statistics will be computed over all of the data in the Feature Group. Data Validation # You can define expectation suites in Great Expectations and assoicate them with feature groups. When you write to a feature group, the expectations are executed, then you can define a policy on the feature group for what to do if any expectation fails. Alerting # HSFS also supports alerts, that can be triggered when there are problems in your feature pipelines, for example, when a write fails due to an error or a failed expectation. You can send alerts to different alerting endpoints, such as email or Slack, that can be configured in the Hopsworks UI. For example, you can send a slack message if features being written to a feature group are missing some input data.","title":"Data Validation/Stats/Alerts"},{"location":"concepts/fs/feature_group/fg_statistics/#statistics","text":"When you create a Feature Group in HSFS, you can configure it to compute statistics over the features inserted into the fFeature Group by setting the statistics_config dict parameter, see Feature Group Statistics for details. Every time you write to the Feature Group, new statistics will be computed over all of the data in the Feature Group.","title":"Statistics"},{"location":"concepts/fs/feature_group/fg_statistics/#data-validation","text":"You can define expectation suites in Great Expectations and assoicate them with feature groups. When you write to a feature group, the expectations are executed, then you can define a policy on the feature group for what to do if any expectation fails.","title":"Data Validation"},{"location":"concepts/fs/feature_group/fg_statistics/#alerting","text":"HSFS also supports alerts, that can be triggered when there are problems in your feature pipelines, for example, when a write fails due to an error or a failed expectation. You can send alerts to different alerting endpoints, such as email or Slack, that can be configured in the Hopsworks UI. For example, you can send a slack message if features being written to a feature group are missing some input data.","title":"Alerting"},{"location":"concepts/fs/feature_group/storage_connector/","text":"","title":"Storage connector"},{"location":"concepts/fs/feature_group/versioning/","text":"Data Versioning of a feature group involves tracking updates to the feature group, so that you can recover the state of the feature group at a given point-in-time in the past. The schema of feature groups is also versioned. If you make a breaking change to the schema of a feature group, you need to increment the version of the feature group, and then backfill the new feature group. A breaking schema change is when you: drop a column from the schema add a new fature without any default value for the new feature change how a feature is computed, such that, for training models, the data for the old feature is not compatible with the data for the new feature. For example, if you have an embedding as a feature and change the algorithm to compute that embedding, you probably should not mix feature values computed with the old embedding model with feature values computed with the new embedding model.","title":"Versioning"},{"location":"concepts/fs/feature_group/write_apis/","text":"You write to feature groups, and read from feature views. There are 3 APIs for writing to feature groups, as shown in the table below: Stream API Batch API Connector API Python X - - Spark X X - Flink X - - External Table - - X Stream API # The Stream API is the only API for Python and Flink clients, and is the preferred API for Spark, as it ensures consistent features between offline and online feature stores. The Stream API first writes data to be ingested to a Kafka topic, and then Hopsworks ensures that the data is synchronized to the Online and Offline Feature Groups through the OnlineFS service and Hudi DeltaStreamer jobs, respectively. The data in the feature groups is guaranteed to arrive at-most-once, through idempotent writes to the online feature group (only the latest values of features are stored there, and duplicates in Kafka only cause idempotent updates) and duplicate removal by Apache Hudi for the offline feature group. Batch API # For very large updates to feature groups, such as when you are backfilling large amounts of data to an offline feature group, it is often preferential to write directly to the Hudi tables in Hopsworks, instead of via Kafka - thus reducing write amplification. Spark clients can write directly to Hudi tables on Hopsworks with Hopsworks libraries and certificates using a HDFS API. This requires network connectivity between the Spark clients and the datanodes in Hopsworks. Connector API # Hopsworks supports external tables as feature groups. You can mount a table from an external database as an offline feature group using the Connector API - you create an external table using the connector. This enables you to use features from your external data source (Snowflake, Redshift, Delta Lake, etc) as you would any feature in an offline feature group in Hopsworks. You can, for example, join features from different feature groups (external or not) together to create feature views and training data for models.","title":"Write APIs"},{"location":"concepts/fs/feature_group/write_apis/#stream-api","text":"The Stream API is the only API for Python and Flink clients, and is the preferred API for Spark, as it ensures consistent features between offline and online feature stores. The Stream API first writes data to be ingested to a Kafka topic, and then Hopsworks ensures that the data is synchronized to the Online and Offline Feature Groups through the OnlineFS service and Hudi DeltaStreamer jobs, respectively. The data in the feature groups is guaranteed to arrive at-most-once, through idempotent writes to the online feature group (only the latest values of features are stored there, and duplicates in Kafka only cause idempotent updates) and duplicate removal by Apache Hudi for the offline feature group.","title":"Stream API"},{"location":"concepts/fs/feature_group/write_apis/#batch-api","text":"For very large updates to feature groups, such as when you are backfilling large amounts of data to an offline feature group, it is often preferential to write directly to the Hudi tables in Hopsworks, instead of via Kafka - thus reducing write amplification. Spark clients can write directly to Hudi tables on Hopsworks with Hopsworks libraries and certificates using a HDFS API. This requires network connectivity between the Spark clients and the datanodes in Hopsworks.","title":"Batch API"},{"location":"concepts/fs/feature_group/write_apis/#connector-api","text":"Hopsworks supports external tables as feature groups. You can mount a table from an external database as an offline feature group using the Connector API - you create an external table using the connector. This enables you to use features from your external data source (Snowflake, Redshift, Delta Lake, etc) as you would any feature in an offline feature group in Hopsworks. You can, for example, join features from different feature groups (external or not) together to create feature views and training data for models.","title":"Connector API"},{"location":"concepts/fs/feature_view/create_fv/","text":"","title":"Create fv"},{"location":"concepts/fs/feature_view/fv_overview/","text":"A feature view is a logical view over (or interface to) a set of features that may come from different feature groups. You create a feature view by joining together features from existing feature groups. In the illustration below, we can see that features are joined together from the two feature groups: seller_delivery_time_monthly and the seller_reviews_quarterly. You can also see that features in the feature view inherit not only the feature type from their feature groups, but also whether they are the primary key and/or the event_time. The image also includes transformation functions that are applied to individual features. Transformation functions are a part of the feature types included in the feature view. That is, a feature in a feature view is not only defined by its data type (int, string, etc) or its feature type (categorical, numerical, embedding), but also by its transformation. Feature views can also include: the label for the supervised ML problem transformation functions that should be applied to specified features consistently between training and serving the ability to create training data the ability to retrieve a feature vector with the most recent feature values In the flow chart below, we can see the decisions that can be taken when creating (1) a feature view, and (2) creating training data with the feature view. We can see here how the feature view is a representation for a model in the feature store - the same feature view is used to retrieve feature vectors for operational model that was created with training data from this feature view. As such, you can see that the most common use case for creating a feature view is to define the features that will be used in a model. In this way, feature views enable features from different feature groups to be reused across different models, and if features are stored untransfromed in feature groups, they become even more reusable, as different feature views can apply different transformations to the same feature.","title":"Overview"},{"location":"concepts/fs/feature_view/offline_api/","text":"The feature view provides an Offline API for creating training data creating batch (scoring) data Training Data # Training data is created using a feature view. You can create training data as either: in-memory Pandas DataFrames, useful when you have a small amount of training data; materialized training data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet). You can apply filters when creating training data from a feature view: start-time and end-time, for example, to create the train-set from an earlier time range, and the test-set from a later (unseen) time range; feature value features, for example, only train a model on customers from a particular country. Note that filters are not applied when retrieving feature vectors using feature views, as we only look up features for a specific entity, like a customer. In this case, the application should know that predictions for this customer should be made on the model trained on customers in USA, for example. Point-in-time Correct Training Data # When you create training data from features in different feature groups, it is possible that the feature groups are updated at different cadences. For example, maybe one feature group is updated hourly, while another feature group is updated daily. It is very complex to write code that joins features together from such feature groups and ensures there is no data leakage in the resultant training data. HSFS hides this complexity by performing the point-in-time JOIN transparently, similar to the illustration below: HSFS uses the event_time columns on both feature groups to determine the most recent (but not newer) feature values that are joined together with the feature values from the feature group containing the label. That is, the features in the feature group containinig the label are the observation times for the features in the resulting training data, and we want feature values from the other feature groups that have the most recent timestamps, but not newer than the timestamp in the label-containing feature group. Splitting Training Data # You can create random train/validation/test splits of your training data using the HSFS API. You can also time-based splits with the HSFS API. Evaluation Sets # Test data can also be split into evaluation sets to help evaluate a model for potential bias. First, you have to identify the classes of samples that could be at risk of bias, and generate evaluation sets from your unseen test set - one evaluation set for each group of samples at risk of bias. For example, if you have a feature group of users, where one of the features is gender, and you want to evaluate the risk of bias due to gender, you can use filters to generate 3 evaluation sets from your test set - one for male, female, and non-binary. Then you score your model against all 3 evaluation sets to ensure that the predicition performance is comparable and non-biased across all 3 gender. Batch (Scoring) Data # Batch data for scoring models is created using a feature view. Similar to training data, you can create batch data as either: in-memory Pandas DataFrames, useful when you have a small amount of data to score; materialized data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet) Batch data requires specification of a start_time for the start of the batch scoring data. You can also specify the end_time (default is the current date).","title":"Offline API"},{"location":"concepts/fs/feature_view/offline_api/#training-data","text":"Training data is created using a feature view. You can create training data as either: in-memory Pandas DataFrames, useful when you have a small amount of training data; materialized training data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet). You can apply filters when creating training data from a feature view: start-time and end-time, for example, to create the train-set from an earlier time range, and the test-set from a later (unseen) time range; feature value features, for example, only train a model on customers from a particular country. Note that filters are not applied when retrieving feature vectors using feature views, as we only look up features for a specific entity, like a customer. In this case, the application should know that predictions for this customer should be made on the model trained on customers in USA, for example.","title":"Training Data"},{"location":"concepts/fs/feature_view/offline_api/#point-in-time-correct-training-data","text":"When you create training data from features in different feature groups, it is possible that the feature groups are updated at different cadences. For example, maybe one feature group is updated hourly, while another feature group is updated daily. It is very complex to write code that joins features together from such feature groups and ensures there is no data leakage in the resultant training data. HSFS hides this complexity by performing the point-in-time JOIN transparently, similar to the illustration below: HSFS uses the event_time columns on both feature groups to determine the most recent (but not newer) feature values that are joined together with the feature values from the feature group containing the label. That is, the features in the feature group containinig the label are the observation times for the features in the resulting training data, and we want feature values from the other feature groups that have the most recent timestamps, but not newer than the timestamp in the label-containing feature group.","title":"Point-in-time Correct Training Data"},{"location":"concepts/fs/feature_view/offline_api/#splitting-training-data","text":"You can create random train/validation/test splits of your training data using the HSFS API. You can also time-based splits with the HSFS API.","title":"Splitting Training Data"},{"location":"concepts/fs/feature_view/offline_api/#evaluation-sets","text":"Test data can also be split into evaluation sets to help evaluate a model for potential bias. First, you have to identify the classes of samples that could be at risk of bias, and generate evaluation sets from your unseen test set - one evaluation set for each group of samples at risk of bias. For example, if you have a feature group of users, where one of the features is gender, and you want to evaluate the risk of bias due to gender, you can use filters to generate 3 evaluation sets from your test set - one for male, female, and non-binary. Then you score your model against all 3 evaluation sets to ensure that the predicition performance is comparable and non-biased across all 3 gender.","title":"Evaluation Sets"},{"location":"concepts/fs/feature_view/offline_api/#batch-scoring-data","text":"Batch data for scoring models is created using a feature view. Similar to training data, you can create batch data as either: in-memory Pandas DataFrames, useful when you have a small amount of data to score; materialized data in files, in a file format of your choice (such as .tfrecord, .csv, or .parquet) Batch data requires specification of a start_time for the start of the batch scoring data. You can also specify the end_time (default is the current date).","title":"Batch (Scoring) Data"},{"location":"concepts/fs/feature_view/online_api/","text":"A feature vector is a row of features (without the primary key(s) and event timestamp): The Feature View's online API provides methods to return an individual feature vector, or a batch of feature vectors, containing the latest feature values, given a valid primary key for the feature view. It may be the case that some features should not be retrieved from the feature store, but are supplied by the client. We call these 'passed' features and, similar to precomputed features from the feature store, they can also be transformed by the HSFS client in the method: feature_view.get_feature_vector( , passed={ .... })","title":"Online API"},{"location":"concepts/fs/feature_view/statistics/","text":"The feature view does not contain any statistics, as it is simply an interface consisting of a number of features and any transformation functions applied to those features. However, training data can have descriptive statistics over it computed by HSFS. Descriptive statistics for training data is important for model monitoring, as it can enable model monitoring. If you compute the same descriptive statistics over windows of input features to models, you can help determine when there is a significant change in the distribution of an input feature, so-called feature shift.","title":"Statistics"},{"location":"concepts/fs/feature_view/training_inference_pipelines/","text":"A training pipeline is a program that orchestrates the training of a machine learning model. For supervised machine learning, a training pipeline requires both features and labels, and these can typically be retrieved from the feature store as either in-memory Pandas DataFrames or read as training data files, created from the feature store. An inference pipeline is a program that takes user input, optionally enriches it with features from the feature store, and builds a feature vector (or batch of feature vectors) with with it uses a model to make a prediction. Transformations # Feature transformations are mathematical operations that change feature values with the goal of improving model convergence or performance properties. Transformation functions take as input a single value (or small number of values), they often require state (such as the mean value of a feature to normalize the input), and they output a single value or a list of values. Training Serving Skew # It is crucial that the transformations performed when creating features (for training or serving) are consistent - use the same code - to avoid training/serving skew. In the image below, you can see that transformations happen after the Feature Store, but that the implementation of the transformation functions need to be consistent between the training and inference pipelines. There are 3 main approaches to prevent training/serving skew that we support in Hopsworks. These are (1) perform transformations in models, (2) perform transformations in pipelines (sklearn, TF, PyTorch) and use the model registry to save the transformation pipeline so that the same transformation is used in your inference pipeline, and (3) use HSFS transformations, defined as UDFs in Python. Transformations as Pre-Processing Layers in Models # Transformation functions can be implemented as preprocessing steps within a model. For example, you can write a transformation function as a pre-processing layer in Keras/TensorFlow. When you save the model, the preprocessing steps will also be saved as part of the model. Any state required to compute the transformation, such as the arithmetic mean of a numerical feature in the train set, is also stored with the function, enabling consistent transformations during inference. When data preprocessing is part of the model, users can just send the untransformed feature values to the model and the model itself will apply any transformation functions as preprocessing layers (such as encoding categorical variables or normalizing numerical variables). Transformation Pipelines in Scikit-Learn/TensorFlow/PyTorch # You have to save your transformation pipeline (serialize the object or the parameters) and make sure you apply exactly the same transformations in your inference pipeline. This means you should version the transformations. In Hopsworks, you can store the transformations with your versioned models in the Model Registry, helping you to ensure the same transformation pipeline is applied to both training/serving for the same model version. Transformations as Python UDFs in HSFS # Hopsworks feature store also supports consistent transformation functions by enabling a Python UDF, that implements a transformation, to be attached a to feature in a feature view. When training data is created with a feature view or when a feature vector is retrieved from a feature view, HSFS ensures that any transformation functions defined over any features will be applied before returning feature values. You can use built-in transformation objects in HSFS or write your own custom transformation functions as Python UDFs. The benefit of this approach is that transformations are applied consistently when creating training data and when retrieving feature data from the online feature store. Transformations no longer need to be included in either your training pipeline or inference pipeline, as they are applied transparently when creating training data and retrieving feature vectors. Hopsworks uses Spark to create training data as files, and any transformation functions for features are executed as Python UDFs in Spark - enabling transformation functions to be applied on large volumes of data and removing potentially CPU-intensive transformations from training pipelines.","title":"Consistent Transformations"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations","text":"Feature transformations are mathematical operations that change feature values with the goal of improving model convergence or performance properties. Transformation functions take as input a single value (or small number of values), they often require state (such as the mean value of a feature to normalize the input), and they output a single value or a list of values.","title":"Transformations"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#training-serving-skew","text":"It is crucial that the transformations performed when creating features (for training or serving) are consistent - use the same code - to avoid training/serving skew. In the image below, you can see that transformations happen after the Feature Store, but that the implementation of the transformation functions need to be consistent between the training and inference pipelines. There are 3 main approaches to prevent training/serving skew that we support in Hopsworks. These are (1) perform transformations in models, (2) perform transformations in pipelines (sklearn, TF, PyTorch) and use the model registry to save the transformation pipeline so that the same transformation is used in your inference pipeline, and (3) use HSFS transformations, defined as UDFs in Python.","title":"Training Serving Skew"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations-as-pre-processing-layers-in-models","text":"Transformation functions can be implemented as preprocessing steps within a model. For example, you can write a transformation function as a pre-processing layer in Keras/TensorFlow. When you save the model, the preprocessing steps will also be saved as part of the model. Any state required to compute the transformation, such as the arithmetic mean of a numerical feature in the train set, is also stored with the function, enabling consistent transformations during inference. When data preprocessing is part of the model, users can just send the untransformed feature values to the model and the model itself will apply any transformation functions as preprocessing layers (such as encoding categorical variables or normalizing numerical variables).","title":"Transformations as Pre-Processing Layers in Models"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformation-pipelines-in-scikit-learntensorflowpytorch","text":"You have to save your transformation pipeline (serialize the object or the parameters) and make sure you apply exactly the same transformations in your inference pipeline. This means you should version the transformations. In Hopsworks, you can store the transformations with your versioned models in the Model Registry, helping you to ensure the same transformation pipeline is applied to both training/serving for the same model version.","title":"Transformation Pipelines in Scikit-Learn/TensorFlow/PyTorch"},{"location":"concepts/fs/feature_view/training_inference_pipelines/#transformations-as-python-udfs-in-hsfs","text":"Hopsworks feature store also supports consistent transformation functions by enabling a Python UDF, that implements a transformation, to be attached a to feature in a feature view. When training data is created with a feature view or when a feature vector is retrieved from a feature view, HSFS ensures that any transformation functions defined over any features will be applied before returning feature values. You can use built-in transformation objects in HSFS or write your own custom transformation functions as Python UDFs. The benefit of this approach is that transformations are applied consistently when creating training data and when retrieving feature data from the online feature store. Transformations no longer need to be included in either your training pipeline or inference pipeline, as they are applied transparently when creating training data and retrieving feature vectors. Hopsworks uses Spark to create training data as files, and any transformation functions for features are executed as Python UDFs in Spark - enabling transformation functions to be applied on large volumes of data and removing potentially CPU-intensive transformations from training pipelines.","title":"Transformations as Python UDFs in HSFS"},{"location":"concepts/fs/feature_view/versioning/","text":"Feature views are interfaces, and if there is a change in the interface (the types of the features, the transformations applied to the features), then you need to change the version, to prevent breaking existing clients. Training datasets are associated with a specific feature view version, and have their own version.","title":"Versioning"},{"location":"concepts/mlops/bi_tools/","text":"The Hopsworks Feature Store is based on an offline data store, queryable via an Apache Hive API, and an online data store, queryable via a MySQL Server API. Given that Feature Groups in Hopsworks have well-defined schemas, features in the Hopsworks Feature Store can be analyzed and reports can be generated from them using any BI Tools that include connectors for MySQL (JDBC) and Apache Hive (2-way TLS required). One platform we use with customers is Apache Superset , as it can be configured alongside Hopsworks to provide BI Tooling capabilities.","title":"BI Tools"},{"location":"concepts/mlops/kserve/","text":"KServe is an open-source framework for model serving on Kubernetes. In Hopsworks, you can easily deploy models from the model registry in KServe or in Docker containers (for Hopsworks Community). You can deploy models in either programs, using the HSML library, or in the UI. A KServe model deployment can include the following components: Transformer A pre-processing and post-processing component that can transform model inputs before predictions are made Predictor A predictor is a ML model in a Python object that takes a feature vector as input and returns a prediction as output Inference Logger Hopsworks logs inputs and outputs of transformers and predictors to a Kafka topic that is part of the same project as the model Inference Batcher Inference requests can be batched in different ways to adjust the trade-off between throughput and latencies of the model predictions Versioned Deployments Model deployments are versioned, enabling A/B testing and more. Istio Model Endpoint You can publish model via a REST Endpoint using Istio and access it over HTTP using a Hopsworks API key (with serving scope). Secure and authorized access is guaranteed by Hopsworks. Models deployed on KServe in Hopsworks can be easily integrated with the Hopsworks feature store using a Transformer Python script, that builds the predictor's input feature vector using the application input and pre-computed features from the feature store.","title":"Model Serving"},{"location":"concepts/mlops/mlops/","text":"","title":"Mlops"},{"location":"concepts/mlops/opensearch/","text":"Hopsworks includes OpenSearch as a multi-tenant service in projects. OpenSearch provides vector database capabilities through its k-NN plugin, that supports the FAISS and nsmlib embedding indexes. Through Hopsworks, OpenSearch also provides enterprise capabilities, including authentication and access control to indexes (an index can be private to a Hopsworks project), filtering, scalability, high availability, and disaster recovery support.","title":"Vector Database"},{"location":"concepts/mlops/prediction_services/","text":"A prediction service is an end-to-end analytical or operational machine learning system that takes in data and outputs predictions that are consumed by users of the prediction service. A prediction service consists of the following components: feature pipeline(s), training pipeline, inference pipeline (for either batch predictions or online predictions) a sink for predictions - either a store or a user-interface. Analytical ML # In the analytical ML figure below, we can see an analytical prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., hourly, daily), and a batch program also runs on a schedule, reading batch scoring data from the feature store, computing predictions for that data with an embedded model, and writing those predictions to a database sink, from where the predictions are used for (predictive, prescriptive) analytical reports and/or to AI-enable operational services. Operational ML # In the operational ML figure below, we can see an operational prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., streaming, hourly, daily), and the operational service sends prediction requests to a model deployed on KServe via its secured Istio endpoint. A deployed model on KServer handles the prediction request by first retrieving pre-computed features from the feature store for the given request, and then building a feature vector that is scored by the model. The prediction result is returned to the client (the operational service). KServe logs both the feature values and the prediction results back to Hopsworks for further analysis and to help create new training data. MLOps Flywheel # Once you have built your analytical or operational ML system, the MLOps flywheel is the path to building a self-managing system that automatically collects and processes feature logs, prediction logs, and outcomes to help create new training data for models. This enables a ML flywheel where new training data and insights are generated from your operational or analytical ML service, by feeding logs back into the feature store. More training data enables the training of bettter models, and with better models, you should hopefully improve you operational/batch services, so that you attract more clients, who in turn produce more data for training models. And, thus, the ML flywheel is bootstrapped and leads to a virtuous cycle of more data leading to better models and more models leading to more users, who produce more data, and so on.","title":"Prediction Services"},{"location":"concepts/mlops/prediction_services/#analytical-ml","text":"In the analytical ML figure below, we can see an analytical prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., hourly, daily), and a batch program also runs on a schedule, reading batch scoring data from the feature store, computing predictions for that data with an embedded model, and writing those predictions to a database sink, from where the predictions are used for (predictive, prescriptive) analytical reports and/or to AI-enable operational services.","title":"Analytical ML"},{"location":"concepts/mlops/prediction_services/#operational-ml","text":"In the operational ML figure below, we can see an operational prediction service, where feature pipelines update the feature store with new feature data, running at some schedule (e.g., streaming, hourly, daily), and the operational service sends prediction requests to a model deployed on KServe via its secured Istio endpoint. A deployed model on KServer handles the prediction request by first retrieving pre-computed features from the feature store for the given request, and then building a feature vector that is scored by the model. The prediction result is returned to the client (the operational service). KServe logs both the feature values and the prediction results back to Hopsworks for further analysis and to help create new training data.","title":"Operational ML"},{"location":"concepts/mlops/prediction_services/#mlops-flywheel","text":"Once you have built your analytical or operational ML system, the MLOps flywheel is the path to building a self-managing system that automatically collects and processes feature logs, prediction logs, and outcomes to help create new training data for models. This enables a ML flywheel where new training data and insights are generated from your operational or analytical ML service, by feeding logs back into the feature store. More training data enables the training of bettter models, and with better models, you should hopefully improve you operational/batch services, so that you attract more clients, who in turn produce more data for training models. And, thus, the ML flywheel is bootstrapped and leads to a virtuous cycle of more data leading to better models and more models leading to more users, who produce more data, and so on.","title":"MLOps Flywheel"},{"location":"concepts/mlops/registry/","text":"Hopsworks Model Registry is designed with specific support for KServe and MLOps, through versioning. It enables developers to publish, test, monitor, govern and share models for collaboration with other teams. The model registry is where developers publish their models during the experimentation phase. The model registry can also be used to share models with the team and stakeholders. Like other project-based multi-tenant services in Hopsworks, a model registry is private to a project. That means you can easily add a development, staging, and production model registry to a cluster, and implement CI/CD processes for transitioning a model from development to staging to production. The model registry for KServe's capability are shown in the diagram below: The model registry centralizes model management, enabling models to be securely accessed and governed. Models are more than just the model itself - the registry also stores sample data for testsing, configuration information, provenance information, environment variables, links to the code used to generate the model, the model version, and tags/descriptions). When you save a model, you can also save model metrics with the model, enabling users to understand, for example, performance of the model on test (or unseen) data. Model Package # A ML model consists of a number of different components in a model package: - Model Input/Output Schema - Model artifacts - Model version information - Model format (based on the ML framework used to train the model - e.g., .pkl or .tb files) You can also optionally include in your packaged model: - Sample data (used to test the model in KServe) - The source notebook/program/experiment used to create the model","title":"Model Registry"},{"location":"concepts/mlops/registry/#model-package","text":"A ML model consists of a number of different components in a model package: - Model Input/Output Schema - Model artifacts - Model version information - Model format (based on the ML framework used to train the model - e.g., .pkl or .tb files) You can also optionally include in your packaged model: - Sample data (used to test the model in KServe) - The source notebook/program/experiment used to create the model","title":"Model Package"},{"location":"concepts/mlops/training/","text":"Hopsworks supports running model training pipelines on any Python environment, whether on an external Python client or on a Hopsworks cluster. The outputs of a training pipeline are typically experiment results, including logs, and possibly a trained model. You can plugin your own experimentation tracking platform or model registry, or you can use Hopsworks. Training Pipelines on Hopsworks # If you train models with Hopsworks, you can setup CI/CD pipelines as shown below, where the experiments are tracked by Hopsworks, and any model created is published to a model registry. Each project has its own private model registry, so when you are working in a development project, you typically publish models to your project's private development registry, and if all model validation tests pass, and the model performance is good enough, the same training pipeline can be submitted via a CI/CD pipeline (e.g., Github push request) to a staging project, and the same procedure can be repeated to push the training pipeline to a production project. Hopsworks Model Registry and Model Serving capabilities can then be used to build a batch or online prediction service using the model.","title":"Model Training"},{"location":"concepts/mlops/training/#training-pipelines-on-hopsworks","text":"If you train models with Hopsworks, you can setup CI/CD pipelines as shown below, where the experiments are tracked by Hopsworks, and any model created is published to a model registry. Each project has its own private model registry, so when you are working in a development project, you typically publish models to your project's private development registry, and if all model validation tests pass, and the model performance is good enough, the same training pipeline can be submitted via a CI/CD pipeline (e.g., Github push request) to a staging project, and the same procedure can be repeated to push the training pipeline to a production project. Hopsworks Model Registry and Model Serving capabilities can then be used to build a batch or online prediction service using the model.","title":"Training Pipelines on Hopsworks"},{"location":"concepts/projects/cicd/","text":"You can setup traditional development, staging, and production environment in Hopsworks using Projects. A project enables you provide access control for the different environments - just like a GitHub repository, owners of projects can add and remove members of projects and assign different roles to project members - the \"data owner\" role can write to feature store, while a \"data scientist\" can only read from the feature store and create training data. Dev, Staging, Prod # You can create dev, staging, and prod projects - either on the same cluster, but mostly commonly, with production on its own cluster: Versioning # Hopsworks supports the versioning of ML assets, including: Feature Groups: the version of its schema - breaking schema changes require a new version and backfilling the new version; Feature Views: the version of its schema, and breaking schema changes only require a new version; Models: the version of a model; Deployments: the version of the deployment of a model - a model with the same version can be found in >1 deployment. Pytest for feature logic and feature pipeline tests # Pytest and Great Expectations can be used for testing feature pipelines. Pytest is used to test feature logic and for end-to-end feature pipeline tests, while Great Expectations is used for data validation tests. Here, we can see how a feature pipeline test uses sample data to compute features and validate they have been written successfully, first to a development feature store, and then they can be pushed to a staging feature store, before finally being promoted to production.","title":"CI/CD"},{"location":"concepts/projects/cicd/#dev-staging-prod","text":"You can create dev, staging, and prod projects - either on the same cluster, but mostly commonly, with production on its own cluster:","title":"Dev, Staging, Prod"},{"location":"concepts/projects/cicd/#versioning","text":"Hopsworks supports the versioning of ML assets, including: Feature Groups: the version of its schema - breaking schema changes require a new version and backfilling the new version; Feature Views: the version of its schema, and breaking schema changes only require a new version; Models: the version of a model; Deployments: the version of the deployment of a model - a model with the same version can be found in >1 deployment.","title":"Versioning"},{"location":"concepts/projects/cicd/#pytest-for-feature-logic-and-feature-pipeline-tests","text":"Pytest and Great Expectations can be used for testing feature pipelines. Pytest is used to test feature logic and for end-to-end feature pipeline tests, while Great Expectations is used for data validation tests. Here, we can see how a feature pipeline test uses sample data to compute features and validate they have been written successfully, first to a development feature store, and then they can be pushed to a staging feature store, before finally being promoted to production.","title":"Pytest for feature logic and feature pipeline tests"},{"location":"concepts/projects/governance/","text":"Hopsworks provides project-level multi-tenancy, a data mesh enabling technology. Think of it as a Github repository for your teams and ML assets. More specifically, a project is a sandbox for team members, ML assets (features, training data, models, vector database, model deployments), and optionally feature pipelines and training pipelines. The ML assets can only be accessed by project members, and there is role-based access control (RBAC) for project members within a project. Dev/Staging/Prod for Data # Projects enable you to define devlopment, staging, and even production projects on the same cluster. Often, companies deploy production projects on dedicated clusters, but development projects and staging projects on a shared cluster. This way, projects can be easily used to implement CI/CD workflows. Data Mesh of Feature Stores # Projects enable you to move beyond the traditional dev/staging/prod ownership model for data. Different teams or lines of business can have their own private feature stores, you can mix them with a group-wide feature store, and feature stores can be securely shared between teams/organizations. Effectively, you can have decentralized ownership of feature stores, with domain-specific projects, and each project managing its own feature pipelines. Hopsworks provides data/feature sharing support between these self-service projects. Audit Logs with REST API # Hopsworks stores audit logs for all calls on its REST API in its file system, HopsFS. The audit log can be used to analyze the historical usage of services by users.","title":"Governance"},{"location":"concepts/projects/governance/#devstagingprod-for-data","text":"Projects enable you to define devlopment, staging, and even production projects on the same cluster. Often, companies deploy production projects on dedicated clusters, but development projects and staging projects on a shared cluster. This way, projects can be easily used to implement CI/CD workflows.","title":"Dev/Staging/Prod for Data"},{"location":"concepts/projects/governance/#data-mesh-of-feature-stores","text":"Projects enable you to move beyond the traditional dev/staging/prod ownership model for data. Different teams or lines of business can have their own private feature stores, you can mix them with a group-wide feature store, and feature stores can be securely shared between teams/organizations. Effectively, you can have decentralized ownership of feature stores, with domain-specific projects, and each project managing its own feature pipelines. Hopsworks provides data/feature sharing support between these self-service projects.","title":"Data Mesh of Feature Stores"},{"location":"concepts/projects/governance/#audit-logs-with-rest-api","text":"Hopsworks stores audit logs for all calls on its REST API in its file system, HopsFS. The audit log can be used to analyze the historical usage of services by users.","title":"Audit Logs with REST API"},{"location":"concepts/projects/storage/","text":"Every project in Hopsworks has its own private assets: a Feature Store (including both Online and Offline Stores) a Filesystem subtree (all directory and files under /Projects/ /) a Model Registry Model Deployments Kafka topics OpenSearch indexes (including KNN indexes - the vector DB) a Hive Database Access control to these assets is controlled using project membership ACLs (access-control lists). Users in a project who have a Data Owner role have read/write access to these assets. Users in a project who have a Data Scientist role have mostly read-only access to these assets, with the exception of the ability to write to well-known directories (Resources, Jupyter, Logs). However, it is often desirable to share assets between projects, with read-only, read/write privileges, and to restrict the privileges to specific role (e.g., Data Owners) in the target project. In Hopsworks, you can explicity share assets between projects without copying the assets. Sharing is managed by ACLs in Hopsworks, see example below:","title":"Data Storage/Sharing"},{"location":"getting_started/fs/fs_gs/","text":"Getting Started: Feature Store #","title":"Getting Started: Feature Store"},{"location":"getting_started/fs/fs_gs/#getting-started-feature-store","text":"","title":"Getting Started: Feature Store"},{"location":"getting_started/fs/tutorials/aml/","text":"","title":"AML Tutorial"},{"location":"reference_guides/","text":".md-typeset h1 { font-size: 0em; } Hopsworks Feature Store (hsfs) Model Registry & Model Serving (hsml)","title":"<div class=\"dropdown\"><button class=\"dropbtn\"> API </button> <div id=\"myDropdown\" class=\"dropdown-content\"> <a id=\"hopsworks_api_link\" href=\"https://docs.hopsworks.ai/hopsworks-api/dev\">Hopsworks API</a> <a id=\"hsfs_api_link\" href=\"https://docs.hopsworks.ai/feature-store-api/dev\">Feature Store API</a> <a id=\"hsml_api_link\" href=\"https://docs.hopsworks.ai/machine-learning-api/dev\">MLOps API</a> </div></div>"},{"location":"setup_installation/aws/cluster_creation/","text":"Getting started with Hopsworks.ai (AWS) # This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai Step 1 starting to create a cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Create a Hopsworks cluster, general information Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Enable EBS encryption # Select the checkbox (5) to enable encryption of EBS drives and shapshots. After enabling, the KMS key to be used for encryption can be specified by its alias, ID or ARN. Leaving the KMS key unspecified results in the EC2 default encryption key being used. S3 bucket configuration # Enter the name of the S3 bucket (6) you want the cluster to store its data in. Note The S3 bucket you are using must be empty. Premium users have the option to use encrypted S3 buckets . To configure an encrypted bucket click on the Advanced tab and select the appropriate encryption type. Note Encryption must have been already enabled for the bucket We support the following encryption schemes: SSE-S3 SSE-KMS S3 managed key User managed key Users can also select an AWS canned ACLs for the objects: bucket-owner-full-control On the main page, you can also choose to aggregate logs in CloudWatch and to opt out of hopsworks.ai log collection. The first one is to aggregate the logs of services running in your cluster in CloudWatch service in your configured AWS account. This can be useful if you want to understand what is happening on your cluster, without having to ssh into the instances. The second one is for hopsworks.ai to collect logs about the services running in your cluster. These logs will help us improve our system and provide support. If you choose to opt-out from the log collection and need support you will have to provide us the log by yourself, which will slow down the support process. Step 3 workers configuration # In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand. Static workers configuration # You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration Autoscaling workers configuration # You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration Step 4 select an SSH key # When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in AWS refer to Create an ssh key Choose SSH key Step 5 select the Instance Profile # To let the cluster instances access the S3 bucket we need to attach an instance profile to the virtual machines. In this step, you choose which profile to use. This profile needs to have access right to the S3 bucket you selected in Step 2 . For more details on how to create the instance profile and give it access to the S3 bucket refer to Creating an instance profile and giving it access to the bucket Choose the instance profile Step 6 set the backup retention policy # To backup the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 7 Managed Containers # Hopsworks can integrate with Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR) to launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. For more detail on how to set up this integration refer to Integration with Amazon EKS and Amazon ECR . Add EKS cluster name Step 8 VPC selection # In this step, you can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the VPC for you, you can choose the CIDR block for this virtual network. Refer to Create a VPC for more details on how to create your own VPC in AWS. Choose a VPC Note If the VPC uses a custom domain read our guide on how to set this up Step 9 Availability Zone selection # If you selected an existing VPC in the previous step, this step lets you select which availability zone of this VPC to use. If you did not select an existing virtual network in the previous step Hopsworks.ai will create an availability zone for you. You can choose the CIDR block this subnet will use. Choose an availability zone Step 10 Security group selection # If you selected an existing VPC in the previous step, this step lets you select which security group to use. Note Hopsworks.ai require some rules for inbound and outbound traffic in your security group, for more details refer to inbound traffic rules and outbound traffic rules . Note Hopsworks.ai attaches a public ip to your cluster by default. However, you can disable this behavior by unchecking the Attach Public IP checkbox. Choose security group Limiting outbound traffic to Hopsworks.ai # Clusters created on Hopsworks.ai need to be able to send http requests to api.hopsworks.ai. If you have strict regulation regarding outbound traffic, you can enable the Use static IPs to communicate with Hopsworks.ai checkbox to get a list of IPs to be allowed as shown below: Enable static IPs Step 11 User management selection # In this step, you can choose which user management system to use. You have four choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 12 Managed RonDB # Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us . Step 13 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 14 add an init script to your instances. # In this step, you can enter an initialization script that will be run at startup on every instance. You can select whether this script will run before or after the VM configuration. Be cautious if you select to run it before the VM configuration as this might affect Cluster creation. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script Step 15 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Cluster Creation"},{"location":"setup_installation/aws/cluster_creation/#getting-started-with-hopsworksai-aws","text":"This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai","title":"Getting started with Hopsworks.ai (AWS)"},{"location":"setup_installation/aws/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"setup_installation/aws/cluster_creation/#step-2-setting-the-general-information","text":"Create a Hopsworks cluster, general information Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node .","title":"Step 2 setting the General information"},{"location":"setup_installation/aws/cluster_creation/#enable-ebs-encryption","text":"Select the checkbox (5) to enable encryption of EBS drives and shapshots. After enabling, the KMS key to be used for encryption can be specified by its alias, ID or ARN. Leaving the KMS key unspecified results in the EC2 default encryption key being used.","title":"Enable EBS encryption"},{"location":"setup_installation/aws/cluster_creation/#s3-bucket-configuration","text":"Enter the name of the S3 bucket (6) you want the cluster to store its data in. Note The S3 bucket you are using must be empty. Premium users have the option to use encrypted S3 buckets . To configure an encrypted bucket click on the Advanced tab and select the appropriate encryption type. Note Encryption must have been already enabled for the bucket We support the following encryption schemes: SSE-S3 SSE-KMS S3 managed key User managed key Users can also select an AWS canned ACLs for the objects: bucket-owner-full-control On the main page, you can also choose to aggregate logs in CloudWatch and to opt out of hopsworks.ai log collection. The first one is to aggregate the logs of services running in your cluster in CloudWatch service in your configured AWS account. This can be useful if you want to understand what is happening on your cluster, without having to ssh into the instances. The second one is for hopsworks.ai to collect logs about the services running in your cluster. These logs will help us improve our system and provide support. If you choose to opt-out from the log collection and need support you will have to provide us the log by yourself, which will slow down the support process.","title":"S3 bucket configuration"},{"location":"setup_installation/aws/cluster_creation/#step-3-workers-configuration","text":"In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand.","title":"Step 3 workers configuration"},{"location":"setup_installation/aws/cluster_creation/#static-workers-configuration","text":"You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration","title":"Static workers configuration"},{"location":"setup_installation/aws/cluster_creation/#autoscaling-workers-configuration","text":"You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration","title":"Autoscaling workers configuration"},{"location":"setup_installation/aws/cluster_creation/#step-4-select-an-ssh-key","text":"When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in AWS refer to Create an ssh key Choose SSH key","title":"Step 4 select an SSH key"},{"location":"setup_installation/aws/cluster_creation/#step-5-select-the-instance-profile","text":"To let the cluster instances access the S3 bucket we need to attach an instance profile to the virtual machines. In this step, you choose which profile to use. This profile needs to have access right to the S3 bucket you selected in Step 2 . For more details on how to create the instance profile and give it access to the S3 bucket refer to Creating an instance profile and giving it access to the bucket Choose the instance profile","title":"Step 5 select the Instance Profile"},{"location":"setup_installation/aws/cluster_creation/#step-6-set-the-backup-retention-policy","text":"To backup the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 6 set the backup retention policy"},{"location":"setup_installation/aws/cluster_creation/#step-7-managed-containers","text":"Hopsworks can integrate with Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR) to launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. For more detail on how to set up this integration refer to Integration with Amazon EKS and Amazon ECR . Add EKS cluster name","title":"Step 7 Managed Containers"},{"location":"setup_installation/aws/cluster_creation/#step-8-vpc-selection","text":"In this step, you can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the VPC for you, you can choose the CIDR block for this virtual network. Refer to Create a VPC for more details on how to create your own VPC in AWS. Choose a VPC Note If the VPC uses a custom domain read our guide on how to set this up","title":"Step 8 VPC selection"},{"location":"setup_installation/aws/cluster_creation/#step-9-availability-zone-selection","text":"If you selected an existing VPC in the previous step, this step lets you select which availability zone of this VPC to use. If you did not select an existing virtual network in the previous step Hopsworks.ai will create an availability zone for you. You can choose the CIDR block this subnet will use. Choose an availability zone","title":"Step 9 Availability Zone selection"},{"location":"setup_installation/aws/cluster_creation/#step-10-security-group-selection","text":"If you selected an existing VPC in the previous step, this step lets you select which security group to use. Note Hopsworks.ai require some rules for inbound and outbound traffic in your security group, for more details refer to inbound traffic rules and outbound traffic rules . Note Hopsworks.ai attaches a public ip to your cluster by default. However, you can disable this behavior by unchecking the Attach Public IP checkbox. Choose security group","title":"Step 10 Security group selection"},{"location":"setup_installation/aws/cluster_creation/#limiting-outbound-traffic-to-hopsworksai","text":"Clusters created on Hopsworks.ai need to be able to send http requests to api.hopsworks.ai. If you have strict regulation regarding outbound traffic, you can enable the Use static IPs to communicate with Hopsworks.ai checkbox to get a list of IPs to be allowed as shown below: Enable static IPs","title":"Limiting outbound traffic to Hopsworks.ai"},{"location":"setup_installation/aws/cluster_creation/#step-11-user-management-selection","text":"In this step, you can choose which user management system to use. You have four choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 11 User management selection"},{"location":"setup_installation/aws/cluster_creation/#step-12-managed-rondb","text":"Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us .","title":"Step 12 Managed RonDB"},{"location":"setup_installation/aws/cluster_creation/#step-13-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 13 add tags to your instances."},{"location":"setup_installation/aws/cluster_creation/#step-14-add-an-init-script-to-your-instances","text":"In this step, you can enter an initialization script that will be run at startup on every instance. You can select whether this script will run before or after the VM configuration. Be cautious if you select to run it before the VM configuration as this might affect Cluster creation. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script","title":"Step 14 add an init script to your instances."},{"location":"setup_installation/aws/cluster_creation/#step-15-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 15 Review and create"},{"location":"setup_installation/aws/custom_domain_name/","text":"Deploy in a VPC with a custom domain name # Some organizations follow network patterns which impose a specific domain name for Instances. In that case, the instance's hostname instead of ip-10-0-0-175.us-east-2.compute.internal would be ip-10-0-0-175.bar.foo The control plane at hopsworks.ai needs to be aware of such case in order to properly initialize the Cluster. Note This feature is enabled only upon request. If you want this feature to be enable for your account please contact sales There are multiple ways to use custom domain names in your organization which are beyond the scope of this guide. We assume your cloud/network team has already setup the infrastructure. If you are using a resolver such as Amazon Route 53 , it is advised to update the record sets automatically. See our guide below for more information. Set cluster domain name # If this feature is enabled for your account, then in the VPC selection step you will have the option to specify the custom domain name as shown in the figure below. VPC with custom domain name In this case, the hostname of the Instance would be INSTANCE_ID.dev.hopsworks.domain Hostnames must be resolvable by all Virtual Machines in the cluster. For that reason we suggest, if possible, to automatically register the hostnames with your DNS. In the following section we present an example of automatic name registration in Amazon Route 53 Auto registration with Amazon Route 53 # It is quite common for organizations in AWS to use Route 53 for DNS or for hosted zones. You can configure a cluster in hopsworks.ai to execute some custom initialization script before any other action. This script will be executed on all nodes of the cluster. Since the hostname of the VM is in the form of INSTANCE_ID.DOMAIN_NAME it is easy to automate the zone update. The script below creates an A record in a configured Route 53 hosted zone. Warning If you want the VM to register itself with Route 53 you must amend the Instance Profile with the following permissions { \"Sid\" : \"Route53RecordSet\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : \"arn:aws:route53:::hostedzone/YOUR_HOSTED_ZONE_ID\" }, { \"Sid\" : \"Route53GetChange\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:GetChange\" ], \"Resource\" : \"arn:aws:route53:::change/*\" } The following script will get the instance ID from the EC2 metadata server and add an A record to the hosted zone in Route53. Update the YOUR_HOSTED_ZONE_ID and YOUR_CUSTOM_DOMAIN_NAME to match yours. #!/usr/bin/env bash set -e HOSTED_ZONE_ID = YOUR_HOSTED_ZONE_ID ZONE = YOUR_CUSTOM_DOMAIN_NAME record_set_file = /tmp/record_set.json instance_id = $( curl --silent http://169.254.169.254/2016-09-02/meta-data/instance-id ) domain_name = \" ${ instance_id } . ${ ZONE } \" local_ip = $( curl --silent http://169.254.169.254/2016-09-02/meta-data/local-ipv4 ) cat << EOC | tee $record_set_file { \"Changes\": [ { \"Action\": \"UPSERT\", \"ResourceRecordSet\": { \"Name\": \"${domain_name}\", \"Type\": \"A\", \"TTL\": 60, \"ResourceRecords\": [ { \"Value\": \"${local_ip}\" } ] } } ] } EOC echo \"Adding A record ${ domain_name } -> ${ local_ip } to Hosted Zone ${ HOSTED_ZONE_ID } \" change_resource_id = $( aws route53 change-resource-record-sets --hosted-zone-id ${ HOSTED_ZONE_ID } --change-batch file:// ${ record_set_file } | jq -r '.ChangeInfo.Id' ) echo \"Change resource ID: ${ change_resource_id } \" aws route53 wait resource-record-sets-changed --id ${ change_resource_id } echo \"Added resource record set\" rm -f ${ record_set_file } Set VM initialization script # As a final step you need to configure the Cluster to use the script above during VM creation with the user init script option. Paste the script to the text box and make sure you select this script to be executed before anything else on the VM. Automatic domain name registration with Route53","title":"Deploy in a VPC with a custom domain name"},{"location":"setup_installation/aws/custom_domain_name/#deploy-in-a-vpc-with-a-custom-domain-name","text":"Some organizations follow network patterns which impose a specific domain name for Instances. In that case, the instance's hostname instead of ip-10-0-0-175.us-east-2.compute.internal would be ip-10-0-0-175.bar.foo The control plane at hopsworks.ai needs to be aware of such case in order to properly initialize the Cluster. Note This feature is enabled only upon request. If you want this feature to be enable for your account please contact sales There are multiple ways to use custom domain names in your organization which are beyond the scope of this guide. We assume your cloud/network team has already setup the infrastructure. If you are using a resolver such as Amazon Route 53 , it is advised to update the record sets automatically. See our guide below for more information.","title":"Deploy in a VPC with a custom domain name"},{"location":"setup_installation/aws/custom_domain_name/#set-cluster-domain-name","text":"If this feature is enabled for your account, then in the VPC selection step you will have the option to specify the custom domain name as shown in the figure below. VPC with custom domain name In this case, the hostname of the Instance would be INSTANCE_ID.dev.hopsworks.domain Hostnames must be resolvable by all Virtual Machines in the cluster. For that reason we suggest, if possible, to automatically register the hostnames with your DNS. In the following section we present an example of automatic name registration in Amazon Route 53","title":"Set cluster domain name"},{"location":"setup_installation/aws/custom_domain_name/#auto-registration-with-amazon-route-53","text":"It is quite common for organizations in AWS to use Route 53 for DNS or for hosted zones. You can configure a cluster in hopsworks.ai to execute some custom initialization script before any other action. This script will be executed on all nodes of the cluster. Since the hostname of the VM is in the form of INSTANCE_ID.DOMAIN_NAME it is easy to automate the zone update. The script below creates an A record in a configured Route 53 hosted zone. Warning If you want the VM to register itself with Route 53 you must amend the Instance Profile with the following permissions { \"Sid\" : \"Route53RecordSet\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:ChangeResourceRecordSets\" ], \"Resource\" : \"arn:aws:route53:::hostedzone/YOUR_HOSTED_ZONE_ID\" }, { \"Sid\" : \"Route53GetChange\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"route53:GetChange\" ], \"Resource\" : \"arn:aws:route53:::change/*\" } The following script will get the instance ID from the EC2 metadata server and add an A record to the hosted zone in Route53. Update the YOUR_HOSTED_ZONE_ID and YOUR_CUSTOM_DOMAIN_NAME to match yours. #!/usr/bin/env bash set -e HOSTED_ZONE_ID = YOUR_HOSTED_ZONE_ID ZONE = YOUR_CUSTOM_DOMAIN_NAME record_set_file = /tmp/record_set.json instance_id = $( curl --silent http://169.254.169.254/2016-09-02/meta-data/instance-id ) domain_name = \" ${ instance_id } . ${ ZONE } \" local_ip = $( curl --silent http://169.254.169.254/2016-09-02/meta-data/local-ipv4 ) cat << EOC | tee $record_set_file { \"Changes\": [ { \"Action\": \"UPSERT\", \"ResourceRecordSet\": { \"Name\": \"${domain_name}\", \"Type\": \"A\", \"TTL\": 60, \"ResourceRecords\": [ { \"Value\": \"${local_ip}\" } ] } } ] } EOC echo \"Adding A record ${ domain_name } -> ${ local_ip } to Hosted Zone ${ HOSTED_ZONE_ID } \" change_resource_id = $( aws route53 change-resource-record-sets --hosted-zone-id ${ HOSTED_ZONE_ID } --change-batch file:// ${ record_set_file } | jq -r '.ChangeInfo.Id' ) echo \"Change resource ID: ${ change_resource_id } \" aws route53 wait resource-record-sets-changed --id ${ change_resource_id } echo \"Added resource record set\" rm -f ${ record_set_file }","title":"Auto registration with Amazon Route 53"},{"location":"setup_installation/aws/custom_domain_name/#set-vm-initialization-script","text":"As a final step you need to configure the Cluster to use the script above during VM creation with the user init script option. Paste the script to the text box and make sure you select this script to be executed before anything else on the VM. Automatic domain name registration with Route53","title":"Set VM initialization script"},{"location":"setup_installation/aws/eks_ecr_integration/","text":"Integration with Amazon EKS and Amazon ECR # This guide shows how to create a cluster in Hopsworks.ai with integrated support for Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR). So that Hopsworks can launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. Warning In the current version, we don't support sharing EKS clusters between Hopsworks clusters. That is, an EKS cluster can be only used by one Hopsworks cluster. Step 1: Create an EKS cluster on AWS # If you have an existing EKS cluster, skip this step and go directly to Step 2. Amazon provides two getting started guides using AWS management console or eksctl to help you create an EKS cluster. The easiest way is to use the eksctl command. Step 1.1: Installing eksctl, aws, and kubectl # Follow the prerequisites section in getting started with eksctl to install aws, eksctl, and kubectl. Step 1.2: Create an EKS cluster using eksctl # You can create a sample EKS cluster with the name my-eks-cluster using Kubernetes version 1.17 with 2 managed nodes in the us-east-2 region by running the following command. For more details on the eksctl usage, check the eksctl documentation . eksctl create cluster --name my-eks-cluster --version 1 .17 --region us-east-2 --nodegroup-name my-nodes --nodes 2 --managed Output: [ \u2139 ] eksctl version 0 .26.0 [ \u2139 ] using region us-east-2 [ \u2139 ] setting availability zones to [ us-east-2b us-east-2a us-east-2c ] [ \u2139 ] subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19 [ \u2139 ] subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19 [ \u2139 ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19 [ \u2139 ] using Kubernetes version 1 .17 [ \u2139 ] creating EKS cluster \"my-eks-cluster\" in \"us-east-2\" region with managed nodes [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] CloudWatch logging will not be enabled for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] Kubernetes API endpoint access will use default of { publicAccess = true, privateAccess = false } for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] 2 sequential tasks: { create cluster control plane \"my-eks-cluster\" , 2 sequential sub-tasks: { no tasks, create managed nodegroup \"my-nodes\" } } [ \u2139 ] building cluster stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] building managed nodegroup stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] waiting for the control plane availability... [ \u2714 ] saved kubeconfig as \"/Users/maism/.kube/config\" [ \u2139 ] no tasks [ \u2714 ] all EKS cluster resources for \"my-eks-cluster\" have been created [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] waiting for at least 2 node ( s ) to become ready in \"my-nodes\" [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] kubectl command should work with \"/Users/maism/.kube/config\" , try 'kubectl get nodes' [ \u2714 ] EKS cluster \"my-eks-cluster\" in \"us-east-2\" region is ready Once the cluster is created, eksctl will write the cluster credentials for the newly created cluster to your local kubeconfig file (~/.kube/config). To test the cluster credentials, you can run the following command to get the list of nodes in the cluster. kubectl get nodes Output: NAME STATUS ROLES AGE VERSION ip-192-168-21-142.us-east-2.compute.internal Ready <none> 2m35s v1.17.9-eks-4c6976 ip-192-168-62-117.us-east-2.compute.internal Ready <none> 2m34s v1.17.9-eks-4c6976 Step 2: Create an instance profile role on AWS # You need to add permission to the instance profile you use for instances deployed by Hopsworks.ai to give them access to EKS and ECR. Go to the IAM service in the AWS management console , click Roles , search for your role, and click on it. Click on Add inline policy . Go to the JSON tab and replace the existing JSON permissions with the JSON permissions below.. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowPullMainImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/filebeat\" , \"arn:aws:ecr:*:*:repository/base\" ] }, { \"Sid\" : \"AllowCreateRespositry\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:CreateRepository\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowPushandPullImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:DeleteRepository\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowDescirbeEKS\" , \"Effect\" : \"Allow\" , \"Action\" : \"eks:DescribeCluster\" , \"Resource\" : \"arn:aws:eks:*:*:cluster/*\" } ] } Click on Review policy . Give a name to your policy and click on Create policy . Copy the Role ARN of your profile (not to be confused with the Instance Profile ARNs two lines bellow). Copy the *Role ARN* Step 3: Allow your role to use your EKS cluster # You need to configure your EKS cluster to accept connections from the role you created above. This is done by using the following kubectl command. For more details, check Managing users or IAM roles for your cluster . Note The kubectl edit command uses the vi editor by default, however, you can override this behavior by setting KUBE_EDITOR to your preferred editor . KUBE_EDITOR = \"vi\" kubectl edit configmap aws-auth -n kube-system Output: # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Follow the example below (lines 13-16) to add your role to mapRoles and assign the system:masters group to your role. Make sure to replace 'YOUR ROLE RoleARN' with the Role ARN you copied in the previous step before saving. Warning Make sure to keep the same formatting as in the example below. The configuration format is sensitive to indentation and copy-pasting does not always keep the correct indentation. # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} - groups: - system:masters rolearn: <YOUR ROLE RoleARN> username: hopsworks kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Once you are done with editing the configmap, save it and exit the editor. The output should be: configmap/aws-auth edited Step 4: Setup network connectivity # For Hopsworks to be able to start containers in the EKS cluster and for these containers to be able to use Hopsworks we need to establish network connectivity between Hopsworks and EKS. For this, we have two solutions. The first option ( A ) is to run Hopsworks and EKS in the same virtual network and security group. The second option ( B ) is to pair the EKS and Hopsworks virtual networks. If you choose this option, make sure to create the peering before starting the Hopsworks cluster as it connects to EKS at startup. Option A : run Hopsworks and EKS in the same virtual network. # Running EKS and Hopsworks in the same security group is the simplest of the two solutions when it comes to setting up the system. All you need to do is to open the ports needed by Hopsworks in the security group created by the EKS cluster. Then you can just select this security group during the Hopsworks cluster creation. We will now see how to open the ports for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. First, you need to get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Go to the Security Groups section of EC2 in the AWS management console and search for your security group using the id obtained above. Note the VPC ID , you will need it when creating the hopsworks cluster. Then, click on it then go to the Inbound rules tab and click on Edit inbound rules . You should now see the following screen. Edit inbound rules Add two rules for HTTP and HTTPS as follows: Edit inbound rules Click Save rules to save the updated rules to the security group. Option B : create a pairing between Hopsworks and EKS # To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. You can create the virtual network by following the AWS documentation steps to create the virtual private network . Make sure to configure your subnet to use an address space that does not overlap with the address space in the Kubernetes network. You then need to select or create a security group for the Hopsworks VPC. You can create the security group by following the steps in the AWS documentation . Remember to open the port for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. Once the Hopsworks VPC and security group are created you need to create a peering between the Hopsworks VPC and the EKS VPC. For this follow the AWS documentation here . Finally, you need to edit the security groups for the EKS cluster and for Hopsworks to allow full communication between both VPC. This can be done following the instruction in the AWS documentation . You can get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Step 5: Allow Hopsworks.ai to delete ECR repositories on your behalf # For hopsworks.ai to be able to clean up the ECR repo when terminating your hopsworks cluster, you need to add a new inline policy to the Cross-Account role or user connected to Hopsworks.ai , that you set up when connecting your AWS account to hopsworks.ai . Navigate to AWS management console , then click on Roles or Users depending on which connection method you have used in Hopsworks.ai, and then search for your role or user name and click on it. Go to the Permissions tab, click on Add inline policy and go to the JSON tab. Replace the existing JSON permissions with the JSON permissions below. Click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowDeletingECRRepositories\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:DeleteRepository\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] } ] } Step 6: Create a Hopsworks cluster with EKS and ECR support # In Hopsworks.ai, select Create cluster . Choose the region of your EKS cluster and fill in the name of your S3 bucket , then click Next: Create Hopsworks cluster Choose your preferred SSH key to use with the cluster, then click Next: Choose SSH key Choose the instance profile role that you have created in Step 2 (click on the refresh button if your instance profile is not in the list), then click Next: Choose instance profile role Choose the backup retention period and click Next: Choose the backup retention policy Choose Enabled to enable the use of Amazon EKS and ECR: Choose Enabled Add your EKS cluster name, then click Next: Add EKS cluster name If you followed option A when setting up the network Choose the VPC of your EKS cluster. Its name should have the form eksctl-YOUR-CLUSTER-NAME-cluster (click on the refresh button if the VPC is not in the list). If you followed option B choose the VPC you created during Step 4 . Choose VPC Choose any of the subnets in the VPC, then click Next. Note Avoid private subnets if you want to enjoy all the hopsworks features . Choose Subnet Choose the security group that you have updated/created in Step 4 , then click Next: Note If you followed option A select the Security Group with the same id as in Step 4 and NOT the ones containing ControlPlaneSecurity or ClusterSharedNode in their name. Choose Security Group Click Review and submit , then Create. Once the cluster is created, Hopsworks will use EKS to launch Python jobs, Jupyter servers, and ML model servings.","title":"EKS/ECR integration"},{"location":"setup_installation/aws/eks_ecr_integration/#integration-with-amazon-eks-and-amazon-ecr","text":"This guide shows how to create a cluster in Hopsworks.ai with integrated support for Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Registry (ECR). So that Hopsworks can launch Python jobs, Jupyter servers, and ML model servings on top of Amazon EKS. Warning In the current version, we don't support sharing EKS clusters between Hopsworks clusters. That is, an EKS cluster can be only used by one Hopsworks cluster.","title":"Integration with Amazon EKS and Amazon ECR"},{"location":"setup_installation/aws/eks_ecr_integration/#step-1-create-an-eks-cluster-on-aws","text":"If you have an existing EKS cluster, skip this step and go directly to Step 2. Amazon provides two getting started guides using AWS management console or eksctl to help you create an EKS cluster. The easiest way is to use the eksctl command.","title":"Step 1: Create an EKS cluster on AWS"},{"location":"setup_installation/aws/eks_ecr_integration/#step-11-installing-eksctl-aws-and-kubectl","text":"Follow the prerequisites section in getting started with eksctl to install aws, eksctl, and kubectl.","title":"Step 1.1: Installing eksctl, aws, and kubectl"},{"location":"setup_installation/aws/eks_ecr_integration/#step-12-create-an-eks-cluster-using-eksctl","text":"You can create a sample EKS cluster with the name my-eks-cluster using Kubernetes version 1.17 with 2 managed nodes in the us-east-2 region by running the following command. For more details on the eksctl usage, check the eksctl documentation . eksctl create cluster --name my-eks-cluster --version 1 .17 --region us-east-2 --nodegroup-name my-nodes --nodes 2 --managed Output: [ \u2139 ] eksctl version 0 .26.0 [ \u2139 ] using region us-east-2 [ \u2139 ] setting availability zones to [ us-east-2b us-east-2a us-east-2c ] [ \u2139 ] subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19 [ \u2139 ] subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19 [ \u2139 ] subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19 [ \u2139 ] using Kubernetes version 1 .17 [ \u2139 ] creating EKS cluster \"my-eks-cluster\" in \"us-east-2\" region with managed nodes [ \u2139 ] will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup [ \u2139 ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] CloudWatch logging will not be enabled for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] you can enable it with 'eksctl utils update-cluster-logging --region=us-east-2 --cluster=my-eks-cluster' [ \u2139 ] Kubernetes API endpoint access will use default of { publicAccess = true, privateAccess = false } for cluster \"my-eks-cluster\" in \"us-east-2\" [ \u2139 ] 2 sequential tasks: { create cluster control plane \"my-eks-cluster\" , 2 sequential sub-tasks: { no tasks, create managed nodegroup \"my-nodes\" } } [ \u2139 ] building cluster stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-cluster\" [ \u2139 ] building managed nodegroup stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] deploying stack \"eksctl-my-eks-cluster-nodegroup-my-nodes\" [ \u2139 ] waiting for the control plane availability... [ \u2714 ] saved kubeconfig as \"/Users/maism/.kube/config\" [ \u2139 ] no tasks [ \u2714 ] all EKS cluster resources for \"my-eks-cluster\" have been created [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] waiting for at least 2 node ( s ) to become ready in \"my-nodes\" [ \u2139 ] nodegroup \"my-nodes\" has 2 node ( s ) [ \u2139 ] node \"ip-192-168-21-142.us-east-2.compute.internal\" is ready [ \u2139 ] node \"ip-192-168-62-117.us-east-2.compute.internal\" is ready [ \u2139 ] kubectl command should work with \"/Users/maism/.kube/config\" , try 'kubectl get nodes' [ \u2714 ] EKS cluster \"my-eks-cluster\" in \"us-east-2\" region is ready Once the cluster is created, eksctl will write the cluster credentials for the newly created cluster to your local kubeconfig file (~/.kube/config). To test the cluster credentials, you can run the following command to get the list of nodes in the cluster. kubectl get nodes Output: NAME STATUS ROLES AGE VERSION ip-192-168-21-142.us-east-2.compute.internal Ready <none> 2m35s v1.17.9-eks-4c6976 ip-192-168-62-117.us-east-2.compute.internal Ready <none> 2m34s v1.17.9-eks-4c6976","title":"Step 1.2: Create an EKS cluster using eksctl"},{"location":"setup_installation/aws/eks_ecr_integration/#step-2-create-an-instance-profile-role-on-aws","text":"You need to add permission to the instance profile you use for instances deployed by Hopsworks.ai to give them access to EKS and ECR. Go to the IAM service in the AWS management console , click Roles , search for your role, and click on it. Click on Add inline policy . Go to the JSON tab and replace the existing JSON permissions with the JSON permissions below.. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowPullMainImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/filebeat\" , \"arn:aws:ecr:*:*:repository/base\" ] }, { \"Sid\" : \"AllowCreateRespositry\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:CreateRepository\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowPushandPullImages\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:GetDownloadUrlForLayer\" , \"ecr:BatchGetImage\" , \"ecr:CompleteLayerUpload\" , \"ecr:UploadLayerPart\" , \"ecr:InitiateLayerUpload\" , \"ecr:DeleteRepository\" , \"ecr:BatchCheckLayerAvailability\" , \"ecr:PutImage\" , \"ecr:ListImages\" , \"ecr:BatchDeleteImage\" , \"ecr:GetLifecyclePolicy\" , \"ecr:PutLifecyclePolicy\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] }, { \"Sid\" : \"AllowGetAuthToken\" , \"Effect\" : \"Allow\" , \"Action\" : \"ecr:GetAuthorizationToken\" , \"Resource\" : \"*\" }, { \"Sid\" : \"AllowDescirbeEKS\" , \"Effect\" : \"Allow\" , \"Action\" : \"eks:DescribeCluster\" , \"Resource\" : \"arn:aws:eks:*:*:cluster/*\" } ] } Click on Review policy . Give a name to your policy and click on Create policy . Copy the Role ARN of your profile (not to be confused with the Instance Profile ARNs two lines bellow). Copy the *Role ARN*","title":"Step 2: Create an instance profile role on AWS"},{"location":"setup_installation/aws/eks_ecr_integration/#step-3-allow-your-role-to-use-your-eks-cluster","text":"You need to configure your EKS cluster to accept connections from the role you created above. This is done by using the following kubectl command. For more details, check Managing users or IAM roles for your cluster . Note The kubectl edit command uses the vi editor by default, however, you can override this behavior by setting KUBE_EDITOR to your preferred editor . KUBE_EDITOR = \"vi\" kubectl edit configmap aws-auth -n kube-system Output: # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Follow the example below (lines 13-16) to add your role to mapRoles and assign the system:masters group to your role. Make sure to replace 'YOUR ROLE RoleARN' with the Role ARN you copied in the previous step before saving. Warning Make sure to keep the same formatting as in the example below. The configuration format is sensitive to indentation and copy-pasting does not always keep the correct indentation. # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxxxx:role/eksctl-my-eks-cluster-nodegroup-m-NodeInstanceRole-FQ7L0HQI4NCC username: system:node: {{ EC2PrivateDNSName }} - groups: - system:masters rolearn: <YOUR ROLE RoleARN> username: hopsworks kind: ConfigMap metadata: creationTimestamp: \"2020-08-24T07:42:31Z\" name: aws-auth namespace: kube-system resourceVersion: \"770\" selfLink: /api/v1/namespaces/kube-system/configmaps/aws-auth uid: c794b2d8-9f10-443d-9072-c65d0f2eb552 Once you are done with editing the configmap, save it and exit the editor. The output should be: configmap/aws-auth edited","title":"Step 3: Allow your role to use your EKS cluster"},{"location":"setup_installation/aws/eks_ecr_integration/#step-4-setup-network-connectivity","text":"For Hopsworks to be able to start containers in the EKS cluster and for these containers to be able to use Hopsworks we need to establish network connectivity between Hopsworks and EKS. For this, we have two solutions. The first option ( A ) is to run Hopsworks and EKS in the same virtual network and security group. The second option ( B ) is to pair the EKS and Hopsworks virtual networks. If you choose this option, make sure to create the peering before starting the Hopsworks cluster as it connects to EKS at startup.","title":"Step 4: Setup network connectivity"},{"location":"setup_installation/aws/eks_ecr_integration/#option-a-run-hopsworks-and-eks-in-the-same-virtual-network","text":"Running EKS and Hopsworks in the same security group is the simplest of the two solutions when it comes to setting up the system. All you need to do is to open the ports needed by Hopsworks in the security group created by the EKS cluster. Then you can just select this security group during the Hopsworks cluster creation. We will now see how to open the ports for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. First, you need to get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\" Go to the Security Groups section of EC2 in the AWS management console and search for your security group using the id obtained above. Note the VPC ID , you will need it when creating the hopsworks cluster. Then, click on it then go to the Inbound rules tab and click on Edit inbound rules . You should now see the following screen. Edit inbound rules Add two rules for HTTP and HTTPS as follows: Edit inbound rules Click Save rules to save the updated rules to the security group.","title":"Option A: run Hopsworks and EKS in the same virtual network."},{"location":"setup_installation/aws/eks_ecr_integration/#option-b-create-a-pairing-between-hopsworks-and-eks","text":"To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. You can create the virtual network by following the AWS documentation steps to create the virtual private network . Make sure to configure your subnet to use an address space that does not overlap with the address space in the Kubernetes network. You then need to select or create a security group for the Hopsworks VPC. You can create the security group by following the steps in the AWS documentation . Remember to open the port for HTTP (80) and HTTPS (443) to allow Hopsworks to run with all its functionalities. Note It is possible not to open ports 80 and 443 at the cost of some features. See Limiting permissions for more details. Once the Hopsworks VPC and security group are created you need to create a peering between the Hopsworks VPC and the EKS VPC. For this follow the AWS documentation here . Finally, you need to edit the security groups for the EKS cluster and for Hopsworks to allow full communication between both VPC. This can be done following the instruction in the AWS documentation . You can get the name of the security group of your EKS cluster by using the following eksctl command. Notice that you need to replace my-eks-cluster with the name of your cluster. eksctl utils describe-stacks --region = us-east-2 --cluster = my-eks-cluster | grep 'OutputKey: \"ClusterSecurityGroupId\"' -a1 Check the output for OutputValue , which will be the id of your EKS security group. ExportName: \"eksctl-my-eks-cluster-cluster::ClusterSecurityGroupId\" , OutputKey: \"ClusterSecurityGroupId\" , OutputValue: \"YOUR_EKS_SECURITY_GROUP_ID\"","title":"Option B: create a pairing between Hopsworks and EKS"},{"location":"setup_installation/aws/eks_ecr_integration/#step-5-allow-hopsworksai-to-delete-ecr-repositories-on-your-behalf","text":"For hopsworks.ai to be able to clean up the ECR repo when terminating your hopsworks cluster, you need to add a new inline policy to the Cross-Account role or user connected to Hopsworks.ai , that you set up when connecting your AWS account to hopsworks.ai . Navigate to AWS management console , then click on Roles or Users depending on which connection method you have used in Hopsworks.ai, and then search for your role or user name and click on it. Go to the Permissions tab, click on Add inline policy and go to the JSON tab. Replace the existing JSON permissions with the JSON permissions below. Click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowDeletingECRRepositories\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ecr:DeleteRepository\" ], \"Resource\" : [ \"arn:aws:ecr:*:*:repository/*/filebeat\" , \"arn:aws:ecr:*:*:repository/*/base\" ] } ] }","title":"Step 5: Allow Hopsworks.ai to delete ECR repositories on your behalf"},{"location":"setup_installation/aws/eks_ecr_integration/#step-6-create-a-hopsworks-cluster-with-eks-and-ecr-support","text":"In Hopsworks.ai, select Create cluster . Choose the region of your EKS cluster and fill in the name of your S3 bucket , then click Next: Create Hopsworks cluster Choose your preferred SSH key to use with the cluster, then click Next: Choose SSH key Choose the instance profile role that you have created in Step 2 (click on the refresh button if your instance profile is not in the list), then click Next: Choose instance profile role Choose the backup retention period and click Next: Choose the backup retention policy Choose Enabled to enable the use of Amazon EKS and ECR: Choose Enabled Add your EKS cluster name, then click Next: Add EKS cluster name If you followed option A when setting up the network Choose the VPC of your EKS cluster. Its name should have the form eksctl-YOUR-CLUSTER-NAME-cluster (click on the refresh button if the VPC is not in the list). If you followed option B choose the VPC you created during Step 4 . Choose VPC Choose any of the subnets in the VPC, then click Next. Note Avoid private subnets if you want to enjoy all the hopsworks features . Choose Subnet Choose the security group that you have updated/created in Step 4 , then click Next: Note If you followed option A select the Security Group with the same id as in Step 4 and NOT the ones containing ControlPlaneSecurity or ClusterSharedNode in their name. Choose Security Group Click Review and submit , then Create. Once the cluster is created, Hopsworks will use EKS to launch Python jobs, Jupyter servers, and ML model servings.","title":"Step 6: Create a Hopsworks cluster with EKS and ECR support"},{"location":"setup_installation/aws/getting_started/","text":"Getting started with Hopsworks.ai (AWS) # Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's AWS account. Step 1: Connecting your AWS account # Hopsworks.ai deploys Hopsworks clusters to your AWS account. To enable this you have to permit us to do so. This can be either achieved by using AWS cross-account roles or AWS access keys. We strongly recommend the usage of cross-account roles whenever possible due to security reasons. Option 1: Using AWS Cross-Account Roles # To create a cross-account role for Hopsworks.ai, you need our AWS account id and the external id we created for you. You can find this information on the first screen of the cross-account configuration flow. Take note of the account id and external id and go to the Roles section of the IAM service in the AWS Management Console and select Create role . Creating the cross-account role instructions Select Another AWS account as trusted entity and fill in our AWS account id and the external id generated for you: Creating the cross-account role step 1 Go to the last step of the wizard, name the role and create it: Creating the cross-account role step 2 As a next step, you need to create an access policy to give Hopsworks.ai permissions to manage clusters in your organization's AWS account. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . Copy the permission JSON from the instructions: Adding the policy instructions Identify your newly created cross-account role in the Roles section of the IAM service in the AWS Management Console and select Add inline policy : Adding the inline policy step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 Copy the Role ARN from the summary of your cross-account role: Adding the inline policy step 4 Paste the Role ARN into Hopsworks.ai and click on Finish : Saving the cross-account role Option 2: Using AWS Access Keys # You can either create a new IAM user or use an existing IAM user to create access keys for Hopsworks.ai. If you want to create a new IAM user, see Creating an IAM User in Your AWS Account . Warning We recommend using Cross-Account Roles instead of Access Keys whenever possible, see Option 1: Using AWS Cross-Account Roles . Hopsworks.ai requires a set of permissions to be able to launch clusters in your AWS account. The permissions can be granted by attaching an access policy to your IAM user. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . The required permissions are shown in the instructions. Copy them if you want to create a new access policy: Configuring access key instructions Add a new Inline policy to your AWS user: Configuring the access key on AWS step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 In the overview of your IAM user, select Create access key : Configuring the access key on AWS step 2 Copy the Access Key ID and the Secret Access Key : Configuring the access key on AWS step 3 Paste the Access Key ID and the Secret Access Key into Hopsworks.ai and click on Finish : Saving the access key pair Step 2: Creating Instance profile # Note If you prefer using terraform, you can skip this step and the remaining steps, and instead follow this guide . Hopsworks cluster nodes need access to certain resources such as S3 bucket and CloudWatch. Follow the instructions in this guide to create an IAM instance profile with access to your S3 bucket: Guide When creating the policy, paste the following in the JSON tab. Step 3: Creating storage # The Hopsworks clusters deployed by hopsworks.ai store their data in an S3 bucket in your AWS account. To enable this you need to create an S3 bucket and an instance profile to give cluster nodes access to the bucket. Proceed to the S3 Management Console and click on Create bucket : Create an S3 bucket Name your bucket and select the region where your Hopsworks cluster will run. Click on Create bucket at the bottom of the page. Create an S3 bucket Step 4: Create an SSH key # When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your AWS EC2 environment. This can be done in two ways: creating a new key pair or importing an existing key pair . Step 4.1: Create a new key pair # Proceed to Key pairs in the EC2 console and click on Create key pair Create a key pair Name your key, select the file format you prefer and click on Create key pair . Create a key pair Step 4.2: Import a key pair # Proceed to Key pairs in the EC2 console , click on Action and click on Import key pair Import a key pair Name your key pair, upload your public key and click on Import key pair . Import a key pair Step 5: Deploying a Hopsworks cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Enter the name of the S3 bucket (5) you created above in S3 bucket . Note The S3 bucket you are using must be empty. Press Next : Create a Hopsworks cluster, general information Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the Instance Profile that you created above: Choose the instance profile To backup the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in day and click on Review and submit : Choose the backup retention policy Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster Step 6: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Getting Started"},{"location":"setup_installation/aws/getting_started/#getting-started-with-hopsworksai-aws","text":"Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's AWS account.","title":"Getting started with Hopsworks.ai (AWS)"},{"location":"setup_installation/aws/getting_started/#step-1-connecting-your-aws-account","text":"Hopsworks.ai deploys Hopsworks clusters to your AWS account. To enable this you have to permit us to do so. This can be either achieved by using AWS cross-account roles or AWS access keys. We strongly recommend the usage of cross-account roles whenever possible due to security reasons.","title":"Step 1: Connecting your AWS account"},{"location":"setup_installation/aws/getting_started/#option-1-using-aws-cross-account-roles","text":"To create a cross-account role for Hopsworks.ai, you need our AWS account id and the external id we created for you. You can find this information on the first screen of the cross-account configuration flow. Take note of the account id and external id and go to the Roles section of the IAM service in the AWS Management Console and select Create role . Creating the cross-account role instructions Select Another AWS account as trusted entity and fill in our AWS account id and the external id generated for you: Creating the cross-account role step 1 Go to the last step of the wizard, name the role and create it: Creating the cross-account role step 2 As a next step, you need to create an access policy to give Hopsworks.ai permissions to manage clusters in your organization's AWS account. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . Copy the permission JSON from the instructions: Adding the policy instructions Identify your newly created cross-account role in the Roles section of the IAM service in the AWS Management Console and select Add inline policy : Adding the inline policy step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 Copy the Role ARN from the summary of your cross-account role: Adding the inline policy step 4 Paste the Role ARN into Hopsworks.ai and click on Finish : Saving the cross-account role","title":"Option 1: Using AWS Cross-Account Roles"},{"location":"setup_installation/aws/getting_started/#option-2-using-aws-access-keys","text":"You can either create a new IAM user or use an existing IAM user to create access keys for Hopsworks.ai. If you want to create a new IAM user, see Creating an IAM User in Your AWS Account . Warning We recommend using Cross-Account Roles instead of Access Keys whenever possible, see Option 1: Using AWS Cross-Account Roles . Hopsworks.ai requires a set of permissions to be able to launch clusters in your AWS account. The permissions can be granted by attaching an access policy to your IAM user. By default, Hopsworks.ai is automating all steps required to launch a new Hopsworks cluster. If you want to limit the required AWS permissions, see restrictive-permissions . The required permissions are shown in the instructions. Copy them if you want to create a new access policy: Configuring access key instructions Add a new Inline policy to your AWS user: Configuring the access key on AWS step 1 Replace the JSON policy with the JSON from our instructions and continue in the wizard: Adding the inline policy step 2 Name and create the policy: Adding the inline policy step 3 In the overview of your IAM user, select Create access key : Configuring the access key on AWS step 2 Copy the Access Key ID and the Secret Access Key : Configuring the access key on AWS step 3 Paste the Access Key ID and the Secret Access Key into Hopsworks.ai and click on Finish : Saving the access key pair","title":"Option 2: Using AWS Access Keys"},{"location":"setup_installation/aws/getting_started/#step-2-creating-instance-profile","text":"Note If you prefer using terraform, you can skip this step and the remaining steps, and instead follow this guide . Hopsworks cluster nodes need access to certain resources such as S3 bucket and CloudWatch. Follow the instructions in this guide to create an IAM instance profile with access to your S3 bucket: Guide When creating the policy, paste the following in the JSON tab.","title":"Step 2: Creating Instance profile"},{"location":"setup_installation/aws/getting_started/#step-3-creating-storage","text":"The Hopsworks clusters deployed by hopsworks.ai store their data in an S3 bucket in your AWS account. To enable this you need to create an S3 bucket and an instance profile to give cluster nodes access to the bucket. Proceed to the S3 Management Console and click on Create bucket : Create an S3 bucket Name your bucket and select the region where your Hopsworks cluster will run. Click on Create bucket at the bottom of the page. Create an S3 bucket","title":"Step 3: Creating storage"},{"location":"setup_installation/aws/getting_started/#step-4-create-an-ssh-key","text":"When deploying clusters, Hopsworks.ai installs an ssh key on the cluster's instances so that you can access them if necessary. For this purpose, you need to add an ssh key to your AWS EC2 environment. This can be done in two ways: creating a new key pair or importing an existing key pair .","title":"Step 4: Create an SSH key"},{"location":"setup_installation/aws/getting_started/#step-41-create-a-new-key-pair","text":"Proceed to Key pairs in the EC2 console and click on Create key pair Create a key pair Name your key, select the file format you prefer and click on Create key pair . Create a key pair","title":"Step 4.1: Create a new key pair"},{"location":"setup_installation/aws/getting_started/#step-42-import-a-key-pair","text":"Proceed to Key pairs in the EC2 console , click on Action and click on Import key pair Import a key pair Name your key pair, upload your public key and click on Import key pair . Import a key pair","title":"Step 4.2: Import a key pair"},{"location":"setup_installation/aws/getting_started/#step-5-deploying-a-hopsworks-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Region in which you want your cluster to run (1), name your cluster (2). Select the Instance type (3) and Local storage (4) size for the cluster Head node . Enter the name of the S3 bucket (5) you created above in S3 bucket . Note The S3 bucket you are using must be empty. Press Next : Create a Hopsworks cluster, general information Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the Instance Profile that you created above: Choose the instance profile To backup the S3 bucket data when taking a cluster backup we need to set a retention policy for S3. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in day and click on Review and submit : Choose the backup retention policy Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 5: Deploying a Hopsworks cluster"},{"location":"setup_installation/aws/getting_started/#step-6-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Step 6: Next steps"},{"location":"setup_installation/aws/instance_profile_permissions/","text":"Replace BUCKET_NAME with the appropriate S3 bucket name. Note Some of these permissions can be removed. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"hopsworksaiInstanceProfile\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"S3:PutObject\" , \"S3:ListBucket\" , \"S3:GetBucketLocation\" , \"S3:GetObject\" , \"S3:DeleteObject\" , \"S3:AbortMultipartUpload\" , \"S3:ListBucketMultipartUploads\" , \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:GetBucketVersioning\" , \"S3:ListBucketVersions\" , \"S3:DeleteObjectVersion\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" , \"arn:aws:s3:::BUCKET_NAME\" ] }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }, { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] }","title":"Instance profile permissions"},{"location":"setup_installation/aws/restrictive_permissions/","text":"Limiting AWS permissions # Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s AWS account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down AWS permissions following the IT security policy principle of least privilege allowing Hopsworks.ai to only access resources in a specific VPC. Limiting the cross-account role permissions # Step 1: Create a VPC # To restrict Hopsworks.ai from accessing resources outside of a specific VPC, you need to create a new VPC connected to an Internet Gateway. This can be achieved in the AWS Management Console following this guide: Create the VPC . The option VPC with a Single Public Subnet from the Launch VPC Wizard should work out of the box. Alternatively, an existing VPC such as the default VPC can be used and Hopsworks.ai will be restricted to this VPC. Note the VPC ID of the VPC you want to use for the following steps. Note Make sure you enable DNS hostnames for your VPC After you have created the VPC either Create a Security Group or use VPC's default. Make sure that the VPC allow the following traffic. Inbound traffic # The Security Group and/or Network ACLs need to be configured so that at least port 80 is reachable from the internet otherwise you will have to use self signed certificate in your Hopsworks cluster. It is imperative the Security Group allows Inbound traffic from any Instance within the same Security Group in any (TCP) port. All VMs of the Cluster should be able to communicate with each other. Outbound traffic # Clusters created on Hopsworks.ai need to be able to send http requests to api.hopsworks.ai . The api.hopsworks.ai domain use a content delivery network for better performance. This result in the impossibility to predict which IP the request will be sent to. If you require a list of static IPs to allow outbound traffic from your security group, use the static IPs option during cluster creation . Similar to Inbound traffic, the Security Group in place must allow Outbound traffic in any (TCP) port towards any VM withing the same Security Group. Note If you intend to use the managed users option on your Hopsworks cluster you should also allow outbound traffic to cognito-idp.us-east-2.amazonaws.com and managedhopsworks-prod.auth.us-east-2.amazoncognito.com . Step 2: Create an instance profile # You need to create an instance profile that will identify all instances started by Hopsworks.ai. Follow this guide to create a role to be used by EC2 with no permissions attached: Creating a Role for an AWS Service (Console) . Take note of the ARN of the role you just created. You will need to add permissions to the instance profile to give access to the S3 bucket where Hopsworks will store its data. For more details about these permissions check our guide here . Check bellow for more information on restricting the permissions given the instance profile. Step 3: Set permissions of the cross-account role # During the account setup for Hopsworks.ai, you were asked to create and provide a cross-account role. If you don\u2019t remember which role you used then you can find it in Settings/Account Settings in Hopsworks.ai. Edit this role in the AWS Management Console and overwrite the existing inline policy with the following policy. Note that you have to replace [INSTANCE_PROFILE_NAME] and [VPC_ID] for multiple occurrences in the given policy. If you want to learn more about how this policy works check out: How to Help Lock Down a User\u2019s Amazon EC2 Capabilities to a Single VPC . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::*:role/[INSTANCE_PROFILE_NAME]\" }, { \"Sid\" : \"EC2RunInstancesOnlyWithGivenRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"EC2RunInstancesOnlyInGivenVpc\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:subnet/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"RemainingRunInstancePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:*:volume/*\" , \"arn:aws:ec2:*::image/*\" , \"arn:aws:ec2:*::snapshot/*\" , \"arn:aws:ec2:*:*:network-interface/*\" , \"arn:aws:ec2:*:*:key-pair/*\" , \"arn:aws:ec2:*:*:security-group/*\" ] }, { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] } Step 4: Create your Hopsworks instance # You can now create a new Hopsworks instance in Hopsworks.ai by selecting the configured instance profile, VPC and security group during instance configuration. Selecting any other VPCs or instance profiles will result in permissions errors. Step 5: Supporting multiple VPCs # The policy can be extended to give Hopsworks.ai access to multiple VPCs. See: Creating a Condition with Multiple Keys or Values . Backup permissions # The following permissions are only needed for the backup feature. You can remove them if you are not going to create backups or if you do not have access to this Enterprise feature. { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" , } Other removable permissions # The following permissions are needed to give an early warning if your VPC and security groups are badly configured. You can remove them if you don't need it. \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in EC2. { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } } Limiting the instance profile permissions # Backups # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring instance profile permissions . For this remove the following permissions from the instance profile: \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:ListBucketVersions\" , \"S3:DeleteObjectVersion\" , CloudWatch Logs # Hopsworks put its logs in Amazon CloudWatch so that you can access them without having to ssh into the machine. If you are not interested in this feature you can remove the following from your instance profile policy: { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" } Upgrade permissions # Removing upgrade permissions # If you do not intend to upgrade your cluster to newer versions of Hopsworks, then you can remove the upgrade permissions statement from the instance profile that you have created here . For this remove the following statement from your instance profile { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } Limiting upgrade permissions # You can use tags to restrict the upgrade permissions to only the resources created for your cluster. For this attach a tag to your cluster during the cluster creation . Then, replace the upgrade permissions with the following policy instead. First you need to replace REGION and ACCOUNT with your region and account where you run your cluster, then replace HEAD_NODE_INSTANCE_ID with your aws instance id of the head node and HEAD_NODE_VOLUME_ID with the volume id attached to the head node, and finally replace the TAG_KEY and TAG_VALUE with your the tag name and value that is used with your cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" :[ { \"Sid\" : \"AllowAttachVolumeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:AttachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" }, { \"Sid\" : \"AllowAttachVolumeForUpgradeOnlyTaggedInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:AttachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:instance/*\" , \"Condition\" :{ \"StringEquals\" :{ \"ec2:ResourceTag/TAG_KEY\" : \"TAG_VALUE\" } } }, { \"Sid\" : \"AllowDetachVolumeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DetachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" }, { \"Sid\" : \"AllowDetachVolumeForUpgradeOnlyTaggedInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DetachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:instance/*\" , \"Condition\" :{ \"StringEquals\" :{ \"ec2:ResourceTag/TAG_KEY\" : \"TAG_VALUE\" } } }, { \"Sid\" : \"AllowModifyInstanceAttributeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:ModifyInstanceAttribute\" , \"Resource\" :[ \"arn:aws:ec2:REGION:ACCOUNT:instance/HEAD_NODE_INSTANCE_ID\" , \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" ] }, { \"Sid\" : \"AllowDescribeVolumesForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DescribeVolumes\" , \"Resource\" : \"*\" } ] }","title":"Limiting Permissions"},{"location":"setup_installation/aws/restrictive_permissions/#limiting-aws-permissions","text":"Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s AWS account. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down AWS permissions following the IT security policy principle of least privilege allowing Hopsworks.ai to only access resources in a specific VPC.","title":"Limiting AWS permissions"},{"location":"setup_installation/aws/restrictive_permissions/#limiting-the-cross-account-role-permissions","text":"","title":"Limiting the cross-account role permissions"},{"location":"setup_installation/aws/restrictive_permissions/#step-1-create-a-vpc","text":"To restrict Hopsworks.ai from accessing resources outside of a specific VPC, you need to create a new VPC connected to an Internet Gateway. This can be achieved in the AWS Management Console following this guide: Create the VPC . The option VPC with a Single Public Subnet from the Launch VPC Wizard should work out of the box. Alternatively, an existing VPC such as the default VPC can be used and Hopsworks.ai will be restricted to this VPC. Note the VPC ID of the VPC you want to use for the following steps. Note Make sure you enable DNS hostnames for your VPC After you have created the VPC either Create a Security Group or use VPC's default. Make sure that the VPC allow the following traffic.","title":"Step 1: Create a VPC"},{"location":"setup_installation/aws/restrictive_permissions/#inbound-traffic","text":"The Security Group and/or Network ACLs need to be configured so that at least port 80 is reachable from the internet otherwise you will have to use self signed certificate in your Hopsworks cluster. It is imperative the Security Group allows Inbound traffic from any Instance within the same Security Group in any (TCP) port. All VMs of the Cluster should be able to communicate with each other.","title":"Inbound traffic"},{"location":"setup_installation/aws/restrictive_permissions/#outbound-traffic","text":"Clusters created on Hopsworks.ai need to be able to send http requests to api.hopsworks.ai . The api.hopsworks.ai domain use a content delivery network for better performance. This result in the impossibility to predict which IP the request will be sent to. If you require a list of static IPs to allow outbound traffic from your security group, use the static IPs option during cluster creation . Similar to Inbound traffic, the Security Group in place must allow Outbound traffic in any (TCP) port towards any VM withing the same Security Group. Note If you intend to use the managed users option on your Hopsworks cluster you should also allow outbound traffic to cognito-idp.us-east-2.amazonaws.com and managedhopsworks-prod.auth.us-east-2.amazoncognito.com .","title":"Outbound traffic"},{"location":"setup_installation/aws/restrictive_permissions/#step-2-create-an-instance-profile","text":"You need to create an instance profile that will identify all instances started by Hopsworks.ai. Follow this guide to create a role to be used by EC2 with no permissions attached: Creating a Role for an AWS Service (Console) . Take note of the ARN of the role you just created. You will need to add permissions to the instance profile to give access to the S3 bucket where Hopsworks will store its data. For more details about these permissions check our guide here . Check bellow for more information on restricting the permissions given the instance profile.","title":"Step 2: Create an instance profile"},{"location":"setup_installation/aws/restrictive_permissions/#step-3-set-permissions-of-the-cross-account-role","text":"During the account setup for Hopsworks.ai, you were asked to create and provide a cross-account role. If you don\u2019t remember which role you used then you can find it in Settings/Account Settings in Hopsworks.ai. Edit this role in the AWS Management Console and overwrite the existing inline policy with the following policy. Note that you have to replace [INSTANCE_PROFILE_NAME] and [VPC_ID] for multiple occurrences in the given policy. If you want to learn more about how this policy works check out: How to Help Lock Down a User\u2019s Amazon EC2 Capabilities to a Single VPC . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"NonResourceBasedPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeInstances\" , \"ec2:DescribeVpcs\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeKeyPairs\" , \"ec2:DescribeInstanceStatus\" , \"iam:ListInstanceProfiles\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" ], \"Resource\" : \"*\" }, { \"Sid\" : \"IAMPassRoleToInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::*:role/[INSTANCE_PROFILE_NAME]\" }, { \"Sid\" : \"EC2RunInstancesOnlyWithGivenRole\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"EC2RunInstancesOnlyInGivenVpc\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : \"arn:aws:ec2:*:*:subnet/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"AllowInstanceActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:StopInstances\" , \"ec2:TerminateInstances\" , \"ec2:StartInstances\" , \"ec2:CreateTags\" , \"ec2:AssociateIamInstanceProfile\" ], \"Resource\" : \"arn:aws:ec2:*:*:instance/*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:InstanceProfile\" : \"arn:aws:iam::*:instance-profile/[INSTANCE_PROFILE_NAME]\" } } }, { \"Sid\" : \"RemainingRunInstancePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:*:volume/*\" , \"arn:aws:ec2:*::image/*\" , \"arn:aws:ec2:*::snapshot/*\" , \"arn:aws:ec2:*:*:network-interface/*\" , \"arn:aws:ec2:*:*:key-pair/*\" , \"arn:aws:ec2:*:*:security-group/*\" ] }, { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }, { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] }","title":"Step 3: Set permissions of the cross-account role"},{"location":"setup_installation/aws/restrictive_permissions/#step-4-create-your-hopsworks-instance","text":"You can now create a new Hopsworks instance in Hopsworks.ai by selecting the configured instance profile, VPC and security group during instance configuration. Selecting any other VPCs or instance profiles will result in permissions errors.","title":"Step 4: Create your Hopsworks instance"},{"location":"setup_installation/aws/restrictive_permissions/#step-5-supporting-multiple-vpcs","text":"The policy can be extended to give Hopsworks.ai access to multiple VPCs. See: Creating a Condition with Multiple Keys or Values .","title":"Step 5: Supporting multiple VPCs"},{"location":"setup_installation/aws/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature. You can remove them if you are not going to create backups or if you do not have access to this Enterprise feature. { \"Sid\" : \"EC2BackupCreation\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" , }","title":"Backup permissions"},{"location":"setup_installation/aws/restrictive_permissions/#other-removable-permissions","text":"The following permissions are needed to give an early warning if your VPC and security groups are badly configured. You can remove them if you don't need it. \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeRouteTables\" The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in EC2. { \"Sid\" : \"EC2VpcNonResourceSpecificActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:RevokeSecurityGroupIngress\" , \"ec2:DeleteSecurityGroup\" ], \"Resource\" : \"*\" , \"Condition\" : { \"ArnLike\" : { \"ec2:vpc\" : \"arn:aws:ec2:*:*:vpc/[VPC_ID]\" } } }","title":"Other removable permissions"},{"location":"setup_installation/aws/restrictive_permissions/#limiting-the-instance-profile-permissions","text":"","title":"Limiting the instance profile permissions"},{"location":"setup_installation/aws/restrictive_permissions/#backups","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring instance profile permissions . For this remove the following permissions from the instance profile: \"S3:PutLifecycleConfiguration\" , \"S3:GetLifecycleConfiguration\" , \"S3:PutBucketVersioning\" , \"S3:ListBucketVersions\" , \"S3:DeleteObjectVersion\" ,","title":"Backups"},{"location":"setup_installation/aws/restrictive_permissions/#cloudwatch-logs","text":"Hopsworks put its logs in Amazon CloudWatch so that you can access them without having to ssh into the machine. If you are not interested in this feature you can remove the following from your instance profile policy: { \"Effect\" : \"Allow\" , \"Action\" : [ \"cloudwatch:PutMetricData\" , \"ec2:DescribeVolumes\" , \"ec2:DescribeTags\" , \"logs:PutLogEvents\" , \"logs:DescribeLogStreams\" , \"logs:DescribeLogGroups\" , \"logs:CreateLogStream\" , \"logs:CreateLogGroup\" ], \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : [ \"ssm:GetParameter\" ], \"Resource\" : \"arn:aws:ssm:*:*:parameter/AmazonCloudWatch-*\" }","title":"CloudWatch Logs"},{"location":"setup_installation/aws/restrictive_permissions/#upgrade-permissions","text":"","title":"Upgrade permissions"},{"location":"setup_installation/aws/restrictive_permissions/#removing-upgrade-permissions","text":"If you do not intend to upgrade your cluster to newer versions of Hopsworks, then you can remove the upgrade permissions statement from the instance profile that you have created here . For this remove the following statement from your instance profile { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" }","title":"Removing upgrade permissions"},{"location":"setup_installation/aws/restrictive_permissions/#limiting-upgrade-permissions","text":"You can use tags to restrict the upgrade permissions to only the resources created for your cluster. For this attach a tag to your cluster during the cluster creation . Then, replace the upgrade permissions with the following policy instead. First you need to replace REGION and ACCOUNT with your region and account where you run your cluster, then replace HEAD_NODE_INSTANCE_ID with your aws instance id of the head node and HEAD_NODE_VOLUME_ID with the volume id attached to the head node, and finally replace the TAG_KEY and TAG_VALUE with your the tag name and value that is used with your cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" :[ { \"Sid\" : \"AllowAttachVolumeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:AttachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" }, { \"Sid\" : \"AllowAttachVolumeForUpgradeOnlyTaggedInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:AttachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:instance/*\" , \"Condition\" :{ \"StringEquals\" :{ \"ec2:ResourceTag/TAG_KEY\" : \"TAG_VALUE\" } } }, { \"Sid\" : \"AllowDetachVolumeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DetachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" }, { \"Sid\" : \"AllowDetachVolumeForUpgradeOnlyTaggedInstance\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DetachVolume\" , \"Resource\" : \"arn:aws:ec2:REGION:ACCOUNT:instance/*\" , \"Condition\" :{ \"StringEquals\" :{ \"ec2:ResourceTag/TAG_KEY\" : \"TAG_VALUE\" } } }, { \"Sid\" : \"AllowModifyInstanceAttributeForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:ModifyInstanceAttribute\" , \"Resource\" :[ \"arn:aws:ec2:REGION:ACCOUNT:instance/HEAD_NODE_INSTANCE_ID\" , \"arn:aws:ec2:REGION:ACCOUNT:volume/HEAD_NODE_VOLUME_ID\" ] }, { \"Sid\" : \"AllowDescribeVolumesForUpgrade\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:DescribeVolumes\" , \"Resource\" : \"*\" } ] }","title":"Limiting upgrade permissions"},{"location":"setup_installation/aws/upgrade/","text":"Upgrade existing clusters on Hopsworks.ai from version 2.2 or older (AWS) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available Step 2: Add upgrade permissions to your instance profile # Note You can skip this step if you already have the following permissions in your instance profile: [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ] We require extra permissions to be added to the instance profile attached to your cluster to proceed with the upgrade. First to get the name of your instance profile, click on the Details tab as shown below: Getting the name of your instance profile Once you get your instance profile name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . Note You can restrict the upgrade permissions given to your instance profile. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] } Step 3: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your instance profile (hopsworks-doc) includes the following permissions: [ \"ec2:DetachVolume\", \"ec2:AttachVolume\", \"ec2:ModifyInstanceAttribute\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your instance profile, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Misconfigured upgrade permissions # During the upgrade process, Hopsworks.ai starts by validating your instance profile permissions to ensure that it includes the required upgrade permissions. If one or more permissions are missing, or if the resource is not set correctly, you will be notified with an error message and a Retry button will appear as shown below: Upgrade permissions are missing Update you instance profile accordingly, then click Retry Upgrade is running Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Version 2.2 or older"},{"location":"setup_installation/aws/upgrade/#upgrade-existing-clusters-on-hopsworksai-from-version-22-or-older-aws","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on Hopsworks.ai from version 2.2 or older (AWS)"},{"location":"setup_installation/aws/upgrade/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/aws/upgrade/#step-2-add-upgrade-permissions-to-your-instance-profile","text":"Note You can skip this step if you already have the following permissions in your instance profile: [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ] We require extra permissions to be added to the instance profile attached to your cluster to proceed with the upgrade. First to get the name of your instance profile, click on the Details tab as shown below: Getting the name of your instance profile Once you get your instance profile name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . Note You can restrict the upgrade permissions given to your instance profile. Refer to this guide for more information. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"UpgradePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:DescribeVolumes\" , \"ec2:DetachVolume\" , \"ec2:AttachVolume\" , \"ec2:ModifyInstanceAttribute\" ], \"Resource\" : \"*\" } ] }","title":"Step 2: Add upgrade permissions to your instance profile"},{"location":"setup_installation/aws/upgrade/#step-3-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your instance profile (hopsworks-doc) includes the following permissions: [ \"ec2:DetachVolume\", \"ec2:AttachVolume\", \"ec2:ModifyInstanceAttribute\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete","title":"Step 3: Run the upgrade process"},{"location":"setup_installation/aws/upgrade/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your instance profile, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"setup_installation/aws/upgrade/#error-1-misconfigured-upgrade-permissions","text":"During the upgrade process, Hopsworks.ai starts by validating your instance profile permissions to ensure that it includes the required upgrade permissions. If one or more permissions are missing, or if the resource is not set correctly, you will be notified with an error message and a Retry button will appear as shown below: Upgrade permissions are missing Update you instance profile accordingly, then click Retry Upgrade is running","title":"Error 1: Misconfigured upgrade permissions"},{"location":"setup_installation/aws/upgrade/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Error 2: Upgrade process error"},{"location":"setup_installation/aws/upgrade_2.4/","text":"Upgrade existing clusters on Hopsworks.ai from version 2.4 or newer (AWS) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available Step 2: Add backup permissions to your cross account role # Note You can skip this step if you already have the following permissions in your cross account role: [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ] We require some extra permissions to be added to the role you have created when connecting your AWS account as described in getting started guide . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. First, check which role or access key you have added to Hopsworks.ai, you can go to the Settings tab, and then click Edit next to the AWS cloud account Cloud Accounts Once you have clicked on Edit , you will be able to see the current assigned role AWS Cross-Account Role Once you get your role name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"HopsworksAIBackup\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] } Step 3: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your cross-account role which you have connected to Hopsworks.ai has the following permissions: [ \"ec2:RegisterImage\", \"ec2:DeregisterImage\", \"ec2:DescribeImages\", \"ec2:CreateSnapshot\", \"ec2:DeleteSnapshot\", \"ec2:DescribeSnapshots\"] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version on the Details tab of your cluster. Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your cross-account role, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Missing backup permissions # If one or more backup permissions are missing, or if the resource is not set correctly, you will be notified with an error message as shown below: Upgrade permissions are missing Update you cross account role as described in Step 2 , then click Start . Once the cluster is up and running, you can try running the upgrade again. Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Version 2.4 or newer"},{"location":"setup_installation/aws/upgrade_2.4/#upgrade-existing-clusters-on-hopsworksai-from-version-24-or-newer-aws","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on Hopsworks.ai from version 2.4 or newer (AWS)"},{"location":"setup_installation/aws/upgrade_2.4/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear. You can proceed by clicking the Upgrade button. A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/aws/upgrade_2.4/#step-2-add-backup-permissions-to-your-cross-account-role","text":"Note You can skip this step if you already have the following permissions in your cross account role: [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ] We require some extra permissions to be added to the role you have created when connecting your AWS account as described in getting started guide . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. First, check which role or access key you have added to Hopsworks.ai, you can go to the Settings tab, and then click Edit next to the AWS cloud account Cloud Accounts Once you have clicked on Edit , you will be able to see the current assigned role AWS Cross-Account Role Once you get your role name, navigate to AWS management console , then click on Roles and then search for your role name and click on it. Go to the Permissions tab, click on Add inline policy , and then go to the JSON tab. Paste the following snippet, click on Review policy , name it, and click Create policy . { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"HopsworksAIBackup\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"ec2:RegisterImage\" , \"ec2:DeregisterImage\" , \"ec2:DescribeImages\" , \"ec2:CreateSnapshot\" , \"ec2:DeleteSnapshot\" , \"ec2:DescribeSnapshots\" ], \"Resource\" : \"*\" } ] }","title":"Step 2: Add backup permissions to your cross account role"},{"location":"setup_installation/aws/upgrade_2.4/#step-3-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your cross-account role which you have connected to Hopsworks.ai has the following permissions: [ \"ec2:RegisterImage\", \"ec2:DeregisterImage\", \"ec2:DescribeImages\", \"ec2:CreateSnapshot\", \"ec2:DeleteSnapshot\", \"ec2:DescribeSnapshots\"] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version on the Details tab of your cluster.","title":"Step 3: Run the upgrade process"},{"location":"setup_installation/aws/upgrade_2.4/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to missing permission or a misconfigured policy in your cross-account role, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"setup_installation/aws/upgrade_2.4/#error-1-missing-backup-permissions","text":"If one or more backup permissions are missing, or if the resource is not set correctly, you will be notified with an error message as shown below: Upgrade permissions are missing Update you cross account role as described in Step 2 , then click Start . Once the cluster is up and running, you can try running the upgrade again.","title":"Error 1: Missing backup permissions"},{"location":"setup_installation/aws/upgrade_2.4/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Error 2: Upgrade process error"},{"location":"setup_installation/azure/aks_acr_integration/","text":"Integration with Azure AKS and ACR # This guide shows how to create a cluster in hopsworks.ai with integrated support for Azure Kubernetes Service (AKS) and Azure Container Registry (ACR). This enables Hopsworks to launch Python jobs, Jupyter servers, and serve models on top of AKS. Hopsworks AKS and ACR integration have four requirements: A virtual network with access to AKS pods and the AKS API servers One Azure container registry configured in your account One AKS cluster Permissions to the ACR and AKS attached to a user-managed identity This guide provides an example setup with a private AKS cluster and public ACR. Note A public AKS cluster means the Kubernetes API server is accessible outside the virtual network it is deployed in. Similarly, a public ACR is accessible through the internet. User assigned managed identity (managed identity) # Note A user assigned managed identity (managed identity) can be created at the subscription level or to a specific resource group in a subscription. The managed identity is attached to the virtual machines that run inside your subscription (or resource group). Hence, the permissions only apply to services that run within your subscription (or resource group). The AKS and ACR integration requires some permissions to be attached to the managed identity used by the Hopsworks cluster. If you have already created a user assigned managed identity for the storage continue to Add role assignment to the managed identity using this identity. To set up the managed identity, go to the resource group where you will add the managed identity - this should be the same resource group you will deploy Hopsworks in. Click on the Add button. In the search dialog, enter \"user assigned managed identity\" . Click on Create . Then give a name to the managed identity and make sure that it is in the Region where you will deploy your cluster. Click on Review + create , and click on Create . Add role assignment to the managed identity # Go to the managed identity created above. Click on Azure role assignments in the left column. Click on Add role assignment . For the Scope select Resource group or Subscription depending on your preference. Select the Role AcrPull and click on Save . Repeat the same operation with the following roles: AcrPull AcrPush AcrDelete Azure Kubernetes Service Cluster User Role Warning You will also need to attach storage access permissions to the managed identity, see Creating and configuring a storage Once finished the role assignments should look similar to the picture below. AKS permissions Private AKS cluster and public ACR # This guide will step through setting up a private AKS cluster and a public ACR. Step 1: Create an AKS cluster # Go to Kubernetes services in the azure portal and click Add then Add Kubernetes cluster . Place the Kubernetes cluster in the same resource group and region as the Hopsworks cluster and choose a name for the Kubernetes cluster. AKS general configuration Next, click on the Authentication tab and verify the settings are as follow: Authentication method: System-assigned managed identity Role-based access control (RBAC): Enabled AKS-managed Azure Active Directory: Disabled Note Currently, AKS is only supported through managed identities. Contact the Logical Clocks sales team if you have a self-managed Kubernetes cluster. AKS authencation configuration Next, go to the networking tab and check Azure CNI . The portal will automatically fill in the IP address ranges for the Kubernetes virtual network. Take note of the virtual network name that is created, in this example the virtual network name was hopsworksstagevnet154. Lastly, check the Enable private cluster option. AKS network configuration Next, go to the Integrations tab. Under container registry click Create new. AKS create ACR Choose a name for the registry and select premium for the SKU. Then press OK . ACR configuration Next press Review + create , then click Create . To prevent the registry from filling up with unnecessary images and artifacts you can enable a retention policy. A retention policy will automatically remove untagged manifests after a specified number of days. To enable a retention policy, go to the registry you created. Go to the Retention (preview) tab and set Status from disabled to enabled . Set the retention policy for 7 days as in the figure below, then press save . ACR retention policy Step 2: create a virtual network for the Hopsworks cluster # Because the Kubernetes API service is private the Hopsworks cluster must be able to reach it over a private network. There are two options to integrate with a private AKS cluster. The first option ( A ) is to put the Hopsworks cluster in a pre-defined virtual network with a peering setup to the Kubernetes network. The second option ( B ) is to create a subnet inside the Kubernetes virtual network where the Hopsworks cluster will be placed. Option A : Peering setup # To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. Go to virtual networks and press create. Choose a name for the new virtual network and select the same resource group you are planning to use for your Hopsworks cluster. Next, go to the IP Addresses tab. Create an address space that does not overlap with the address space in the Kubernetes network. In the previous example, the automatically created Kubernetes network used the address space 10.0.0.0/8 . Hence, the address space 172.18.0.0/16 can safely be used. Next click Review + Create , then Create . Next, go to the created virtual network and go to the Peerings tab. Then click Add . Virtual network peering Choose a name for the peering link. Check the Traffic to remote virtual network as Allow , and Traffic forwarded from remote virtual network to Block . Virtual network peering configuration For the virtual network select the virtual network which was created by AKS, in our example this was hopsworksstagevnet154 . Then press Add . Virtual network peering configuration continuation The last step is to set up a DNS private link to be able to use DNS resolution for the Kubernetes API servers. Go to resource groups in the Azure portal and find the resource group of the Kubernetes cluster. This will be in the form of MC_ in this example it was MC_hopsworks-stage_hopsworks-aks_northeurope . Open the resource group and click on the DNS zone. Private DNS link setup In the left plane there is a tab called Virtual network links , click on the tab. Next press Add . Private DNS link configuration Choose a name for the private link and select the virtual network you will use for the Hopsworks cluster, then press OK. Private DNS link configuration The setup is now finalized and you can create the Hopsworks cluster. Option B : Subnet in AKS network # With this setup, the Hopsworks cluster will reside in the same virtual network as the AKS cluster. The difference is that a new subnet in the virtual network will be used for the Hopsworks cluster. To set up the subnet, first, go to the virtual network that was created by AKS. In our example, this was hopsworksstagevnet154. Next, go to the subnets tab. AKS subnet setup Press + Subnet . Choose a name for the subnet, for example, \"hopsworks\" and an IP range that does not overlap with the Kubernetes network. Then save. AKS subnet setup Create the Hopsworks cluster # This step assumes you are creating your Hopsworks cluster using hopsworks.ai. The AKS configuration can be set under the Managed containers tab. Set Use Azure AKS and Azure ACR as enabled. Two new fields will pop up. Fill them with the name of the container registry and the AKS you created above. In the previous example, we created an ACR with the name hopsworksaks and an AKS cluster with the name hopsaks-cluster . Hence, the configuration should look similar to the picture below Hopsworks AKS configuration In the virtual network tab, you have to select either the virtual network you created for the peering setup or the Kubernetes virtual network depending on which approach you choose. Under the subnet tab, you have to choose the default subnet if you choose the peering approach or the subnet you created if you choose to create a new subnet inside the AKS virtual network.","title":"AKS/ACR integration"},{"location":"setup_installation/azure/aks_acr_integration/#integration-with-azure-aks-and-acr","text":"This guide shows how to create a cluster in hopsworks.ai with integrated support for Azure Kubernetes Service (AKS) and Azure Container Registry (ACR). This enables Hopsworks to launch Python jobs, Jupyter servers, and serve models on top of AKS. Hopsworks AKS and ACR integration have four requirements: A virtual network with access to AKS pods and the AKS API servers One Azure container registry configured in your account One AKS cluster Permissions to the ACR and AKS attached to a user-managed identity This guide provides an example setup with a private AKS cluster and public ACR. Note A public AKS cluster means the Kubernetes API server is accessible outside the virtual network it is deployed in. Similarly, a public ACR is accessible through the internet.","title":"Integration with Azure AKS and ACR"},{"location":"setup_installation/azure/aks_acr_integration/#user-assigned-managed-identity-managed-identity","text":"Note A user assigned managed identity (managed identity) can be created at the subscription level or to a specific resource group in a subscription. The managed identity is attached to the virtual machines that run inside your subscription (or resource group). Hence, the permissions only apply to services that run within your subscription (or resource group). The AKS and ACR integration requires some permissions to be attached to the managed identity used by the Hopsworks cluster. If you have already created a user assigned managed identity for the storage continue to Add role assignment to the managed identity using this identity. To set up the managed identity, go to the resource group where you will add the managed identity - this should be the same resource group you will deploy Hopsworks in. Click on the Add button. In the search dialog, enter \"user assigned managed identity\" . Click on Create . Then give a name to the managed identity and make sure that it is in the Region where you will deploy your cluster. Click on Review + create , and click on Create .","title":"User assigned managed identity (managed identity)"},{"location":"setup_installation/azure/aks_acr_integration/#add-role-assignment-to-the-managed-identity","text":"Go to the managed identity created above. Click on Azure role assignments in the left column. Click on Add role assignment . For the Scope select Resource group or Subscription depending on your preference. Select the Role AcrPull and click on Save . Repeat the same operation with the following roles: AcrPull AcrPush AcrDelete Azure Kubernetes Service Cluster User Role Warning You will also need to attach storage access permissions to the managed identity, see Creating and configuring a storage Once finished the role assignments should look similar to the picture below. AKS permissions","title":"Add role assignment to the managed identity"},{"location":"setup_installation/azure/aks_acr_integration/#private-aks-cluster-and-public-acr","text":"This guide will step through setting up a private AKS cluster and a public ACR.","title":"Private AKS cluster and public ACR"},{"location":"setup_installation/azure/aks_acr_integration/#step-1-create-an-aks-cluster","text":"Go to Kubernetes services in the azure portal and click Add then Add Kubernetes cluster . Place the Kubernetes cluster in the same resource group and region as the Hopsworks cluster and choose a name for the Kubernetes cluster. AKS general configuration Next, click on the Authentication tab and verify the settings are as follow: Authentication method: System-assigned managed identity Role-based access control (RBAC): Enabled AKS-managed Azure Active Directory: Disabled Note Currently, AKS is only supported through managed identities. Contact the Logical Clocks sales team if you have a self-managed Kubernetes cluster. AKS authencation configuration Next, go to the networking tab and check Azure CNI . The portal will automatically fill in the IP address ranges for the Kubernetes virtual network. Take note of the virtual network name that is created, in this example the virtual network name was hopsworksstagevnet154. Lastly, check the Enable private cluster option. AKS network configuration Next, go to the Integrations tab. Under container registry click Create new. AKS create ACR Choose a name for the registry and select premium for the SKU. Then press OK . ACR configuration Next press Review + create , then click Create . To prevent the registry from filling up with unnecessary images and artifacts you can enable a retention policy. A retention policy will automatically remove untagged manifests after a specified number of days. To enable a retention policy, go to the registry you created. Go to the Retention (preview) tab and set Status from disabled to enabled . Set the retention policy for 7 days as in the figure below, then press save . ACR retention policy","title":"Step 1: Create an AKS cluster"},{"location":"setup_installation/azure/aks_acr_integration/#step-2-create-a-virtual-network-for-the-hopsworks-cluster","text":"Because the Kubernetes API service is private the Hopsworks cluster must be able to reach it over a private network. There are two options to integrate with a private AKS cluster. The first option ( A ) is to put the Hopsworks cluster in a pre-defined virtual network with a peering setup to the Kubernetes network. The second option ( B ) is to create a subnet inside the Kubernetes virtual network where the Hopsworks cluster will be placed.","title":"Step 2: create a virtual network for the Hopsworks cluster"},{"location":"setup_installation/azure/aks_acr_integration/#option-a-peering-setup","text":"To establish virtual peering between the Kubernetes cluster and Hopsworks, you need to select or create a virtual network for Hopsworks. Go to virtual networks and press create. Choose a name for the new virtual network and select the same resource group you are planning to use for your Hopsworks cluster. Next, go to the IP Addresses tab. Create an address space that does not overlap with the address space in the Kubernetes network. In the previous example, the automatically created Kubernetes network used the address space 10.0.0.0/8 . Hence, the address space 172.18.0.0/16 can safely be used. Next click Review + Create , then Create . Next, go to the created virtual network and go to the Peerings tab. Then click Add . Virtual network peering Choose a name for the peering link. Check the Traffic to remote virtual network as Allow , and Traffic forwarded from remote virtual network to Block . Virtual network peering configuration For the virtual network select the virtual network which was created by AKS, in our example this was hopsworksstagevnet154 . Then press Add . Virtual network peering configuration continuation The last step is to set up a DNS private link to be able to use DNS resolution for the Kubernetes API servers. Go to resource groups in the Azure portal and find the resource group of the Kubernetes cluster. This will be in the form of MC_ in this example it was MC_hopsworks-stage_hopsworks-aks_northeurope . Open the resource group and click on the DNS zone. Private DNS link setup In the left plane there is a tab called Virtual network links , click on the tab. Next press Add . Private DNS link configuration Choose a name for the private link and select the virtual network you will use for the Hopsworks cluster, then press OK. Private DNS link configuration The setup is now finalized and you can create the Hopsworks cluster.","title":"Option A: Peering setup"},{"location":"setup_installation/azure/aks_acr_integration/#option-b-subnet-in-aks-network","text":"With this setup, the Hopsworks cluster will reside in the same virtual network as the AKS cluster. The difference is that a new subnet in the virtual network will be used for the Hopsworks cluster. To set up the subnet, first, go to the virtual network that was created by AKS. In our example, this was hopsworksstagevnet154. Next, go to the subnets tab. AKS subnet setup Press + Subnet . Choose a name for the subnet, for example, \"hopsworks\" and an IP range that does not overlap with the Kubernetes network. Then save. AKS subnet setup","title":"Option B: Subnet in AKS network"},{"location":"setup_installation/azure/aks_acr_integration/#create-the-hopsworks-cluster","text":"This step assumes you are creating your Hopsworks cluster using hopsworks.ai. The AKS configuration can be set under the Managed containers tab. Set Use Azure AKS and Azure ACR as enabled. Two new fields will pop up. Fill them with the name of the container registry and the AKS you created above. In the previous example, we created an ACR with the name hopsworksaks and an AKS cluster with the name hopsaks-cluster . Hence, the configuration should look similar to the picture below Hopsworks AKS configuration In the virtual network tab, you have to select either the virtual network you created for the peering setup or the Kubernetes virtual network depending on which approach you choose. Under the subnet tab, you have to choose the default subnet if you choose the peering approach or the subnet you created if you choose to create a new subnet inside the AKS virtual network.","title":"Create the Hopsworks cluster"},{"location":"setup_installation/azure/cluster_creation/","text":"Getting started with Hopsworks.ai (Azure) # This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai Step 1 starting to create a cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Select the Resource Group (1) you want to use. Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . To provide the capacity of adding and removing workers on demand, the Hopsworks clusters deployed by Hopsworks.ai store their data in an Azure storage container. In this step, you select which storage account and container to use for this purpose. Select the storage account (6) you want to use in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (7). You can change this name. For more details on how to create and configure a storage in Azure refer to Creating and configuring a storage Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. General configuration On this page, you can also choose to opt out of hopsworks.ai log collection. Hopsworks.ai collects logs about the services running in your cluster to help us improve our system and to provide support. If you choose to opt-out from the log collection and need support you will have to provide us the log by yourself, which will slow down the support process. Step 3 workers configuration # In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand. Static workers configuration # You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration Autoscaling workers configuration # You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration Step 4 select a SSH key # When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in Azure refer to Adding a ssh key to your resource group Choose SSH key Step 5 select the User assigned managed identity: # In order to let the cluster instances access to the Azure storage we need to attach a User assigned managed identity to the virtual machines. In this step you choose which identity to use. This identity need to have access right to the storage account you selected in Step 2 . For more information about how to create this identity and give it access to the storage account refer to Creating and configuring a storage : Choose the User assigned managed identity Step 6 set the backup retention policy: # To backup the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 7 Virtual network selection # In this step, you can select the virtual network which will be used by the Hopsworks cluster. You can either select an existing virtual network or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the virtual network for you, you can choose the CIDR block for this virtual network. Refer to Create a virtual network and subnet for more details on how to create your own virtual network in Azure. Choose virtual network Step 8 Subnet selection # If you selected an existing virtual network in the previous step, this step lets you select which subnet of this virtual network to use. For more information about creating your own subnet refer to Create a virtual network and subnet . If you did not select an existing virtual network in the previous step Hopsworks.ai will create the subnet for you. You can choose the CIDR block this subnet will use. Choose subnet Step 9 Network Security group selection # In this step, you can select the network security group you want to use to manage the inbound and outbound network rules. You can either let Hopsworks.ai create a network security group for you or select an existing security group. For more information about how to create your own network security group in Azure refer to Create a network security group . Note Hopsworks.ai require some rules for inbound and outbound traffic in your security group, for more details refer to inbound traffic rules and outbound traffic rules . Note Hopsworks.ai attaches a public ip to your cluster by default. However, you can disable this behavior by unchecking the Attach Public IP checkbox. Choose security group Limiting outbound traffic to Hopsworks.ai # Clusters created on Hopsworks.ai need to be able to send http requests to api.hopsworks.ai. If you have strict regulation regarding outbound traffic, you can enable the Use static IPs to communicate with Hopsworks.ai checkbox to get the list of IPs to be allowed as shown below: Enable static IPs Step 10 User management selection # In this step, you can choose which user management system to use. You have four choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 12 Managed RonDB # Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us . Step 13 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 14 add an init script to your instances. # In this step, you can enter an initialization script that will be run at startup on every instance. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script Step 15 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Cluster Creation"},{"location":"setup_installation/azure/cluster_creation/#getting-started-with-hopsworksai-azure","text":"This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai","title":"Getting started with Hopsworks.ai (Azure)"},{"location":"setup_installation/azure/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"setup_installation/azure/cluster_creation/#step-2-setting-the-general-information","text":"Select the Resource Group (1) you want to use. Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . To provide the capacity of adding and removing workers on demand, the Hopsworks clusters deployed by Hopsworks.ai store their data in an Azure storage container. In this step, you select which storage account and container to use for this purpose. Select the storage account (6) you want to use in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (7). You can change this name. For more details on how to create and configure a storage in Azure refer to Creating and configuring a storage Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. General configuration On this page, you can also choose to opt out of hopsworks.ai log collection. Hopsworks.ai collects logs about the services running in your cluster to help us improve our system and to provide support. If you choose to opt-out from the log collection and need support you will have to provide us the log by yourself, which will slow down the support process.","title":"Step 2 setting the General information"},{"location":"setup_installation/azure/cluster_creation/#step-3-workers-configuration","text":"In this step, you configure the workers. There are two possible setups static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: the documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand.","title":"Step 3 workers configuration"},{"location":"setup_installation/azure/cluster_creation/#static-workers-configuration","text":"You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration","title":"Static workers configuration"},{"location":"setup_installation/azure/cluster_creation/#autoscaling-workers-configuration","text":"You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration","title":"Autoscaling workers configuration"},{"location":"setup_installation/azure/cluster_creation/#step-4-select-a-ssh-key","text":"When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. Select the SSH key that you want to use to access cluster instances. For more detail on how to add a shh key in Azure refer to Adding a ssh key to your resource group Choose SSH key","title":"Step 4 select a SSH key"},{"location":"setup_installation/azure/cluster_creation/#step-5-select-the-user-assigned-managed-identity","text":"In order to let the cluster instances access to the Azure storage we need to attach a User assigned managed identity to the virtual machines. In this step you choose which identity to use. This identity need to have access right to the storage account you selected in Step 2 . For more information about how to create this identity and give it access to the storage account refer to Creating and configuring a storage : Choose the User assigned managed identity","title":"Step 5 select the User assigned managed identity:"},{"location":"setup_installation/azure/cluster_creation/#step-6-set-the-backup-retention-policy","text":"To backup the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 6 set the backup retention policy:"},{"location":"setup_installation/azure/cluster_creation/#step-7-virtual-network-selection","text":"In this step, you can select the virtual network which will be used by the Hopsworks cluster. You can either select an existing virtual network or let Hopsworks.ai create one for you. If you decide to let Hopsworks.ai create the virtual network for you, you can choose the CIDR block for this virtual network. Refer to Create a virtual network and subnet for more details on how to create your own virtual network in Azure. Choose virtual network","title":"Step 7 Virtual network selection"},{"location":"setup_installation/azure/cluster_creation/#step-8-subnet-selection","text":"If you selected an existing virtual network in the previous step, this step lets you select which subnet of this virtual network to use. For more information about creating your own subnet refer to Create a virtual network and subnet . If you did not select an existing virtual network in the previous step Hopsworks.ai will create the subnet for you. You can choose the CIDR block this subnet will use. Choose subnet","title":"Step 8 Subnet selection"},{"location":"setup_installation/azure/cluster_creation/#step-9-network-security-group-selection","text":"In this step, you can select the network security group you want to use to manage the inbound and outbound network rules. You can either let Hopsworks.ai create a network security group for you or select an existing security group. For more information about how to create your own network security group in Azure refer to Create a network security group . Note Hopsworks.ai require some rules for inbound and outbound traffic in your security group, for more details refer to inbound traffic rules and outbound traffic rules . Note Hopsworks.ai attaches a public ip to your cluster by default. However, you can disable this behavior by unchecking the Attach Public IP checkbox. Choose security group","title":"Step 9 Network Security group selection"},{"location":"setup_installation/azure/cluster_creation/#limiting-outbound-traffic-to-hopsworksai","text":"Clusters created on Hopsworks.ai need to be able to send http requests to api.hopsworks.ai. If you have strict regulation regarding outbound traffic, you can enable the Use static IPs to communicate with Hopsworks.ai checkbox to get the list of IPs to be allowed as shown below: Enable static IPs","title":"Limiting outbound traffic to Hopsworks.ai"},{"location":"setup_installation/azure/cluster_creation/#step-10-user-management-selection","text":"In this step, you can choose which user management system to use. You have four choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 10 User management selection"},{"location":"setup_installation/azure/cluster_creation/#step-12-managed-rondb","text":"Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us .","title":"Step 12 Managed RonDB"},{"location":"setup_installation/azure/cluster_creation/#step-13-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 13 add tags to your instances."},{"location":"setup_installation/azure/cluster_creation/#step-14-add-an-init-script-to-your-instances","text":"In this step, you can enter an initialization script that will be run at startup on every instance. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script","title":"Step 14 add an init script to your instances."},{"location":"setup_installation/azure/cluster_creation/#step-15-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 15 Review and create"},{"location":"setup_installation/azure/getting_started/","text":"Getting started with Hopsworks.ai (Azure) # Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's Azure account. Step 1: Connecting your Azure account # Hopsworks.ai deploys Hopsworks clusters to your Azure account. To enable this, you have to create a service principal and a custom role for Hopsworks.ai granting access to either a subscription or resource group. Step 1.0: Prerequisite # For Hopsworks.ai to deploy a cluster the following resource providers need to be registered on your Azure subscription. You can verify that they are registered by going to your subscription in the Azure portal and click on Resource providers . If one of the resource providers is not registered select it and click on Register . Microso ft .Ne t work Microso ft .Compu te Microso ft .S t orage Microso ft .Ma na gedIde nt i t y Step 1.1: Creating a service principal for Hopsworks.ai # On Hopsworks.ai, go to Settings/Cloud Accounts and choose to Configure Azure: Cloud account settings Select Add subscription key : Add subscription keys The Azure account configuration will show you the required steps and permissions. Ensure that you have the Azure CLI installed Install the Azure CLI and are logged in Sign in with Azure CLI . Copy the Azure CLI command from the first step and open a terminal: Connect your Azure Account Paste the command into the terminal and execute it: Add service principal At this point, you might get the following error message. This means that your Azure user does not have sufficient permissions to add the service principal. In this case, please ask your Azure administrator to add it for you or give you the required permissions. Error az ad sp create --id d4abcc44-2c40-40bd-9bba-986df591c28f When using this permission, the backing application of the service principal being created must in the local tenant. Step 1.2: Creating a custom role for Hopsworks.ai # Proceed to the Azure Portal and open either a Subscription or Resource Group that you want to use for Hopsworks.ai. Click on Access control (IAM) Select Add and choose Add custom role . Note Granting access to a Subscription will grant access to all Resource Groups in that Subscription . If you are uncertain if that is what you want, then start with a Resource Group . Add custom role Name the role and proceed to Assignable scopes : Name custom role Ensure the scope is set to the Subscription or Resource Group you want to use. You can change it here if required. Proceed to the JSON tab: Review assignable scope Select Edit and replace the actions part of the JSON with the one from Hopsworks.ai Azure account configuration workflow: Hopsworks.ai permission list Note If the access rights provided by Hopsworks.ai Azure account configuration workflow are too permissive, you can go to Limiting Azure permissions for more details on how to limit the permissions. Press Save , proceed to Review + create and create the role: Update permission JSON Step 1.3: Assigning the custom role to Hopsworks.ai # Back in the Subscription or Resource Group , in Access control (IAM) , select Add and choose Add role assignment : Add role assignment Choose the custom role you just created, select User, group, or service principal to Assign access to and select the hopsworks.ai service principal. Press Save : Configure Hopsworks.ai as role assignment Go back to the Hopsworks.ai Azure account configuration workflow and proceed to the next step. Copy the CLI command shown: Configure subscription and tenant id Paste the CLI command into your terminal and execute it. Note that you might have multiple entries listed here. If so, ensure that you pick the subscription that you want to use. Show subscription and tenant id Copy the value of id and paste it into the Subscription id field on Hopsworks.ai. Go back to the terminal and copy the value of tenantId . Ensure to NOT use the tenantId under managedByTenants . Paste the value into the Tenant ID field on Hopsworks.ai and press Finish . Congratulations, you have successfully connected you Azure account to Hopsworks.ai. Store subscription and tenant id Step 2: Creating and configuring a storage # Note If you prefer using terraform, you can skip this step and the remaining steps, and instead follow this guide . The Hopsworks clusters deployed by hopsworks.ai store their data in a container in your Azure account. To enable this you need to perform the following operations Create a restrictive role to limit access to the storage account Create a User Assigned Managed Identity Create a storage account and give Hopsworks clusters access to the storage using the restrictive role Step 2.1: Creating a Restrictive Role for Accessing Storage # Similarly to Step 1.2 create a new role named Hopsworks Storage Role . Add the following permissions to the role \"permissions\" : [ { \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/blobServices/read\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ], \"notActions\" : [], \"dataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"notDataActions\" : [] } ] Note Some of these permissions can be removed at the cost of Hopsworks features, see Limiting Azure permissions for more details. Step 2.2: Creating a User Assigned Managed Identity # Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for User Assigned Managed Identity and click on it. Search User Assigned Managed Identity Click on Create . Then, select the Location you want to use and name the identity. Click on Review + create . Finally click on Create . Create a User Assigned Managed Identity Step 2.3: Creating a Storage account # Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for Storage account and click on it. Search Storage Account Identity Click on Create , name your storage account, select the Location you want to use and click on Review + create . Finally click on Create . Create a Storage Account Step 2.4: Give the Managed Identity access to the storage # Proceed to the Storage Account you just created and click on Access Control (IAM) (1). Click on Add (2), then click on Add role assignment (3). In Role select Hopsworks Storage Role (4). In Assign access to select User assigned managed identity (5). Select the identity you created in step 2.1 (6). Click on Save (7). Add role assignment to storage Step 3: Adding a ssh key to your resource group # When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. For this purpose you need to add a ssh key to your resource group. Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for SSH Key and click on it. Click on Create. Then, name your key pair and choose between Generate a new key pair and Upload existing public key . Click on Review + create . Finally click on Create . Add to resource group Step 4: Deploying a Hopsworks cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Resource Group (1) in which you created your storage account and user assigned managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Select the storage account (6) you created above in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (7), you can modify it if needed. Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the User assigned managed identity that you created above: Choose the User assigned managed identity Step 6 set the backup retention policy: # To backup the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit . Choose the backup retention policy Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. You can find more details about these steps here The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster Step 5: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Getting Started"},{"location":"setup_installation/azure/getting_started/#getting-started-with-hopsworksai-azure","text":"Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's Azure account.","title":"Getting started with Hopsworks.ai (Azure)"},{"location":"setup_installation/azure/getting_started/#step-1-connecting-your-azure-account","text":"Hopsworks.ai deploys Hopsworks clusters to your Azure account. To enable this, you have to create a service principal and a custom role for Hopsworks.ai granting access to either a subscription or resource group.","title":"Step 1: Connecting your Azure account"},{"location":"setup_installation/azure/getting_started/#step-10-prerequisite","text":"For Hopsworks.ai to deploy a cluster the following resource providers need to be registered on your Azure subscription. You can verify that they are registered by going to your subscription in the Azure portal and click on Resource providers . If one of the resource providers is not registered select it and click on Register . Microso ft .Ne t work Microso ft .Compu te Microso ft .S t orage Microso ft .Ma na gedIde nt i t y","title":"Step 1.0: Prerequisite"},{"location":"setup_installation/azure/getting_started/#step-11-creating-a-service-principal-for-hopsworksai","text":"On Hopsworks.ai, go to Settings/Cloud Accounts and choose to Configure Azure: Cloud account settings Select Add subscription key : Add subscription keys The Azure account configuration will show you the required steps and permissions. Ensure that you have the Azure CLI installed Install the Azure CLI and are logged in Sign in with Azure CLI . Copy the Azure CLI command from the first step and open a terminal: Connect your Azure Account Paste the command into the terminal and execute it: Add service principal At this point, you might get the following error message. This means that your Azure user does not have sufficient permissions to add the service principal. In this case, please ask your Azure administrator to add it for you or give you the required permissions. Error az ad sp create --id d4abcc44-2c40-40bd-9bba-986df591c28f When using this permission, the backing application of the service principal being created must in the local tenant.","title":"Step 1.1: Creating a service principal for Hopsworks.ai"},{"location":"setup_installation/azure/getting_started/#step-12-creating-a-custom-role-for-hopsworksai","text":"Proceed to the Azure Portal and open either a Subscription or Resource Group that you want to use for Hopsworks.ai. Click on Access control (IAM) Select Add and choose Add custom role . Note Granting access to a Subscription will grant access to all Resource Groups in that Subscription . If you are uncertain if that is what you want, then start with a Resource Group . Add custom role Name the role and proceed to Assignable scopes : Name custom role Ensure the scope is set to the Subscription or Resource Group you want to use. You can change it here if required. Proceed to the JSON tab: Review assignable scope Select Edit and replace the actions part of the JSON with the one from Hopsworks.ai Azure account configuration workflow: Hopsworks.ai permission list Note If the access rights provided by Hopsworks.ai Azure account configuration workflow are too permissive, you can go to Limiting Azure permissions for more details on how to limit the permissions. Press Save , proceed to Review + create and create the role: Update permission JSON","title":"Step 1.2: Creating a custom role for Hopsworks.ai"},{"location":"setup_installation/azure/getting_started/#step-13-assigning-the-custom-role-to-hopsworksai","text":"Back in the Subscription or Resource Group , in Access control (IAM) , select Add and choose Add role assignment : Add role assignment Choose the custom role you just created, select User, group, or service principal to Assign access to and select the hopsworks.ai service principal. Press Save : Configure Hopsworks.ai as role assignment Go back to the Hopsworks.ai Azure account configuration workflow and proceed to the next step. Copy the CLI command shown: Configure subscription and tenant id Paste the CLI command into your terminal and execute it. Note that you might have multiple entries listed here. If so, ensure that you pick the subscription that you want to use. Show subscription and tenant id Copy the value of id and paste it into the Subscription id field on Hopsworks.ai. Go back to the terminal and copy the value of tenantId . Ensure to NOT use the tenantId under managedByTenants . Paste the value into the Tenant ID field on Hopsworks.ai and press Finish . Congratulations, you have successfully connected you Azure account to Hopsworks.ai. Store subscription and tenant id","title":"Step 1.3: Assigning the custom role to Hopsworks.ai"},{"location":"setup_installation/azure/getting_started/#step-2-creating-and-configuring-a-storage","text":"Note If you prefer using terraform, you can skip this step and the remaining steps, and instead follow this guide . The Hopsworks clusters deployed by hopsworks.ai store their data in a container in your Azure account. To enable this you need to perform the following operations Create a restrictive role to limit access to the storage account Create a User Assigned Managed Identity Create a storage account and give Hopsworks clusters access to the storage using the restrictive role","title":"Step 2: Creating and configuring a storage"},{"location":"setup_installation/azure/getting_started/#step-21-creating-a-restrictive-role-for-accessing-storage","text":"Similarly to Step 1.2 create a new role named Hopsworks Storage Role . Add the following permissions to the role \"permissions\" : [ { \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/write\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/read\" , \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/blobServices/read\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ], \"notActions\" : [], \"dataActions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/delete\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/read\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/move/action\" , \"Microsoft.Storage/storageAccounts/blobServices/containers/blobs/write\" ], \"notDataActions\" : [] } ] Note Some of these permissions can be removed at the cost of Hopsworks features, see Limiting Azure permissions for more details.","title":"Step 2.1: Creating a Restrictive Role for Accessing Storage"},{"location":"setup_installation/azure/getting_started/#step-22-creating-a-user-assigned-managed-identity","text":"Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for User Assigned Managed Identity and click on it. Search User Assigned Managed Identity Click on Create . Then, select the Location you want to use and name the identity. Click on Review + create . Finally click on Create . Create a User Assigned Managed Identity","title":"Step 2.2: Creating a User Assigned Managed Identity"},{"location":"setup_installation/azure/getting_started/#step-23-creating-a-storage-account","text":"Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for Storage account and click on it. Search Storage Account Identity Click on Create , name your storage account, select the Location you want to use and click on Review + create . Finally click on Create . Create a Storage Account","title":"Step 2.3: Creating a Storage account"},{"location":"setup_installation/azure/getting_started/#step-24-give-the-managed-identity-access-to-the-storage","text":"Proceed to the Storage Account you just created and click on Access Control (IAM) (1). Click on Add (2), then click on Add role assignment (3). In Role select Hopsworks Storage Role (4). In Assign access to select User assigned managed identity (5). Select the identity you created in step 2.1 (6). Click on Save (7). Add role assignment to storage","title":"Step 2.4: Give the Managed Identity access to the storage"},{"location":"setup_installation/azure/getting_started/#step-3-adding-a-ssh-key-to-your-resource-group","text":"When deploying clusters, Hopsworks.ai installs a ssh key on the cluster's instances so that you can access them if necessary. For this purpose you need to add a ssh key to your resource group. Proceed to the Azure Portal and open the Resource Group that you want to use for Hopsworks.ai. Click on Add then Marketplace . Add to resource group Search for SSH Key and click on it. Click on Create. Then, name your key pair and choose between Generate a new key pair and Upload existing public key . Click on Review + create . Finally click on Create . Add to resource group","title":"Step 3: Adding a ssh key to your resource group"},{"location":"setup_installation/azure/getting_started/#step-4-deploying-a-hopsworks-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Select the Resource Group (1) in which you created your storage account and user assigned managed identity (see above). Note If the Resource Group does not appear in the drop-down, make sure that you properly created and set the custom role for this resource group. Name your cluster (2). Your cluster will be deployed in the Location of your Resource Group (3). Select the Instance type (4) and Local storage (5) size for the cluster Head node . Select the storage account (6) you created above in Azure Storage account name . The name of the container in which the data will be stored is displayed in Azure Container name (7), you can modify it if needed. Note You can choose to use a container already existing in your storage account by using the name of this container, but you need to first make sure that this container is empty. Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Select the SSH key that you want to use to access cluster instances: Choose SSH key Select the User assigned managed identity that you created above: Choose the User assigned managed identity","title":"Step 4: Deploying a Hopsworks cluster"},{"location":"setup_installation/azure/getting_started/#step-6-set-the-backup-retention-policy","text":"To backup the Azure blob storage data when taking a cluster backups we need to set a retention policy for the blob storage. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit . Choose the backup retention policy Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. You can find more details about these steps here The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster","title":"Step 6 set the backup retention policy:"},{"location":"setup_installation/azure/getting_started/#step-5-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Step 5: Next steps"},{"location":"setup_installation/azure/restrictive_permissions/","text":"Limiting Azure permissions # Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s Azure resource group. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege. Limiting the cross-account role permissions # Step 1: Create a virtual network and subnet # To restrict Hopsworks.ai from having write and delete access on virtual networks and subnet you need to create them manually. This can be achieved in the Azure portal following this guide: Create a virtual network . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For the remaining of the configuration, the default options proposed by the portal should work out of the box. Note the names of the virtual network and subnet you want to use for the following steps. Step 2: Create a network security group # To restrict Hopsworks.ai from having write and delete access on network security groups you need to create it manually. This can be achieved in the Azure portal following this guide: Create a network security group . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. Inbound traffic # For Hopsworks.ai to create the SSL certificates the network security group needs to allow inbound traffic on port 80. For this, you need to add an inbound security rule to your network security group. This can be achieved in the Azure portal following this guide: Create a security rule . Setting the destination port ranges to 80 and letting the default values for the other fields should work out of the box. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443. Outbound traffic # Clusters created on Hopsworks.ai need to be able to send http requests to api.hopsworks.ai . The api.hopsworks.ai domain use a content delivery network for better performance. This result in the impossibility to predict which IP the request will be sent to. If you require a list of static IPs to allow outbound traffic from your security group, use the static IPs option during cluster creation . Note If you intend to use the managed users option on your Hopsworks cluster you should also allow outbound traffic to cognito-idp.us-east-2.amazonaws.com and managedhopsworks-prod.auth.us-east-2.amazoncognito.com . Step 3: Set permissions of the cross-account role # During the account setup for Hopsworks.ai, you were asked to create create a custom role for your resource group. Edit this role in the Azure portal by going to your resource group, clicking on Access control (IAM) , opening the tab Roles , searching for the role you created, clicking on the three dots at the end of the role line and clicking on edit. You can then navigate to the JSON tab and overwrite the \"action\" field with the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" ] Step 4: Create your Hopsworks instance # You can now create a new Hopsworks instance in Hopsworks.ai by selecting the virtual network, subnet, and network security group during the instance configuration. Backup permissions # The following permissions are only needed for the backup feature: \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you are not going to create backups or if you do not have access to this Enterprise feature, you can further limit the permission of the cross-account role to the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , ] Public IP Addresses permissions # The following permissions are used to create and attach a public IP Address to the head node. If you do not want to use a public IP Address for the head node, you can remove them: \"actions\" : [ \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , ] You then have to make sure that you uncheck the Attach Public IP check box in the Security Group section of the cluster creation: Attach Public IP Other removable permissions # The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in Azure. \"actions\" : [ \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , ] The following permission is only needed to select the Azure Storage account through a drop-down during cluster creation. You can remove it from the cross-account role and enter the value manually \"actions\" : [ \"Microsoft.Storage/storageAccounts/read\" , ] Limiting the User Assigned Managed Identity permissions # Backups # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your managed identity storage permissions. For this remove the following actions from your user assigned managed identity : \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ] Upgrades # If you do not intend to upgrade your cluster to newer versions of Hopsworks, then you can remove the permissions required for upgrade from the custom role that you have created here . For this remove the following actions from your custom role: \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ]","title":"Limiting Permissions"},{"location":"setup_installation/azure/restrictive_permissions/#limiting-azure-permissions","text":"Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s Azure resource group. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege.","title":"Limiting Azure permissions"},{"location":"setup_installation/azure/restrictive_permissions/#limiting-the-cross-account-role-permissions","text":"","title":"Limiting the cross-account role permissions"},{"location":"setup_installation/azure/restrictive_permissions/#step-1-create-a-virtual-network-and-subnet","text":"To restrict Hopsworks.ai from having write and delete access on virtual networks and subnet you need to create them manually. This can be achieved in the Azure portal following this guide: Create a virtual network . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster. For the remaining of the configuration, the default options proposed by the portal should work out of the box. Note the names of the virtual network and subnet you want to use for the following steps.","title":"Step 1: Create a virtual network and subnet"},{"location":"setup_installation/azure/restrictive_permissions/#step-2-create-a-network-security-group","text":"To restrict Hopsworks.ai from having write and delete access on network security groups you need to create it manually. This can be achieved in the Azure portal following this guide: Create a network security group . Make sure to use the resource group and location in which you intend to deploy your Hopsworks cluster.","title":"Step 2: Create a network security group"},{"location":"setup_installation/azure/restrictive_permissions/#inbound-traffic","text":"For Hopsworks.ai to create the SSL certificates the network security group needs to allow inbound traffic on port 80. For this, you need to add an inbound security rule to your network security group. This can be achieved in the Azure portal following this guide: Create a security rule . Setting the destination port ranges to 80 and letting the default values for the other fields should work out of the box. Note If you intend to use the managed users option on your Hopsworks cluster you should also add a rule to open port 443.","title":"Inbound traffic"},{"location":"setup_installation/azure/restrictive_permissions/#outbound-traffic","text":"Clusters created on Hopsworks.ai need to be able to send http requests to api.hopsworks.ai . The api.hopsworks.ai domain use a content delivery network for better performance. This result in the impossibility to predict which IP the request will be sent to. If you require a list of static IPs to allow outbound traffic from your security group, use the static IPs option during cluster creation . Note If you intend to use the managed users option on your Hopsworks cluster you should also allow outbound traffic to cognito-idp.us-east-2.amazonaws.com and managedhopsworks-prod.auth.us-east-2.amazoncognito.com .","title":"Outbound traffic"},{"location":"setup_installation/azure/restrictive_permissions/#step-3-set-permissions-of-the-cross-account-role","text":"During the account setup for Hopsworks.ai, you were asked to create create a custom role for your resource group. Edit this role in the Azure portal by going to your resource group, clicking on Access control (IAM) , opening the tab Roles , searching for the role you created, clicking on the three dots at the end of the role line and clicking on edit. You can then navigate to the JSON tab and overwrite the \"action\" field with the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , \"Microsoft.Compute/disks/read\" ]","title":"Step 3: Set permissions of the cross-account role"},{"location":"setup_installation/azure/restrictive_permissions/#step-4-create-your-hopsworks-instance","text":"You can now create a new Hopsworks instance in Hopsworks.ai by selecting the virtual network, subnet, and network security group during the instance configuration.","title":"Step 4: Create your Hopsworks instance"},{"location":"setup_installation/azure/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature: \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you are not going to create backups or if you do not have access to this Enterprise feature, you can further limit the permission of the cross-account role to the following: \"actions\" : [ \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/virtualMachines/start/action\" , \"Microsoft.Compute/virtualMachines/powerOff/action\" , \"Microsoft.Compute/virtualMachines/restart/action\" , \"Microsoft.Compute/virtualMachines/delete\" , \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/deallocate/action\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Compute/disks/delete\" , \"Microsoft.Network/networkInterfaces/read\" , \"Microsoft.Network/networkInterfaces/join/action\" , \"Microsoft.Network/networkInterfaces/write\" , \"Microsoft.Network/networkInterfaces/delete\" , \"Microsoft.Network/networkSecurityGroups/read\" , \"Microsoft.Network/networkSecurityGroups/join/action\" , \"Microsoft.Network/networkSecurityGroups/defaultSecurityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/read\" , \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , \"Microsoft.Network/virtualNetworks/read\" , \"Microsoft.Network/virtualNetworks/subnets/read\" , \"Microsoft.Network/virtualNetworks/subnets/join/action\" , \"Microsoft.Resources/subscriptions/resourceGroups/read\" , \"Microsoft.Compute/sshPublicKeys/read\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" , \"Microsoft.ManagedIdentity/userAssignedIdentities/read\" , \"Microsoft.Storage/storageAccounts/read\" , ]","title":"Backup permissions"},{"location":"setup_installation/azure/restrictive_permissions/#public-ip-addresses-permissions","text":"The following permissions are used to create and attach a public IP Address to the head node. If you do not want to use a public IP Address for the head node, you can remove them: \"actions\" : [ \"Microsoft.Network/publicIPAddresses/join/action\" , \"Microsoft.Network/publicIPAddresses/read\" , \"Microsoft.Network/publicIPAddresses/write\" , \"Microsoft.Network/publicIPAddresses/delete\" , ] You then have to make sure that you uncheck the Attach Public IP check box in the Security Group section of the cluster creation: Attach Public IP","title":"Public IP Addresses permissions"},{"location":"setup_installation/azure/restrictive_permissions/#other-removable-permissions","text":"The following permissions are used to let you close and open ports on your cluster from hopswork.ai, you can remove them if you do not want to open ports on your cluster or if you want to manually open ports in Azure. \"actions\" : [ \"Microsoft.Network/networkSecurityGroups/securityRules/write\" , \"Microsoft.Network/networkSecurityGroups/securityRules/delete\" , ] The following permission is only needed to select the Azure Storage account through a drop-down during cluster creation. You can remove it from the cross-account role and enter the value manually \"actions\" : [ \"Microsoft.Storage/storageAccounts/read\" , ]","title":"Other removable permissions"},{"location":"setup_installation/azure/restrictive_permissions/#limiting-the-user-assigned-managed-identity-permissions","text":"","title":"Limiting the User Assigned Managed Identity permissions"},{"location":"setup_installation/azure/restrictive_permissions/#backups","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your managed identity storage permissions. For this remove the following actions from your user assigned managed identity : \"actions\" : [ \"Microsoft.Storage/storageAccounts/blobServices/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ]","title":"Backups"},{"location":"setup_installation/azure/restrictive_permissions/#upgrades","text":"If you do not intend to upgrade your cluster to newer versions of Hopsworks, then you can remove the permissions required for upgrade from the custom role that you have created here . For this remove the following actions from your custom role: \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ]","title":"Upgrades"},{"location":"setup_installation/azure/upgrade/","text":"Upgrade existing clusters on Hopsworks.ai from version 2.2 or older (Azure) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available Step 2: Add upgrade permissions to your user assigned managed identity # Note You can skip this step if you already have the following permissions in your user assigned managed identity : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ] Make sure that the scope of these permissions is your resource group. We require extra permissions to be added to the user assigned managed identity attached to your cluster to proceed with the upgrade. First to get the name of your user assigned managed identity and the resource group of your cluster, click on the Details tab as shown below: Get the resource group name (1) and the user assigned managed identity (2) of your cluster Step 2.1: Add custom role for upgrade permissions # Once you get the names of the resource group and user-assigned managed identity, follow the same steps as in getting started to add a custom role . First, navigate to Azure portal , then click on Resource groups and then search for your resource group and click on it. Go to the Access control (IAM) tab, select Add , and click on Add custom role Add a custom role for upgrade Name the custom role and then click on next till you reach the JSON tab. Name the custom role for upgrade Once you reach the JSON tab, click on Edit to edit the role permissions: Edit the JSON permissions for the custom role for upgrade Once you have clicked on Edit , replace the permissions array with the following snippet: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ], \"notActions\" : [], \"dataActions\" : [], \"notDataActions\" : [] } ] Then, click on Save to save the updated permissions Save permissions for the custom role for upgrade Click on Review and create and then click on Create to create the custom role: Save permissions for the custom role for upgrade Step 2.2: Assign the custom role to your user-assigned managed identity # Navigate back to the your Resource group home page at Azure portal , click on Add and then click on Add role assignment Assign upgrade role to your user assigned managed identity (1) choose the upgrade role that you have just created in Step 2.1 , (2) choose User Assigned Managed Identity , (3) search for the user assigned managed identity attached to your cluster and select it. Finally, (4) click on Save to save the role assignment. Assign upgrade role to your user assigned managed identity Warning When you assign roles or remove role assignments, it can take up to 30 minutes for changes to take effect. Step 3: Add disk read permissions to your role connected to Hopsworks.ai # We require extra permission (\"Microsoft.Compute/disks/read\") to be added to the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 . If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai Now, add the missing permission \"Microsoft.Compute/disks/read\" to the list of actions, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Step 4: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your user assigned managed identity (hopsworks-doc-identity) includes the following permissions: [ \"Microsoft.Compute/virtualMachines/read\", \"Microsoft.Compute/virtualMachines/write\", \"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\", \"Microsoft.Storage/storageAccounts/listKeys/action\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to Hopsworks.ai, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Missing permissions error # If you encounter the following permission error right after starting an upgrade, then you need to make sure that the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 , have permissions to read and write disks (\"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\") . Missing permission error If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai In our example, we were missing only the read permission (\"Microsoft.Compute/disks/read\"). First, add the missing permission, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Once you have updated your role, click on Retry to retry the upgrade process. Retry the upgrade process Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Version 2.2 or older"},{"location":"setup_installation/azure/upgrade/#upgrade-existing-clusters-on-hopsworksai-from-version-22-or-older-azure","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on Hopsworks.ai from version 2.2 or older (Azure)"},{"location":"setup_installation/azure/upgrade/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/azure/upgrade/#step-2-add-upgrade-permissions-to-your-user-assigned-managed-identity","text":"Note You can skip this step if you already have the following permissions in your user assigned managed identity : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ] Make sure that the scope of these permissions is your resource group. We require extra permissions to be added to the user assigned managed identity attached to your cluster to proceed with the upgrade. First to get the name of your user assigned managed identity and the resource group of your cluster, click on the Details tab as shown below: Get the resource group name (1) and the user assigned managed identity (2) of your cluster","title":"Step 2: Add upgrade permissions to your user assigned managed identity"},{"location":"setup_installation/azure/upgrade/#step-21-add-custom-role-for-upgrade-permissions","text":"Once you get the names of the resource group and user-assigned managed identity, follow the same steps as in getting started to add a custom role . First, navigate to Azure portal , then click on Resource groups and then search for your resource group and click on it. Go to the Access control (IAM) tab, select Add , and click on Add custom role Add a custom role for upgrade Name the custom role and then click on next till you reach the JSON tab. Name the custom role for upgrade Once you reach the JSON tab, click on Edit to edit the role permissions: Edit the JSON permissions for the custom role for upgrade Once you have clicked on Edit , replace the permissions array with the following snippet: \"permissions\" : [ { \"actions\" : [ \"Microsoft.Compute/virtualMachines/read\" , \"Microsoft.Compute/virtualMachines/write\" , \"Microsoft.Compute/disks/read\" , \"Microsoft.Compute/disks/write\" , \"Microsoft.Storage/storageAccounts/listKeys/action\" ], \"notActions\" : [], \"dataActions\" : [], \"notDataActions\" : [] } ] Then, click on Save to save the updated permissions Save permissions for the custom role for upgrade Click on Review and create and then click on Create to create the custom role: Save permissions for the custom role for upgrade","title":"Step 2.1: Add custom role for upgrade permissions"},{"location":"setup_installation/azure/upgrade/#step-22-assign-the-custom-role-to-your-user-assigned-managed-identity","text":"Navigate back to the your Resource group home page at Azure portal , click on Add and then click on Add role assignment Assign upgrade role to your user assigned managed identity (1) choose the upgrade role that you have just created in Step 2.1 , (2) choose User Assigned Managed Identity , (3) search for the user assigned managed identity attached to your cluster and select it. Finally, (4) click on Save to save the role assignment. Assign upgrade role to your user assigned managed identity Warning When you assign roles or remove role assignments, it can take up to 30 minutes for changes to take effect.","title":"Step 2.2: Assign the custom role to your user-assigned managed identity"},{"location":"setup_installation/azure/upgrade/#step-3-add-disk-read-permissions-to-your-role-connected-to-hopsworksai","text":"We require extra permission (\"Microsoft.Compute/disks/read\") to be added to the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 . If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai Now, add the missing permission \"Microsoft.Compute/disks/read\" to the list of actions, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai","title":"Step 3: Add disk read permissions to your role connected to Hopsworks.ai"},{"location":"setup_installation/azure/upgrade/#step-4-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your user assigned managed identity (hopsworks-doc-identity) includes the following permissions: [ \"Microsoft.Compute/virtualMachines/read\", \"Microsoft.Compute/virtualMachines/write\", \"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\", \"Microsoft.Storage/storageAccounts/listKeys/action\" ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the Details tab of your cluster as below: Upgrade is complete","title":"Step 4: Run the upgrade process"},{"location":"setup_installation/azure/upgrade/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to Hopsworks.ai, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"setup_installation/azure/upgrade/#error-1-missing-permissions-error","text":"If you encounter the following permission error right after starting an upgrade, then you need to make sure that the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 , have permissions to read and write disks (\"Microsoft.Compute/disks/read\", \"Microsoft.Compute/disks/write\") . Missing permission error If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopswork.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopswork.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopswork.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopswork.ai In our example, we were missing only the read permission (\"Microsoft.Compute/disks/read\"). First, add the missing permission, then click on Save , click on Review + update , and finally click on Update . Add missing permissions to your role connected to hopswork.ai Once you have updated your role, click on Retry to retry the upgrade process. Retry the upgrade process","title":"Error 1: Missing permissions error"},{"location":"setup_installation/azure/upgrade/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Error 2: Upgrade process error"},{"location":"setup_installation/azure/upgrade_2.4/","text":"Upgrade existing clusters on Hopsworks.ai from version 2.4 or newer (Azure) # This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks. Step 1: Make sure your cluster is running # It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available Step 2: Add backup permissions to your role connected to Hopsworks.ai # We require extra permission to be added to the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopsworks.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopsworks.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopsworks.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopsworks.ai Now, add the following permissions to the list of actions, then click on Save , click on Review + update , and finally click on Update . \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , Add missing permissions to your role connected to hopsworks.ai Step 3: Run the upgrade process # You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your custom role which you have connected to Hopsworks.ai has the following permissions: [ \"Microsoft.Compute/snapshots/write\", \"Microsoft.Compute/snapshots/read\", \"Microsoft.Compute/snapshots/delete\", \"Microsoft.Compute/disks/beginGetAccess/action\", ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version number on the Details tab of your cluster. Error handling # There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to Hopsworks.ai, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 . Error 1: Missing permissions error # If one or more backup permissions are missing, or if the resource is not set correctly, you will be notified with an error message as shown below: Missing permission error Update your cross custom role as described in Step 2 , then click Start . Once the cluster is up and running, you can try running the upgrade again. Error 2: Upgrade process error # If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Version 2.4 or newer"},{"location":"setup_installation/azure/upgrade_2.4/#upgrade-existing-clusters-on-hopsworksai-from-version-24-or-newer-azure","text":"This guide shows you how to upgrade your existing Hopsworks cluster to a newer version of Hopsworks.","title":"Upgrade existing clusters on Hopsworks.ai from version 2.4 or newer (Azure)"},{"location":"setup_installation/azure/upgrade_2.4/#step-1-make-sure-your-cluster-is-running","text":"It is important that your cluster is Running . Otherwise you will not be able to upgrade. As soon as a new version is available an upgrade notification will appear: A new Hopsworks version is available","title":"Step 1: Make sure your cluster is running"},{"location":"setup_installation/azure/upgrade_2.4/#step-2-add-backup-permissions-to-your-role-connected-to-hopsworksai","text":"We require extra permission to be added to the role you used to connect to Hopsworks.ai, the one that you have created in Getting started Step 1.2 . These permissions are required to create a snapshot of your cluster before proceeding with the upgrade. \"actions\" : [ \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , ] If you don't remember the name of the role that you have created in Getting started Step 1.2 , you can navigate to your Resource group, (1) click on Access Control , (2) navigate to the Check Access tab, (3) search for hopsworks.ai , (4) click on it, (5) now you have the name of your custom role used to connect to hopsworks.ai. Get your role connected to hopsworks.ai To edit the permissions associated with your role, stay on the same Access Control page, (1) click on the Roles tab, (2) search for your role name (the one you obtained above), (3) click on ... , (4) click on Edit . Edit your role connected to hopsworks.ai You will arrive at the Update a custom role page as shown below: Edit your role connected to hopsworks.ai Navigate to the JSON tab, then click on Edit , as shown below: Edit your role connected to hopsworks.ai Now, add the following permissions to the list of actions, then click on Save , click on Review + update , and finally click on Update . \"Microsoft.Compute/snapshots/write\" , \"Microsoft.Compute/snapshots/read\" , \"Microsoft.Compute/snapshots/delete\" , \"Microsoft.Compute/disks/beginGetAccess/action\" , Add missing permissions to your role connected to hopsworks.ai","title":"Step 2: Add backup permissions to your role connected to Hopsworks.ai"},{"location":"setup_installation/azure/upgrade_2.4/#step-3-run-the-upgrade-process","text":"You need to click on Upgrade to start the upgrade process. You will be prompted with the screen shown below to confirm your intention to upgrade: Note No need to worry about the following message since this is done already in Step 2 Make sure that your custom role which you have connected to Hopsworks.ai has the following permissions: [ \"Microsoft.Compute/snapshots/write\", \"Microsoft.Compute/snapshots/read\", \"Microsoft.Compute/snapshots/delete\", \"Microsoft.Compute/disks/beginGetAccess/action\", ] Upgrade confirmation Check the Yes, upgrade cluster checkbox to proceed, then the Upgrade button will be activated as shown below: Warning Currently, we only support upgrade for the head node and you will need to recreate your workers once the upgrade is successfully completed. Upgrade confirmation Depending on how big your current cluster is, the upgrade process may take from 1 hour to a few hours until completion. Note We don't delete your old cluster until the upgrade process is successfully completed. Upgrade is running Once the upgrade is completed, you can confirm that you have the new Hopsworks version by checking the version number on the Details tab of your cluster.","title":"Step 3: Run the upgrade process"},{"location":"setup_installation/azure/upgrade_2.4/#error-handling","text":"There are two categories of errors that you may encounter during an upgrade. First, a permission error due to a missing permission in your role connected to Hopsworks.ai, see Error 1 . Second, an error during the upgrade process running on your cluster, see Error 2 .","title":"Error handling"},{"location":"setup_installation/azure/upgrade_2.4/#error-1-missing-permissions-error","text":"If one or more backup permissions are missing, or if the resource is not set correctly, you will be notified with an error message as shown below: Missing permission error Update your cross custom role as described in Step 2 , then click Start . Once the cluster is up and running, you can try running the upgrade again.","title":"Error 1: Missing permissions error"},{"location":"setup_installation/azure/upgrade_2.4/#error-2-upgrade-process-error","text":"If an error occurs during the upgrade process, you will have the option to rollback to your old cluster as shown below: Error occurred during upgrade Click on Rollback to recover your old cluster before upgrade. Upgrade rollback confirmation Check the Yes, rollback cluster checkbox to proceed, then the Rollback button will be activated as shown below: Upgrade rollback confirmation Once the rollback is completed, you will be able to continue working as normal with your old cluster. Note The old cluster will be stopped after the rollback. You have to click on the Start button.","title":"Error 2: Upgrade process error"},{"location":"setup_installation/common/adding_removing_workers/","text":"Adding and removing workers # Once you have started a hopsworks cluster you can add and remove workers from the cluster to accommodate your workload. Adding workers # If the computation you are running is using all the resources of your Hopsworks cluster you can add workers to your cluster. To add workers to a cluster, go to the Details tab of this cluster and click on Add workers . Add worker Select the number of workers you want to add (1). Select the type of instance you want the workers to run on (2). Select the local storage size for the workers (3). Click on Next . Add workers Review your request and click Add . Add workers Hopsworks.ai will start the new workers and you will be able to use them in your cluster as soon as they have finished starting. Removing workers # If the load on your Hopsworks cluster is low, you can decide to remove worker nodes from your cluster. Warning When removing workers Hopsworks.ai will try to select workers that can be removed while interfering as little as possible with any ongoing computation. It will also wait for the workers to be done with their computation before stopping them. But, if this computation lasts too long, the worker may get stopped before the computation properly finish. This could interfere with your ongoing computation. Note You can remove all the workers of your cluster. If you do so the cluster will be able to store data but not run any computations. This may affect feature store functionality. To remove workers from a cluster, go to the Details tab of this cluster and click on Remove workers Remove workers For each of the types of instances existing in your cluster select the number of workers you want to remove and click on Next . Remove workers Review your request and click Remove . Remove workers Hopsworks.ai will select the workers corresponding to your criteria which can be stopped with as little interferences as possible with any ongoing computation. It will set them to decommission and stop them when they have finished decommissioning.","title":"Adding and Removing workers"},{"location":"setup_installation/common/adding_removing_workers/#adding-and-removing-workers","text":"Once you have started a hopsworks cluster you can add and remove workers from the cluster to accommodate your workload.","title":"Adding and removing workers"},{"location":"setup_installation/common/adding_removing_workers/#adding-workers","text":"If the computation you are running is using all the resources of your Hopsworks cluster you can add workers to your cluster. To add workers to a cluster, go to the Details tab of this cluster and click on Add workers . Add worker Select the number of workers you want to add (1). Select the type of instance you want the workers to run on (2). Select the local storage size for the workers (3). Click on Next . Add workers Review your request and click Add . Add workers Hopsworks.ai will start the new workers and you will be able to use them in your cluster as soon as they have finished starting.","title":"Adding workers"},{"location":"setup_installation/common/adding_removing_workers/#removing-workers","text":"If the load on your Hopsworks cluster is low, you can decide to remove worker nodes from your cluster. Warning When removing workers Hopsworks.ai will try to select workers that can be removed while interfering as little as possible with any ongoing computation. It will also wait for the workers to be done with their computation before stopping them. But, if this computation lasts too long, the worker may get stopped before the computation properly finish. This could interfere with your ongoing computation. Note You can remove all the workers of your cluster. If you do so the cluster will be able to store data but not run any computations. This may affect feature store functionality. To remove workers from a cluster, go to the Details tab of this cluster and click on Remove workers Remove workers For each of the types of instances existing in your cluster select the number of workers you want to remove and click on Next . Remove workers Review your request and click Remove . Remove workers Hopsworks.ai will select the workers corresponding to your criteria which can be stopped with as little interferences as possible with any ongoing computation. It will set them to decommission and stop them when they have finished decommissioning.","title":"Removing workers"},{"location":"setup_installation/common/api_key/","text":"Hopsworks.ai API Key # Hopsworks.ai allows users to generate an API Key that can be used to authenticate and access the Hopsworks.ai REST APIs. Generate an API Key # First, login to your Hopsworks.ai account, then click on the Settings tab as shown below: Click on the Settings tab Click on the API Key tab, and then click on the Generate API Key button: Generate an API Key Copy the generated API Key and store it in a secure location. Warning Make sure to copy your API Key now. You won\u2019t be able to see it again. However, you can always delete it and generate a new one. Copy the generated API Key Use the API Key # To access the Hopsworks.ai REST APIs, you should pass the API key as a header x-api-key when executing requests on Hopsworks.ai as shown below: curl -XGET -H \"x-api-key: <YOUR API KEY>\" https://api.hopsworks.ai/api/clusters Alternatively, you can use your API Key with the Hopsworks.ai terraform provider to manage your Hopsworks clusters using terraform . Delete your API Key # First, login to your Hopsworks.ai account, click on the Settings tab, then click on the API Key tab, and finally click on Delete API Key as shown below: Delete your API Key","title":"API Key"},{"location":"setup_installation/common/api_key/#hopsworksai-api-key","text":"Hopsworks.ai allows users to generate an API Key that can be used to authenticate and access the Hopsworks.ai REST APIs.","title":"Hopsworks.ai API Key"},{"location":"setup_installation/common/api_key/#generate-an-api-key","text":"First, login to your Hopsworks.ai account, then click on the Settings tab as shown below: Click on the Settings tab Click on the API Key tab, and then click on the Generate API Key button: Generate an API Key Copy the generated API Key and store it in a secure location. Warning Make sure to copy your API Key now. You won\u2019t be able to see it again. However, you can always delete it and generate a new one. Copy the generated API Key","title":"Generate an API Key"},{"location":"setup_installation/common/api_key/#use-the-api-key","text":"To access the Hopsworks.ai REST APIs, you should pass the API key as a header x-api-key when executing requests on Hopsworks.ai as shown below: curl -XGET -H \"x-api-key: <YOUR API KEY>\" https://api.hopsworks.ai/api/clusters Alternatively, you can use your API Key with the Hopsworks.ai terraform provider to manage your Hopsworks clusters using terraform .","title":"Use the API Key"},{"location":"setup_installation/common/api_key/#delete-your-api-key","text":"First, login to your Hopsworks.ai account, click on the Settings tab, then click on the API Key tab, and finally click on Delete API Key as shown below: Delete your API Key","title":"Delete your API Key"},{"location":"setup_installation/common/autoscaling/","text":"Autoscaling # If you run a Hopsworks cluster version 2.2 or above you can enable autoscaling to let hopsworks.ai start and stop workers depending on the demand. Enabling and configuring the autoscaling # Once you have created a cluster you can enable autoscaling by going to the Details tab and clicking on Configure autoscale . You can also set up autoscaling during the cluster creation. For more details about this see the cluster creation documentation ( AWS , AZURE ). Configure autoscale Once you have clicked on Configure autoscale you will access a form allowing you to configure the autoscaling. This form is in two parts. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Configure autoscale details Once you have set your configuration you can review it and enable the autoscaling. Note There are two scenarios if you already have workers in your cluster when enabling the autoscaling: The preexisting workers have the same instance type as the one you set up in the autoscaling. In this case, the autoscaling system will manage these workers and start or stop them automatically. The preexisting workers have a different instance type from the one you set up in the autoscaling. In this case, the autoscaling will not manage these nodes but you will still be able to remove them manually. Configure autoscale review Modifying the autoscaling configuration # You can update the autoscale configuration by going to the Details tab of the cluster and clicking on Configure autoscale . You will then go through the same steps as above. Note that if you change the instance type , nodes that currently exist in the cluster with a different instance type will not be managed by the autoscale system anymore and you will have to remove them manually. Disabling the autoscaling # To disable the autoscaling go to the Details tab, click on Disable autoscale and confirm your action. When you disable autoscaling the nodes that are currently running will keep running. You will need to stop them manually. Disable autoscale","title":"Autoscaling"},{"location":"setup_installation/common/autoscaling/#autoscaling","text":"If you run a Hopsworks cluster version 2.2 or above you can enable autoscaling to let hopsworks.ai start and stop workers depending on the demand.","title":"Autoscaling"},{"location":"setup_installation/common/autoscaling/#enabling-and-configuring-the-autoscaling","text":"Once you have created a cluster you can enable autoscaling by going to the Details tab and clicking on Configure autoscale . You can also set up autoscaling during the cluster creation. For more details about this see the cluster creation documentation ( AWS , AZURE ). Configure autoscale Once you have clicked on Configure autoscale you will access a form allowing you to configure the autoscaling. This form is in two parts. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources in standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the amount of workers you want to be in standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook request the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Configure autoscale details Once you have set your configuration you can review it and enable the autoscaling. Note There are two scenarios if you already have workers in your cluster when enabling the autoscaling: The preexisting workers have the same instance type as the one you set up in the autoscaling. In this case, the autoscaling system will manage these workers and start or stop them automatically. The preexisting workers have a different instance type from the one you set up in the autoscaling. In this case, the autoscaling will not manage these nodes but you will still be able to remove them manually. Configure autoscale review","title":"Enabling and configuring the autoscaling"},{"location":"setup_installation/common/autoscaling/#modifying-the-autoscaling-configuration","text":"You can update the autoscale configuration by going to the Details tab of the cluster and clicking on Configure autoscale . You will then go through the same steps as above. Note that if you change the instance type , nodes that currently exist in the cluster with a different instance type will not be managed by the autoscale system anymore and you will have to remove them manually.","title":"Modifying the autoscaling configuration"},{"location":"setup_installation/common/autoscaling/#disabling-the-autoscaling","text":"To disable the autoscaling go to the Details tab, click on Disable autoscale and confirm your action. When you disable autoscaling the nodes that are currently running will keep running. You will need to stop them manually. Disable autoscale","title":"Disabling the autoscaling"},{"location":"setup_installation/common/gpu_support/","text":"GPU support # Hopsworks can harness the power of GPUs to speed up machine learning processes. You can take advantage of this feature in Hopsworks.ai by adding GPU equipped workers to your cluster. This can be done in two way: creating a cluster with GPU equipped workers or adding GPU equipped workers to an existing cluster. Creating a cluster with GPU equipped workers # When selecting the workers' instance type during the cluster creation, you can select an instance type equipped with GPUs. The cluster will then be created and Hopsworks will automatically detect the GPU resource. Create cluster with GPUs Adding GPU equipped workers to an existing cluster. # When adding workers to a cluster, you can select an instance type equipped with GPUs. The workers will then be added to the cluster and Hopsworks will automatically detect the new GPU resource. Add GPUs to cluster Using the GPUs # Once workers with GPUs have been added to your cluster you can use them by allocating GPUs to JupyterLab or Jobs. Using GPUs in JupyterLab Using GPUs in jobs For more information about using GPUs in Hopsworks you can consult Hopsworks Experiments documentation .","title":"GPU support"},{"location":"setup_installation/common/gpu_support/#gpu-support","text":"Hopsworks can harness the power of GPUs to speed up machine learning processes. You can take advantage of this feature in Hopsworks.ai by adding GPU equipped workers to your cluster. This can be done in two way: creating a cluster with GPU equipped workers or adding GPU equipped workers to an existing cluster.","title":"GPU support"},{"location":"setup_installation/common/gpu_support/#creating-a-cluster-with-gpu-equipped-workers","text":"When selecting the workers' instance type during the cluster creation, you can select an instance type equipped with GPUs. The cluster will then be created and Hopsworks will automatically detect the GPU resource. Create cluster with GPUs","title":"Creating a cluster with GPU equipped workers"},{"location":"setup_installation/common/gpu_support/#adding-gpu-equipped-workers-to-an-existing-cluster","text":"When adding workers to a cluster, you can select an instance type equipped with GPUs. The workers will then be added to the cluster and Hopsworks will automatically detect the new GPU resource. Add GPUs to cluster","title":"Adding GPU equipped workers to an existing cluster."},{"location":"setup_installation/common/gpu_support/#using-the-gpus","text":"Once workers with GPUs have been added to your cluster you can use them by allocating GPUs to JupyterLab or Jobs. Using GPUs in JupyterLab Using GPUs in jobs For more information about using GPUs in Hopsworks you can consult Hopsworks Experiments documentation .","title":"Using the GPUs"},{"location":"setup_installation/common/rondb/","text":"Managed RonDB # For applications where Feature Store's performance and scalability is paramount we give users the option to create clusters with Managed RonDB . You don't need to worry about configuration as hopsworks.ai will automatically pick the best options for your setup. Single node RonDB # The minimum setup for a Hopsworks cluster is to run all database services on their own virtual machine additionally to the Head node. This way the database can scale independently and does not affect other cluster services. Configure RonDB Note For cluster versions <= 2.5.0 the database services run on Head node RonDB cluster # To setup a cluster with multiple RonDB nodes, select RonDB cluster during cluster creation. If this option is not available contact us . General # If you enable Managed RonDB you will see a basic configuration page where you can configure the database nodes. RonDB basic configuration Data node # First, you need to select the instance type and local storage size for the data nodes. These are the database nodes that will store data. RonDB is an in-memory database, so in order to fit more data you need to choose an instance type with more memory. Local storage is used for offline storage of recovery data, and requires a disk at least 40 GiB plus 1.8 times the memory size of the data node VM. Number of replicas # Next you need to select the number of replicas . This is the number of copies the cluster will maintain of your data. Choosing 1 replica is the cheapest option since it requires the lowest number of nodes, but this way you don't have High Availability. Whenever any of the nodes in the cluster fails the cluster will also fail, so only choose 1 replica if you are willing to accept cluster failure. The default and recommended is 2 replicas, which allows the cluster to continue operating after any one node fails. With 3 replicas, the cluster can continue operating after any two node failures that happen after each other. MySQLd nodes # Next you can configure the number of MySQLd nodes. These are dedicated nodes for performing SQL queries against your RonDB cluster. In many use cases you can choose zero, since the cluster's Head node already comes with a MySQL server. If you expect high load on MySQL servers, for example if you intend to run a custom streaming application that performs SQL queries in short intervals, then you can add more. If you select at least one MySQLd node, you then get to select the instance type and local storage size for the MySQLd nodes. Advanced # The advanced tab offers less common options. We recommend keeping the defaults unless you know what you are doing. RonDB advanced configuration RonDB Data node groups # You can choose the number of node groups, also known as database shards. The default is 1 node group, which means that all nodes will have a complete copy of all data. Increasing the number of node groups will split the data evenly. This way, you can create a cluster with higher capacity than a single node. For use cases where it is possible, we recommend using 1 node group and choose an instance type with enough memory to fit all data. Below the number of node groups, you will see a summary of cluster resources. The number of data nodes is an important consideration for the cost of the cluster. It is calculated as the number of node groups multiplied by the number of replicas. The memory available to the cluster is calculated as the number of node groups multiplied by the memory per node. Note that the number replicas does not affect the available memory. The CPUs available to the cluster is calculated as the number of node groups multiplied by the number of CPUs per node. Note that the number replicas does not affect the available CPUs. API nodes # API nodes are specialized nodes which can run user code connecting directly to RonDB datanodes for increased performance. You can choose the number of nodes, the instance type and local storage size. There is also a checkbox to grant access to benchmark tools. This will let a benchmark user access specific database tables, so that you can benchmark RonDB safely. RonDB details # Once the cluster is created you can view some details by clicking on the RonDB tab as shown in the picture below. RonDB cluster details","title":"Managed RonDB"},{"location":"setup_installation/common/rondb/#managed-rondb","text":"For applications where Feature Store's performance and scalability is paramount we give users the option to create clusters with Managed RonDB . You don't need to worry about configuration as hopsworks.ai will automatically pick the best options for your setup.","title":"Managed RonDB"},{"location":"setup_installation/common/rondb/#single-node-rondb","text":"The minimum setup for a Hopsworks cluster is to run all database services on their own virtual machine additionally to the Head node. This way the database can scale independently and does not affect other cluster services. Configure RonDB Note For cluster versions <= 2.5.0 the database services run on Head node","title":"Single node RonDB"},{"location":"setup_installation/common/rondb/#rondb-cluster","text":"To setup a cluster with multiple RonDB nodes, select RonDB cluster during cluster creation. If this option is not available contact us .","title":"RonDB cluster"},{"location":"setup_installation/common/rondb/#general","text":"If you enable Managed RonDB you will see a basic configuration page where you can configure the database nodes. RonDB basic configuration","title":"General"},{"location":"setup_installation/common/rondb/#data-node","text":"First, you need to select the instance type and local storage size for the data nodes. These are the database nodes that will store data. RonDB is an in-memory database, so in order to fit more data you need to choose an instance type with more memory. Local storage is used for offline storage of recovery data, and requires a disk at least 40 GiB plus 1.8 times the memory size of the data node VM.","title":"Data node"},{"location":"setup_installation/common/rondb/#number-of-replicas","text":"Next you need to select the number of replicas . This is the number of copies the cluster will maintain of your data. Choosing 1 replica is the cheapest option since it requires the lowest number of nodes, but this way you don't have High Availability. Whenever any of the nodes in the cluster fails the cluster will also fail, so only choose 1 replica if you are willing to accept cluster failure. The default and recommended is 2 replicas, which allows the cluster to continue operating after any one node fails. With 3 replicas, the cluster can continue operating after any two node failures that happen after each other.","title":"Number of replicas"},{"location":"setup_installation/common/rondb/#mysqld-nodes","text":"Next you can configure the number of MySQLd nodes. These are dedicated nodes for performing SQL queries against your RonDB cluster. In many use cases you can choose zero, since the cluster's Head node already comes with a MySQL server. If you expect high load on MySQL servers, for example if you intend to run a custom streaming application that performs SQL queries in short intervals, then you can add more. If you select at least one MySQLd node, you then get to select the instance type and local storage size for the MySQLd nodes.","title":"MySQLd nodes"},{"location":"setup_installation/common/rondb/#advanced","text":"The advanced tab offers less common options. We recommend keeping the defaults unless you know what you are doing. RonDB advanced configuration","title":"Advanced"},{"location":"setup_installation/common/rondb/#rondb-data-node-groups","text":"You can choose the number of node groups, also known as database shards. The default is 1 node group, which means that all nodes will have a complete copy of all data. Increasing the number of node groups will split the data evenly. This way, you can create a cluster with higher capacity than a single node. For use cases where it is possible, we recommend using 1 node group and choose an instance type with enough memory to fit all data. Below the number of node groups, you will see a summary of cluster resources. The number of data nodes is an important consideration for the cost of the cluster. It is calculated as the number of node groups multiplied by the number of replicas. The memory available to the cluster is calculated as the number of node groups multiplied by the memory per node. Note that the number replicas does not affect the available memory. The CPUs available to the cluster is calculated as the number of node groups multiplied by the number of CPUs per node. Note that the number replicas does not affect the available CPUs.","title":"RonDB Data node groups"},{"location":"setup_installation/common/rondb/#api-nodes","text":"API nodes are specialized nodes which can run user code connecting directly to RonDB datanodes for increased performance. You can choose the number of nodes, the instance type and local storage size. There is also a checkbox to grant access to benchmark tools. This will let a benchmark user access specific database tables, so that you can benchmark RonDB safely.","title":"API nodes"},{"location":"setup_installation/common/rondb/#rondb-details","text":"Once the cluster is created you can view some details by clicking on the RonDB tab as shown in the picture below. RonDB cluster details","title":"RonDB details"},{"location":"setup_installation/common/scalingup/","text":"Scaling up # If you run into limitations due to the instance types you chose during a cluster creation it is possible to scale up the instances to overcome these limitations. Scaling up the workers # If spark jobs are not starting in your cluster it may come from the fact that you don't have worker resources to run them. As workers are stateless the best way to solve this problem is to add new workers with enough resources to handle your job. Or to configure autoscalling to automatically add the workers when needed. Scaling up the head node # You may run into the need to scale up the head node for different reasons. For example: You are running a cluster without dedicated RonDB nodes and have a workload with a high demand on the online feature store. You are running a cluster without managed containers and want to run an important number of jupyter notebooks simultaneously. While we are working on implementing a solution to add these features to an existing cluster you can use the following approach to run your head node on an instance with more vcores and memory to handle more load. To scale up the head node you first have to stop your cluster. Stop the cluster Once the cluster is stopped you can go to the Details tab and click on the head node instance type . Go to details tab an click on the head node instance type This will open a new window. Select the type of instance you want to change to and click on Review and submit Select the new instance type for the heade node Verify your choice and click on Modify Note If you set up your account with AWS in a period predating the introduction of this feature you may need to add the indicated permission to your hopsworks.ai role. Validate your choice You can now start your cluster. The head node will be started on an instance type of the new type you chose. Scaling up the RonDB nodes # If you are running a cluster with dedicated RonDB nodes and have a workload with a high demand on the online feature store you may need to scale up the RonDB Datanodes and MySQLd nodes. For this stop the cluster. Stop the cluster Once the cluster is stopped you can go to the RonDB tab. To scale MySQLd or API nodes, click on the instance type for the node you want to scale up. To scale all datanodes, click on the Change button over their instance types. Datanodes cannot be scaled individually. Go to RonDB tab and click on the instance type you want to change or, for datanodes, click on the Change button This will open a new window. Select the type of instance you want to change to and click on Review and submit Select the new instance type for the heade node Verify your choice and click on Modify Note If you set up your account with AWS in a period predating the introduction of this feature you may need to add the indicated permission to your hopsworks.ai role. Validate your choice You can now start your cluster. The nodes will be started on an instance type of the new type you chose.","title":"Scaling up"},{"location":"setup_installation/common/scalingup/#scaling-up","text":"If you run into limitations due to the instance types you chose during a cluster creation it is possible to scale up the instances to overcome these limitations.","title":"Scaling up"},{"location":"setup_installation/common/scalingup/#scaling-up-the-workers","text":"If spark jobs are not starting in your cluster it may come from the fact that you don't have worker resources to run them. As workers are stateless the best way to solve this problem is to add new workers with enough resources to handle your job. Or to configure autoscalling to automatically add the workers when needed.","title":"Scaling up the workers"},{"location":"setup_installation/common/scalingup/#scaling-up-the-head-node","text":"You may run into the need to scale up the head node for different reasons. For example: You are running a cluster without dedicated RonDB nodes and have a workload with a high demand on the online feature store. You are running a cluster without managed containers and want to run an important number of jupyter notebooks simultaneously. While we are working on implementing a solution to add these features to an existing cluster you can use the following approach to run your head node on an instance with more vcores and memory to handle more load. To scale up the head node you first have to stop your cluster. Stop the cluster Once the cluster is stopped you can go to the Details tab and click on the head node instance type . Go to details tab an click on the head node instance type This will open a new window. Select the type of instance you want to change to and click on Review and submit Select the new instance type for the heade node Verify your choice and click on Modify Note If you set up your account with AWS in a period predating the introduction of this feature you may need to add the indicated permission to your hopsworks.ai role. Validate your choice You can now start your cluster. The head node will be started on an instance type of the new type you chose.","title":"Scaling up the head node"},{"location":"setup_installation/common/scalingup/#scaling-up-the-rondb-nodes","text":"If you are running a cluster with dedicated RonDB nodes and have a workload with a high demand on the online feature store you may need to scale up the RonDB Datanodes and MySQLd nodes. For this stop the cluster. Stop the cluster Once the cluster is stopped you can go to the RonDB tab. To scale MySQLd or API nodes, click on the instance type for the node you want to scale up. To scale all datanodes, click on the Change button over their instance types. Datanodes cannot be scaled individually. Go to RonDB tab and click on the instance type you want to change or, for datanodes, click on the Change button This will open a new window. Select the type of instance you want to change to and click on Review and submit Select the new instance type for the heade node Verify your choice and click on Modify Note If you set up your account with AWS in a period predating the introduction of this feature you may need to add the indicated permission to your hopsworks.ai role. Validate your choice You can now start your cluster. The nodes will be started on an instance type of the new type you chose.","title":"Scaling up the RonDB nodes"},{"location":"setup_installation/common/services/","text":"Services # Hopsworks clusters provide several services that can be accessed from outside Hopsworks. In this documentation, we first show how to make these services accessible to external networks. We will then go through the different services to give a short introduction and link to the associated documentation. Outside Access to the Feature Store # By default, only the Hopsworks UI is made available to clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to the Services tab, selecting a service, and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store If you do not want the ports to be open to the internet you can set up VPC peering between the Hopsworks VPC and your client VPC. You then need to make sure that the ports associated with the services you want to use are open between the two VPCs. The ports associated with each of the services are indicated in the descriptions of the services below. Feature store # The Feature Store is a data management system for managing machine learning features, including the feature engineering code and the feature data. The Feature Store helps ensure that features used during training and serving are consistent and that features are documented and reused within enterprises. You can find the full feature store documentation here and information about how to connect to the Feature Store from different external services here Ports: 8020, 30010, 9083 and 9085 Online Feature store # The online Feature store is required for online applications, where the goal is to retrieve a single feature vector with low latency and the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. You can find a more detailed explanation of the difference between Online and Offline Feature Store here . Once you have opened the ports, the Online Feature store can be used with the same library as the offline feature store. You can find the full documentation here . Port: 3306 Kafka # Hopsworks provides Kafka-as-a-Service for streaming applications and to ingest data. You can find more information about how to use Kafka in Hopsworks in this documentation Port: 9092 SSH # If you want to be able to SSH into the virtual machines running the Hopsworks cluster, you can open the ports using the Services tab. You can then SSH into the machine using your cluster operation system ( ubuntu or centos ) as the user name and the ssh key you selected during the cluster creation. Port: 22. Limiting outbound traffic to Hopsworks.ai # If you have enabled the use of static IPs to communicate with Hopsworks.ai as described in AWS and AZURE , you need to ensure that your security group allow outbound traffic to the two IPs indicated in the service page. Limiting outbound traffic to Hopsworks.ai","title":"Services"},{"location":"setup_installation/common/services/#services","text":"Hopsworks clusters provide several services that can be accessed from outside Hopsworks. In this documentation, we first show how to make these services accessible to external networks. We will then go through the different services to give a short introduction and link to the associated documentation.","title":"Services"},{"location":"setup_installation/common/services/#outside-access-to-the-feature-store","text":"By default, only the Hopsworks UI is made available to clients on external networks, like the Internet. To integrate with external platforms and access APIs for services such as the Feature Store, you have to open the service's ports. Open ports by going to the Services tab, selecting a service, and pressing Update . This will update the Security Group attached to the Hopsworks cluster to allow incoming traffic on the relevant ports. Outside Access to the Feature Store If you do not want the ports to be open to the internet you can set up VPC peering between the Hopsworks VPC and your client VPC. You then need to make sure that the ports associated with the services you want to use are open between the two VPCs. The ports associated with each of the services are indicated in the descriptions of the services below.","title":"Outside Access to the Feature Store"},{"location":"setup_installation/common/services/#feature-store","text":"The Feature Store is a data management system for managing machine learning features, including the feature engineering code and the feature data. The Feature Store helps ensure that features used during training and serving are consistent and that features are documented and reused within enterprises. You can find the full feature store documentation here and information about how to connect to the Feature Store from different external services here Ports: 8020, 30010, 9083 and 9085","title":"Feature store"},{"location":"setup_installation/common/services/#online-feature-store","text":"The online Feature store is required for online applications, where the goal is to retrieve a single feature vector with low latency and the same logic as was applied to generate the training dataset, such that the vector can subsequently be passed to a machine learning model in production to compute a prediction. You can find a more detailed explanation of the difference between Online and Offline Feature Store here . Once you have opened the ports, the Online Feature store can be used with the same library as the offline feature store. You can find the full documentation here . Port: 3306","title":"Online Feature store"},{"location":"setup_installation/common/services/#kafka","text":"Hopsworks provides Kafka-as-a-Service for streaming applications and to ingest data. You can find more information about how to use Kafka in Hopsworks in this documentation Port: 9092","title":"Kafka"},{"location":"setup_installation/common/services/#ssh","text":"If you want to be able to SSH into the virtual machines running the Hopsworks cluster, you can open the ports using the Services tab. You can then SSH into the machine using your cluster operation system ( ubuntu or centos ) as the user name and the ssh key you selected during the cluster creation. Port: 22.","title":"SSH"},{"location":"setup_installation/common/services/#limiting-outbound-traffic-to-hopsworksai","text":"If you have enabled the use of static IPs to communicate with Hopsworks.ai as described in AWS and AZURE , you need to ensure that your security group allow outbound traffic to the two IPs indicated in the service page. Limiting outbound traffic to Hopsworks.ai","title":"Limiting outbound traffic to Hopsworks.ai"},{"location":"setup_installation/common/terraform/","text":"Hopsworks.ai Terraform Provider # Hopsworks.ai allows users to create and manage their clusters using the Hopsworks.ai terraform provider . In this guide, we first provide brief description on how to get started on AWS and AZURE, then we show how to import an existing cluster to be managed by terraform. Getting Started with AWS # Complete the following steps to start using Hopsworks.ai Terraform Provider on AWS. Create a Hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AWS CLI and run aws configurre to configure your AWS credentials. Example # In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"4.16.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"region\" { type = string default = \"us-east-2\" } provider \"aws\" { region = var.region } provider \"hopsworksai\" { } # Create the required aws resources, an ssh key, an s3 bucket, and an instance profile with the required hopsworks permissions module \"aws\" { source = \"logicalclocks/helpers/hopsworksai//modules/aws\" region = var.region version = \"2.0.0\" } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.aws.ssh_key_pair_name head { } aws_attributes { region = var.region instance_profile_arn = module.aws.instance_profile_arn bucket { name = module.aws.bucket_name } } rondb { } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your Hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AWS resources using the following command terraform destroy Getting Started with AZURE # Complete the following steps to start using Hopsworks.ai Terraform Provider on AZURE. Create a Hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AZURE CLI and run az login to configure your AZURE credentials. Example # In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"3.8.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"resource_group\" { type = string } provider \"azurerm\" { features {} skip_provider_registration = true } provider \"hopsworksai\" { } data \"azurerm_resource_group\" \"rg\" { name = var.resource_group } # Create the required azure resources, an ssh key, a storage account, and an user assigned managed identity with the required hopsworks permissions module \"azure\" { source = \"logicalclocks/helpers/hopsworksai//modules/azure\" resource_group = var.resource_group version = \"2.0.0\" } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.azure.ssh_key_pair_name head { } azure_attributes { location = module.azure.location resource_group = module.azure.resource_group user_assigned_managed_identity = module.azure.user_assigned_identity_name container { storage_account = module.azure.storage_account_name } } rondb { } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your Hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AZURE resources using the following command terraform destroy Importing an existing cluster to terraform # In this section, we show how to use terraform import to manage your existing Hopsworks cluster. Step 1 : In your Hopsworks.ai dashboard , choose the cluster you want to import to terraform, then go to the Details tab and copy the Id as shown in the figure below Click on the Details tab and copy the Id Step 2 : In your terminal, create an empty directory and cd to it. mkdir import-demo cd import-demo Step 3 : In this empty directory, create an empty file versions.tf . Open the file and paste the following configurations. Note Notice that you need to change these configurations depending on your cluster, in this example, the Hopsworks cluster reside in region us-east-2 on AWS. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"4.16.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } provider \"aws\" { region = us-east-2 } provider \"hopsworksai\" { } Step 4 : Initialize the terraform directory by running the following command terraform init Step 5 : Create another file main.tf . Open the file and paste the following configuration. resource \"hopsworksai_cluster\" \"cluster\" { } Step 6 : Import the cluster state using terraform import , in this step you need the cluster id from Step 1 (33ae7ae0-d03c-11eb-84e2-af555fb63565). terraform import hopsworksai_cluster.cluster 33ae7ae0-d03c-11eb-84e2-af555fb63565 The output should be similar to the following snippet hopsworksai_cluster.cluster: Importing from ID \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" ... hopsworksai_cluster.cluster: Import prepared! Prepared hopsworksai_cluster for import hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Step 7 : At that moment the local terraform state is updated, however, if we try to run terraform plan or terraform apply it will complain about missing configurations. The reason is that our local resource configuration in main.tf is empty, we should populate it using the terraform state commands as shown below: terraform show -no-color > main.tf Step 8 : If you try to run terraform plan again, the command will complain that the read-only attributes are set (Computed attributes) as shown below. The solution is to remove these attributes from the main.tf and retry again until you have no errors. Error: Computed attributes cannot be set on main.tf line 3 , in resource \"hopsworksai_cluster\" \"cluster\" : 3 : activation_state = \"stoppable\" Computed attributes cannot be set, but a value was set for \"activation_state\" . Error: Computed attributes cannot be set on main.tf line 6 , in resource \"hopsworksai_cluster\" \"cluster\" : 6 : cluster_id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Computed attributes cannot be set, but a value was set for \"cluster_id\" . Error: Computed attributes cannot be set on main.tf line 7 , in resource \"hopsworksai_cluster\" \"cluster\" : 7 : creation_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"creation_date\" . Error: Invalid or unknown key on main.tf line 8 , in resource \"hopsworksai_cluster\" \"cluster\" : 8 : id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Error: Computed attributes cannot be set on main.tf line 13 , in resource \"hopsworksai_cluster\" \"cluster\" : 13 : start_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"start_date\" . Error: Computed attributes cannot be set on main.tf line 14 , in resource \"hopsworksai_cluster\" \"cluster\" : 14 : state = \"running\" Computed attributes cannot be set, but a value was set for \"state\" . Error: Computed attributes cannot be set on main.tf line 17 , in resource \"hopsworksai_cluster\" \"cluster\" : 17 : url = \"https://33ae7ae0-d03c-11eb-84e2-af555fb63565.dev-cloud.hopsworks.ai/hopsworks/#!/\" Computed attributes cannot be set, but a value was set for \"url\" . Step 9 : Once you have fixed all the errors, you should get the following output when running terraform plan . With that, you can proceed as normal to manage this cluster locally using terraform. hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] No changes. Infrastructure is up-to-date. This means that Terraform did not detect any differences between your configuration and real physical resources that exist. As a result, no actions need to be performed. Next Steps # Check the Hopsworks.ai terraform provider documentation for more details about the different resources and data sources supported by the provider and a description of their attributes. Check the Hopsworks.ai terraform AWS examples , each example contains a README file describing how to run it and more details about configuring it. Check the Hopsworks.ai terraform AZURE examples , each example contains a README file describing how to run it and more details about configuring it.","title":"Terraform"},{"location":"setup_installation/common/terraform/#hopsworksai-terraform-provider","text":"Hopsworks.ai allows users to create and manage their clusters using the Hopsworks.ai terraform provider . In this guide, we first provide brief description on how to get started on AWS and AZURE, then we show how to import an existing cluster to be managed by terraform.","title":"Hopsworks.ai Terraform Provider"},{"location":"setup_installation/common/terraform/#getting-started-with-aws","text":"Complete the following steps to start using Hopsworks.ai Terraform Provider on AWS. Create a Hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AWS CLI and run aws configurre to configure your AWS credentials.","title":"Getting Started with AWS"},{"location":"setup_installation/common/terraform/#example","text":"In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"4.16.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"region\" { type = string default = \"us-east-2\" } provider \"aws\" { region = var.region } provider \"hopsworksai\" { } # Create the required aws resources, an ssh key, an s3 bucket, and an instance profile with the required hopsworks permissions module \"aws\" { source = \"logicalclocks/helpers/hopsworksai//modules/aws\" region = var.region version = \"2.0.0\" } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.aws.ssh_key_pair_name head { } aws_attributes { region = var.region instance_profile_arn = module.aws.instance_profile_arn bucket { name = module.aws.bucket_name } } rondb { } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your Hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AWS resources using the following command terraform destroy","title":"Example"},{"location":"setup_installation/common/terraform/#getting-started-with-azure","text":"Complete the following steps to start using Hopsworks.ai Terraform Provider on AZURE. Create a Hopsworks.ai API KEY as described in details here , and export the API KEY as follows export HOPSWORKSAI_API_KEY = <YOUR_API_KEY> Download the proper Terraform CLI for your os from here . Install the AZURE CLI and run az login to configure your AZURE credentials.","title":"Getting Started with AZURE"},{"location":"setup_installation/common/terraform/#example_1","text":"In this section, we provide a simple example to create a Hopsworks cluster on AWS along with all its required resources (ssh key, S3 bucket, and instance profile with the required permissions). In your terminal, run the following to create a demo directory and cd to it mkdir demo cd demo In this empty directory, create an empty file main.tf . Open the file and paste the following configurations to it then save it. terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"3.8.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } variable \"resource_group\" { type = string } provider \"azurerm\" { features {} skip_provider_registration = true } provider \"hopsworksai\" { } data \"azurerm_resource_group\" \"rg\" { name = var.resource_group } # Create the required azure resources, an ssh key, a storage account, and an user assigned managed identity with the required hopsworks permissions module \"azure\" { source = \"logicalclocks/helpers/hopsworksai//modules/azure\" resource_group = var.resource_group version = \"2.0.0\" } # Create a cluster with no workers resource \"hopsworksai_cluster\" \"cluster\" { name = \"tf-hopsworks-cluster\" ssh_key = module.azure.ssh_key_pair_name head { } azure_attributes { location = module.azure.location resource_group = module.azure.resource_group user_assigned_managed_identity = module.azure.user_assigned_identity_name container { storage_account = module.azure.storage_account_name } } rondb { } open_ports { ssh = true } } output \"hopsworks_cluster_url\" { value = hopsworksai_cluster.cluster.url } Initialize the terraform directory by running the following command terraform init Now you can apply the changes to create all required resources terraform apply Once terraform finishes creating the resources, it will output the url to the newly created cluster. Notice that for now, you have to navigate to your Hopsworks.ai dashboard to get your login credentials. After you finish working with the cluster, you can terminate it along with the other AZURE resources using the following command terraform destroy","title":"Example"},{"location":"setup_installation/common/terraform/#importing-an-existing-cluster-to-terraform","text":"In this section, we show how to use terraform import to manage your existing Hopsworks cluster. Step 1 : In your Hopsworks.ai dashboard , choose the cluster you want to import to terraform, then go to the Details tab and copy the Id as shown in the figure below Click on the Details tab and copy the Id Step 2 : In your terminal, create an empty directory and cd to it. mkdir import-demo cd import-demo Step 3 : In this empty directory, create an empty file versions.tf . Open the file and paste the following configurations. Note Notice that you need to change these configurations depending on your cluster, in this example, the Hopsworks cluster reside in region us-east-2 on AWS. terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"4.16.0\" } hopsworksai = { source = \"logicalclocks/hopsworksai\" } } } provider \"aws\" { region = us-east-2 } provider \"hopsworksai\" { } Step 4 : Initialize the terraform directory by running the following command terraform init Step 5 : Create another file main.tf . Open the file and paste the following configuration. resource \"hopsworksai_cluster\" \"cluster\" { } Step 6 : Import the cluster state using terraform import , in this step you need the cluster id from Step 1 (33ae7ae0-d03c-11eb-84e2-af555fb63565). terraform import hopsworksai_cluster.cluster 33ae7ae0-d03c-11eb-84e2-af555fb63565 The output should be similar to the following snippet hopsworksai_cluster.cluster: Importing from ID \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" ... hopsworksai_cluster.cluster: Import prepared! Prepared hopsworksai_cluster for import hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Step 7 : At that moment the local terraform state is updated, however, if we try to run terraform plan or terraform apply it will complain about missing configurations. The reason is that our local resource configuration in main.tf is empty, we should populate it using the terraform state commands as shown below: terraform show -no-color > main.tf Step 8 : If you try to run terraform plan again, the command will complain that the read-only attributes are set (Computed attributes) as shown below. The solution is to remove these attributes from the main.tf and retry again until you have no errors. Error: Computed attributes cannot be set on main.tf line 3 , in resource \"hopsworksai_cluster\" \"cluster\" : 3 : activation_state = \"stoppable\" Computed attributes cannot be set, but a value was set for \"activation_state\" . Error: Computed attributes cannot be set on main.tf line 6 , in resource \"hopsworksai_cluster\" \"cluster\" : 6 : cluster_id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Computed attributes cannot be set, but a value was set for \"cluster_id\" . Error: Computed attributes cannot be set on main.tf line 7 , in resource \"hopsworksai_cluster\" \"cluster\" : 7 : creation_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"creation_date\" . Error: Invalid or unknown key on main.tf line 8 , in resource \"hopsworksai_cluster\" \"cluster\" : 8 : id = \"33ae7ae0-d03c-11eb-84e2-af555fb63565\" Error: Computed attributes cannot be set on main.tf line 13 , in resource \"hopsworksai_cluster\" \"cluster\" : 13 : start_date = \"2021-06-18T15:51:07+02:00\" Computed attributes cannot be set, but a value was set for \"start_date\" . Error: Computed attributes cannot be set on main.tf line 14 , in resource \"hopsworksai_cluster\" \"cluster\" : 14 : state = \"running\" Computed attributes cannot be set, but a value was set for \"state\" . Error: Computed attributes cannot be set on main.tf line 17 , in resource \"hopsworksai_cluster\" \"cluster\" : 17 : url = \"https://33ae7ae0-d03c-11eb-84e2-af555fb63565.dev-cloud.hopsworks.ai/hopsworks/#!/\" Computed attributes cannot be set, but a value was set for \"url\" . Step 9 : Once you have fixed all the errors, you should get the following output when running terraform plan . With that, you can proceed as normal to manage this cluster locally using terraform. hopsworksai_cluster.cluster: Refreshing state... [ id = 33ae7ae0-d03c-11eb-84e2-af555fb63565 ] No changes. Infrastructure is up-to-date. This means that Terraform did not detect any differences between your configuration and real physical resources that exist. As a result, no actions need to be performed.","title":"Importing an existing cluster to terraform"},{"location":"setup_installation/common/terraform/#next-steps","text":"Check the Hopsworks.ai terraform provider documentation for more details about the different resources and data sources supported by the provider and a description of their attributes. Check the Hopsworks.ai terraform AWS examples , each example contains a README file describing how to run it and more details about configuring it. Check the Hopsworks.ai terraform AZURE examples , each example contains a README file describing how to run it and more details about configuring it.","title":"Next Steps"},{"location":"setup_installation/common/user_management/","text":"User management # In Hopsworks.ai users can be grouped into organizations to access the same resources. When a new user registers with Hopsworks.ai a new organization is created. This user later on can invite other registered users to their organization so they can share access to the same clusters. Cloud Accounts configuration is also shared among users of the same organization. So if user Alice has configured her account with her credentials, all member of her organization will automatically deploy clusters in her cloud account. Credits and cluster usage are also grouped to ease reporting. Adding members to an organization # Organization membership can be edited by clicking Members on the left of Hopsworks.ai Dashboard page. Organization membership To add a new member to your organization add the user's email and click Add . The invited user will receive an email with the invitation. You can set the user as administrator by checking the Admin checkbox. More details about organization administrators can be found [here].(#administrator-role) An invited user must accept the invitation to be part of the organization. An invitation will show up on the invitee's Dashboard. The invitee may have to close the Welcome splash screen to be able to see the invitation. In this example, Alice has invited Bob to her organization, but Bob hasn't accepted the invitation yet. Alice has sent the invitation Bob's dashboard Sharing resources # Once Bob has accepted the invitation he does not have to configure his account, they share the same configuration. Also, he will be able to view the same Dashboard as Alice, so he can start, stop or terminate clusters in the organization. Alice's dashboard Bob's dashboard If Alice had existing clusters running and she had selected Managed user management during cluster creation, an account will be automatically created for Bob on these clusters. Removing members from an organization # To remove a member from your organization simply go to Members page and click the Remove button next to the user you want to remove. You will stop sharing any resource and the user will be blocked from any shared cluster. Delete organization member Organization permissions # The owner and the administrators of an organization can set permissions at the organization level. For this got to the members tab, check the checkboxes in the Member permissions section and click on Update . The supported permissions are: Non admin members can invite new members to the organization . If this permission is enabled, any member of the organization will be able to invite other members to the organization. Note that only the owner and the administrators will be able to invite new members as administrators. If this permission is not enabled only the owner and the administrators can invite new members to the organization. Non admin members can create and terminate clusters . If this permission is enabled, any member of the organization will be able to create and terminate clusters. If it is not enabled, only the owner and the administrators will be able to create and terminate clusters. Non admin members can open clusters ports . If this permission is enabled, any member of the organization can open and close services ports on organization's clusters. If it is not enabled, only the organization owner and administrators will be able to do so. Modify permissions Administrator role # Members of an organization can be set as administrators. This can be done by checking the admin checkbox at the time of invitation or by checking the admin checkbox then clicking the Update button next to a member email. Administrators can do all the actions described in the Organization permissions section of this documentation. They can also update the configuration of these permissions and set other users as administrators. Finally, administrators are automatically set as administrators on all the clusters of the organization that have Managed user enabled and are version 2.6.0 or above. Set a member as admin","title":"User management"},{"location":"setup_installation/common/user_management/#user-management","text":"In Hopsworks.ai users can be grouped into organizations to access the same resources. When a new user registers with Hopsworks.ai a new organization is created. This user later on can invite other registered users to their organization so they can share access to the same clusters. Cloud Accounts configuration is also shared among users of the same organization. So if user Alice has configured her account with her credentials, all member of her organization will automatically deploy clusters in her cloud account. Credits and cluster usage are also grouped to ease reporting.","title":"User management"},{"location":"setup_installation/common/user_management/#adding-members-to-an-organization","text":"Organization membership can be edited by clicking Members on the left of Hopsworks.ai Dashboard page. Organization membership To add a new member to your organization add the user's email and click Add . The invited user will receive an email with the invitation. You can set the user as administrator by checking the Admin checkbox. More details about organization administrators can be found [here].(#administrator-role) An invited user must accept the invitation to be part of the organization. An invitation will show up on the invitee's Dashboard. The invitee may have to close the Welcome splash screen to be able to see the invitation. In this example, Alice has invited Bob to her organization, but Bob hasn't accepted the invitation yet. Alice has sent the invitation Bob's dashboard","title":"Adding members to an organization"},{"location":"setup_installation/common/user_management/#sharing-resources","text":"Once Bob has accepted the invitation he does not have to configure his account, they share the same configuration. Also, he will be able to view the same Dashboard as Alice, so he can start, stop or terminate clusters in the organization. Alice's dashboard Bob's dashboard If Alice had existing clusters running and she had selected Managed user management during cluster creation, an account will be automatically created for Bob on these clusters.","title":"Sharing resources"},{"location":"setup_installation/common/user_management/#removing-members-from-an-organization","text":"To remove a member from your organization simply go to Members page and click the Remove button next to the user you want to remove. You will stop sharing any resource and the user will be blocked from any shared cluster. Delete organization member","title":"Removing members from an organization"},{"location":"setup_installation/common/user_management/#organization-permissions","text":"The owner and the administrators of an organization can set permissions at the organization level. For this got to the members tab, check the checkboxes in the Member permissions section and click on Update . The supported permissions are: Non admin members can invite new members to the organization . If this permission is enabled, any member of the organization will be able to invite other members to the organization. Note that only the owner and the administrators will be able to invite new members as administrators. If this permission is not enabled only the owner and the administrators can invite new members to the organization. Non admin members can create and terminate clusters . If this permission is enabled, any member of the organization will be able to create and terminate clusters. If it is not enabled, only the owner and the administrators will be able to create and terminate clusters. Non admin members can open clusters ports . If this permission is enabled, any member of the organization can open and close services ports on organization's clusters. If it is not enabled, only the organization owner and administrators will be able to do so. Modify permissions","title":"Organization permissions"},{"location":"setup_installation/common/user_management/#administrator-role","text":"Members of an organization can be set as administrators. This can be done by checking the admin checkbox at the time of invitation or by checking the admin checkbox then clicking the Update button next to a member email. Administrators can do all the actions described in the Organization permissions section of this documentation. They can also update the configuration of these permissions and set other users as administrators. Finally, administrators are automatically set as administrators on all the clusters of the organization that have Managed user enabled and are version 2.6.0 or above. Set a member as admin","title":"Administrator role"},{"location":"setup_installation/common/sso/ldap/","text":"Configure your hopsworks cluster to use LDAP for user management. # If you want to use your organization's LDAP as an identity provider to manage users in your Hopsworks cluster this document will guide you through the necessary steps to configure Hopsworks.ai to use LDAP. The LDAP attributes below are used to configure JNDI resources in the hopsworks server. The JNDI resource will communicate with your LDAP server to perform the authentication. Setup LDAP jndilookupname : should contain the LDAP domain. java.naming.provider.url : url of your LDAP server with port. java.naming.ldap.attributes.binary : is the binary unique identifier that will be used in subsequent logins to identify the user. java.naming.security.authentication : how to authenticate to the LDAP server. java.naming.security.principal : contains the username of the user that will be used to query LDAP. java.naming.security.credentials : contains the password of the user that will be used to query LDAP. java.naming.referral : whether to follow or ignore an alternate location in which an LDAP Request may be processed. After configuring LDAP and creating your cluster you can log into your hopsworks cluster and edit the LDAP attributes to field names to match your server. By default all attributes to field names are set to the values in OpenLDAP . See Configure LDAP on how to edit the LDAP default configurations. Note A default admin user that can log in with username and password will be created for the user that is creating the cluster. This user can be removed after making sure users can log in using LDAP.","title":"LDAP"},{"location":"setup_installation/common/sso/ldap/#configure-your-hopsworks-cluster-to-use-ldap-for-user-management","text":"If you want to use your organization's LDAP as an identity provider to manage users in your Hopsworks cluster this document will guide you through the necessary steps to configure Hopsworks.ai to use LDAP. The LDAP attributes below are used to configure JNDI resources in the hopsworks server. The JNDI resource will communicate with your LDAP server to perform the authentication. Setup LDAP jndilookupname : should contain the LDAP domain. java.naming.provider.url : url of your LDAP server with port. java.naming.ldap.attributes.binary : is the binary unique identifier that will be used in subsequent logins to identify the user. java.naming.security.authentication : how to authenticate to the LDAP server. java.naming.security.principal : contains the username of the user that will be used to query LDAP. java.naming.security.credentials : contains the password of the user that will be used to query LDAP. java.naming.referral : whether to follow or ignore an alternate location in which an LDAP Request may be processed. After configuring LDAP and creating your cluster you can log into your hopsworks cluster and edit the LDAP attributes to field names to match your server. By default all attributes to field names are set to the values in OpenLDAP . See Configure LDAP on how to edit the LDAP default configurations. Note A default admin user that can log in with username and password will be created for the user that is creating the cluster. This user can be removed after making sure users can log in using LDAP.","title":"Configure your hopsworks cluster to use LDAP for user management."},{"location":"setup_installation/common/sso/oauth/","text":"Configure your hopsworks cluster to use OAuth2 for user management. # If you want to use your organization's OAuth 2.0 identity provider to manage users in your hopsworks cluster this document will guide you through the necessary steps to register your identity provider in Hopsworks.ai. Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . Examples on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively. In the User management step of cluster creation ( AWS , Azure ) you can choose which user management system to use. Select OAuth2 (OpenId) from the dropdown and configure your identity provider. Setup OAuth Register your identity provider by setting the following fields: Create Administrator password user : if checked an administrator that can log in to the hopsworks cluster, with email and password, will be created for the user creating the cluster. If Not checked a group mapping that maps at least one group in the identity provider to HOPS_ADMIN is required. ClientId : the client id generated when registering hopsworks in your identity provider. Client Secret : the client secret generated when registering hopsworks in your identity provider. Provider URI : is the base uri of the identity provider (URI should contain scheme http:// or https://). Provider Name : a unique name to identify the identity provider in your hopsworks cluster. This name will be used in the login page as an alternative login method if Provider DisplayName is not set. Optionally you can also set: Provider DisplayName : the name to display for the alternative login method (if not set Provider Name will be used) Provider Logo URI : a logo URL to an image. The logo will be shown on the login page with the provider name. Code Challenge Method : if your identity provider requires a code challenge for authorization request check the code challenge check box. This will allow you to choose a code challenge method that can be either plain or S256. Group Mapping : will allow you to map groups in your identity provider to groups in hopsworks. You can choose to map all users to HOPS_USER or HOPS_ADMIN. Alternatively you can add mappings as in the example below. IT->HOPS_ADMIN;DATA_SCIENCE->HOPS_USER This will map users in the IT group in your identity provider to HOPS_ADMIN and users in the DATA_SCIENCE group to HOPS_USER. Verify Email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Activate user : if not checked an administrator in hopsworks needs to activate users before they can login. Need consent : if checked, users will be asked for consent when logging in for the first time. Disable registration : if unchecked users will have the possibility to create accounts in the hopsworks cluster using user name and password instead of OAuth. Provider Metadata Endpoint Supported : if your provider defines a discovery mechanism, called OpenID Connect Discovery, where it publishes its metadata at a well-known URL, typically https://server.com/.well-known/openid-configuration you can check this and the metadata will be discovered by hopsworks. If your provider does not publish its metadata you need to supply these values manually. Setup Provider Authorization Endpoint : the authorization endpoint of your identity provider, typically https://server.com/oauth2/authorize End Session Endpoint : the logout endpoint of your identity provider, typically https://server.com/oauth2/logout Token Endpoint : the token endpoint of your identity provider, typically https://server.com/oauth2/token UserInfo Endpoint : the user info endpoint of your identity provider, typically https://server.com/oauth2/userinfo JWKS URI : the JSON Web Key Set endpoint of your identity provider, typically https://server.com/oauth2/keys After configuring OAuth2 you can click on Next to configure the rest of your cluster. You can also configure OAuth2 once you have created a Hopsworks cluster. For instructions on how to configure OAUth2 on Hopsworks see Authentication Methods .","title":"OAuth2"},{"location":"setup_installation/common/sso/oauth/#configure-your-hopsworks-cluster-to-use-oauth2-for-user-management","text":"If you want to use your organization's OAuth 2.0 identity provider to manage users in your hopsworks cluster this document will guide you through the necessary steps to register your identity provider in Hopsworks.ai. Before registering your identity provider in Hopsworks you need to create a client application in your identity provider and acquire a client id and a client secret . Examples on how to create a client using Okta and Azure Active Directory identity providers can be found here and here respectively. In the User management step of cluster creation ( AWS , Azure ) you can choose which user management system to use. Select OAuth2 (OpenId) from the dropdown and configure your identity provider. Setup OAuth Register your identity provider by setting the following fields: Create Administrator password user : if checked an administrator that can log in to the hopsworks cluster, with email and password, will be created for the user creating the cluster. If Not checked a group mapping that maps at least one group in the identity provider to HOPS_ADMIN is required. ClientId : the client id generated when registering hopsworks in your identity provider. Client Secret : the client secret generated when registering hopsworks in your identity provider. Provider URI : is the base uri of the identity provider (URI should contain scheme http:// or https://). Provider Name : a unique name to identify the identity provider in your hopsworks cluster. This name will be used in the login page as an alternative login method if Provider DisplayName is not set. Optionally you can also set: Provider DisplayName : the name to display for the alternative login method (if not set Provider Name will be used) Provider Logo URI : a logo URL to an image. The logo will be shown on the login page with the provider name. Code Challenge Method : if your identity provider requires a code challenge for authorization request check the code challenge check box. This will allow you to choose a code challenge method that can be either plain or S256. Group Mapping : will allow you to map groups in your identity provider to groups in hopsworks. You can choose to map all users to HOPS_USER or HOPS_ADMIN. Alternatively you can add mappings as in the example below. IT->HOPS_ADMIN;DATA_SCIENCE->HOPS_USER This will map users in the IT group in your identity provider to HOPS_ADMIN and users in the DATA_SCIENCE group to HOPS_USER. Verify Email : if checked only users with verified email address (in the identity provider) can log in to Hopsworks. Activate user : if not checked an administrator in hopsworks needs to activate users before they can login. Need consent : if checked, users will be asked for consent when logging in for the first time. Disable registration : if unchecked users will have the possibility to create accounts in the hopsworks cluster using user name and password instead of OAuth. Provider Metadata Endpoint Supported : if your provider defines a discovery mechanism, called OpenID Connect Discovery, where it publishes its metadata at a well-known URL, typically https://server.com/.well-known/openid-configuration you can check this and the metadata will be discovered by hopsworks. If your provider does not publish its metadata you need to supply these values manually. Setup Provider Authorization Endpoint : the authorization endpoint of your identity provider, typically https://server.com/oauth2/authorize End Session Endpoint : the logout endpoint of your identity provider, typically https://server.com/oauth2/logout Token Endpoint : the token endpoint of your identity provider, typically https://server.com/oauth2/token UserInfo Endpoint : the user info endpoint of your identity provider, typically https://server.com/oauth2/userinfo JWKS URI : the JSON Web Key Set endpoint of your identity provider, typically https://server.com/oauth2/keys After configuring OAuth2 you can click on Next to configure the rest of your cluster. You can also configure OAuth2 once you have created a Hopsworks cluster. For instructions on how to configure OAUth2 on Hopsworks see Authentication Methods .","title":"Configure your hopsworks cluster to use OAuth2 for user management."},{"location":"setup_installation/common/sso/sso/","text":"Hopsworks.ai Single Sign-On # We will see here how to set up Single Sign-On for Hopsworks.ai. Once this is set up users from your organization will be able to directly sign in to Hopsworks.ai using your identity provider and without the need to manually create an account. They will then be able to manage the clusters of your organization and if you set up user management on your clusters an account will automatically be created for them in the clusters. Note See Hopsworks Single Sing-On if you do not want to give users the rights to manage your organization clusters but want to use your identity provider to manage access to your Hopsworks clusters. Configure your identity provider. # We will give here the examples of Azure Active Directory and AWS Single Sign-On but a similar setup can be done with any identity provider supporting SAML. Azure Active Directory # Go to your hopsworks.ai dashboard . Click on Settings . Click on SSO . Click on Setup SSO . Setup SSO Click on Azure Active Directory . You will need the two copyable entries on this page in the following steps. Azure Active Directory Go to the Azure Portal then proceed to the Active Directory and click on Enterprise applications . Click on New application . New application Search for hopsworks.ai . Click on it then click on create . Create your own application Click on Single sign-on . Then click on SAML . SAML Click on Edit in the Basic SAML Configuration section. Paste the Identifier (Entity ID) and Reply URL that you copied from the Hopsworks.ai setup page. Delete the wild card Identifier (Entity ID) that is already set. For the Sign on URL copy the provided pattern ( https://managed.hopsworks.ai/sso-open/ ) and replace ORGANIZATION by the name of your organization. Click on Save . Configure SAML In the SAML Signing Certificate section copy the App Federation Metadata URL . App Federation Metadata URL Click on Users and groups , in the left column, and add the users and groups you want to have access to hopsworks.ai. Go back to Hopsworks.ai. Click on Next step and keep following the documentation at Configure Hopsworks.ai . Next step Set the organization name you chose above. This name will be used in your login URL so choose something you will remember. Here we will use hopsworks-demo . Paste the Metadata URL you copied above and click Finish . Configure Hopsworks.ai Note if the organization name you chose is already used you will need to set a new one and to update the Sign on URL in Azure. If you go back to the SSO tab of Settings you will get a logging page link. By using this link you will automatically be redirected to your identity provider to login. An account will automatically be created in hopsworks.ai for users of your organization when they log in for the first time. Configure Hopsworks.ai AWS Single Sign-On # Go to your hopsworks.ai dashboard . Click on Settings . Click on SSO . Click on Setup SSO . Setup SSO Click on AWS SSO . You will need the copyable entries on this page in the following steps. AWS SSO Go to AWS Single Sign-On in the AWS Management Console and click on Applications , then click on Add New Application . Add New application Click on Add a custom SAML 2.0 application . Add a custom SAML 2.0 application Give a name to your application, for example, hopsworks_sso . Scroll to the bottom and click on If you don't have a metadata file, you can manually type your metadata values . Application configuration Paste the Application ACS URL and Application SAML audience that you copy from the Hopsworks.ai setup page. Click on Save changes . Application configuration 2 Go to the Attribute mappings tab. On the first line enter the value Subject and select unspecified for the format. then, Click on Add new attribute mapping 3 times. Attribute mapping For each of the created lines enter the following values in the first and second columns and let the format as unspecified. First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress , second: ${user:email} First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/surname , Second: ${user:familyName} First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname , Second: ${user:givenName} Click on Save changes . Attribute mapping 2 Return to the Configuration tab and click on Edit configuration . Edit configuration Click on Copy URL on the AWS SSO SAML metadata file line. We will call this URL Metadata URL in the coming steps. Metadata URL Go back to Hopsworks.ai. Click on Next step . Next step Give a name to your organization. This name will be used in your login URL so choose something you will remember. Here we will use hopsworks-demo . Paste the Metadata URL you copied above and click Finish . Configure Hopsworks.ai If you go back to the SSO tab of Settings you will get a logging page link. By using this link you will automatically be redirected to your identity provider to login. An account will automatically be created in hopsworks.ai for users of your organization when they log in for the first time. Configure Hopsworks.ai","title":"Hopsworks.ai"},{"location":"setup_installation/common/sso/sso/#hopsworksai-single-sign-on","text":"We will see here how to set up Single Sign-On for Hopsworks.ai. Once this is set up users from your organization will be able to directly sign in to Hopsworks.ai using your identity provider and without the need to manually create an account. They will then be able to manage the clusters of your organization and if you set up user management on your clusters an account will automatically be created for them in the clusters. Note See Hopsworks Single Sing-On if you do not want to give users the rights to manage your organization clusters but want to use your identity provider to manage access to your Hopsworks clusters.","title":"Hopsworks.ai Single Sign-On"},{"location":"setup_installation/common/sso/sso/#configure-your-identity-provider","text":"We will give here the examples of Azure Active Directory and AWS Single Sign-On but a similar setup can be done with any identity provider supporting SAML.","title":"Configure your identity provider."},{"location":"setup_installation/common/sso/sso/#azure-active-directory","text":"Go to your hopsworks.ai dashboard . Click on Settings . Click on SSO . Click on Setup SSO . Setup SSO Click on Azure Active Directory . You will need the two copyable entries on this page in the following steps. Azure Active Directory Go to the Azure Portal then proceed to the Active Directory and click on Enterprise applications . Click on New application . New application Search for hopsworks.ai . Click on it then click on create . Create your own application Click on Single sign-on . Then click on SAML . SAML Click on Edit in the Basic SAML Configuration section. Paste the Identifier (Entity ID) and Reply URL that you copied from the Hopsworks.ai setup page. Delete the wild card Identifier (Entity ID) that is already set. For the Sign on URL copy the provided pattern ( https://managed.hopsworks.ai/sso-open/ ) and replace ORGANIZATION by the name of your organization. Click on Save . Configure SAML In the SAML Signing Certificate section copy the App Federation Metadata URL . App Federation Metadata URL Click on Users and groups , in the left column, and add the users and groups you want to have access to hopsworks.ai. Go back to Hopsworks.ai. Click on Next step and keep following the documentation at Configure Hopsworks.ai . Next step Set the organization name you chose above. This name will be used in your login URL so choose something you will remember. Here we will use hopsworks-demo . Paste the Metadata URL you copied above and click Finish . Configure Hopsworks.ai Note if the organization name you chose is already used you will need to set a new one and to update the Sign on URL in Azure. If you go back to the SSO tab of Settings you will get a logging page link. By using this link you will automatically be redirected to your identity provider to login. An account will automatically be created in hopsworks.ai for users of your organization when they log in for the first time. Configure Hopsworks.ai","title":"Azure Active Directory"},{"location":"setup_installation/common/sso/sso/#aws-single-sign-on","text":"Go to your hopsworks.ai dashboard . Click on Settings . Click on SSO . Click on Setup SSO . Setup SSO Click on AWS SSO . You will need the copyable entries on this page in the following steps. AWS SSO Go to AWS Single Sign-On in the AWS Management Console and click on Applications , then click on Add New Application . Add New application Click on Add a custom SAML 2.0 application . Add a custom SAML 2.0 application Give a name to your application, for example, hopsworks_sso . Scroll to the bottom and click on If you don't have a metadata file, you can manually type your metadata values . Application configuration Paste the Application ACS URL and Application SAML audience that you copy from the Hopsworks.ai setup page. Click on Save changes . Application configuration 2 Go to the Attribute mappings tab. On the first line enter the value Subject and select unspecified for the format. then, Click on Add new attribute mapping 3 times. Attribute mapping For each of the created lines enter the following values in the first and second columns and let the format as unspecified. First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress , second: ${user:email} First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/surname , Second: ${user:familyName} First: http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname , Second: ${user:givenName} Click on Save changes . Attribute mapping 2 Return to the Configuration tab and click on Edit configuration . Edit configuration Click on Copy URL on the AWS SSO SAML metadata file line. We will call this URL Metadata URL in the coming steps. Metadata URL Go back to Hopsworks.ai. Click on Next step . Next step Give a name to your organization. This name will be used in your login URL so choose something you will remember. Here we will use hopsworks-demo . Paste the Metadata URL you copied above and click Finish . Configure Hopsworks.ai If you go back to the SSO tab of Settings you will get a logging page link. By using this link you will automatically be redirected to your identity provider to login. An account will automatically be created in hopsworks.ai for users of your organization when they log in for the first time. Configure Hopsworks.ai","title":"AWS Single Sign-On"},{"location":"setup_installation/gcp/cluster_creation/","text":"Cluster creation in Hopsworks.ai (GCP) # This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai Step 1 starting to create a cluster # In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster Step 2 setting the General information # Select the GCP Project (1) in which you want the cluster to run. Note If the Project does not appear in the drop-down, make sure that you properly Connected your GCP account for this project. Name your cluster (2). Choose the Region (3) and Zone (4) in which to deploy the cluster. Select the Instance type (5) and Local storage (6) size for the cluster Head node . Enter the name of the bucket in which the hopsworks cluster will store its data in Cloud Storage Bucket (7) Warning The bucket must be empty and must be in a region accessible from the region in which the cluster is deployed. General configuration Step 3 workers configuration # In this step, you configure the workers. There are two possible setups: static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand, for more details: documentation . Static workers configuration # You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration Autoscaling workers configuration # You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form, allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources on standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the number of workers you want to be on standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook requests the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration Step 4 select the Service Account # Hopsworks cluster store their data in a storage bucket. To let the cluster instances access the bucket we need to attach a Service Account to the virtual machines. In this step, you set which Service Account to use by entering its Email . This Service Account needs to have access right to the bucket you selected in Step 2 . For more details on how to create the Service Account and give it access to the bucket refer to Creating and configuring a storage Set the instance service account Step 5 set the backup retention policy # To backup the storage bucket data when taking a cluster backup we need to set a retention policy for the bucket. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy Step 6 VPC and Subnet selection # You can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let Hopsworks.ai create one for you. If you decide to use restricted hopsworks.ai permissions (see restrictive-permissions for more details) you will need to select an existing VPC here. Select the vpc If you selected an existing VPC in the previous step, this step lets you select which subnet of this VPC to use. If you did not select an existing virtual network in the previous step Hopsworks.ai will create a subnet for you. You can choose the CIDR block this subnet will use. Select the Subnet to be used by your cluster and press Next . Select the subnet Step 7 User management selection # In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type Step 8 Managed RonDB # Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us . Step 9 add tags to your instances. # In this step, you can define tags that will be added to the cluster virtual machines. Add tags Step 10 add an init script to your instances. # In this step, you can enter an initialization script that will be run at startup on every instance. You can select whether this script will run before or after the VM configuration. Be cautious if you select to run it before the VM configuration as this might affect Cluster creation. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script Step 11 Review and create # Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Cluster Creation"},{"location":"setup_installation/gcp/cluster_creation/#cluster-creation-in-hopsworksai-gcp","text":"This guide goes into detail for each of the steps of the cluster creation in Hopsworks.ai","title":"Cluster creation in Hopsworks.ai (GCP)"},{"location":"setup_installation/gcp/cluster_creation/#step-1-starting-to-create-a-cluster","text":"In Hopsworks.ai, select Create cluster : Create a Hopsworks cluster","title":"Step 1 starting to create a cluster"},{"location":"setup_installation/gcp/cluster_creation/#step-2-setting-the-general-information","text":"Select the GCP Project (1) in which you want the cluster to run. Note If the Project does not appear in the drop-down, make sure that you properly Connected your GCP account for this project. Name your cluster (2). Choose the Region (3) and Zone (4) in which to deploy the cluster. Select the Instance type (5) and Local storage (6) size for the cluster Head node . Enter the name of the bucket in which the hopsworks cluster will store its data in Cloud Storage Bucket (7) Warning The bucket must be empty and must be in a region accessible from the region in which the cluster is deployed. General configuration","title":"Step 2 setting the General information"},{"location":"setup_installation/gcp/cluster_creation/#step-3-workers-configuration","text":"In this step, you configure the workers. There are two possible setups: static or autoscaling. In the static setup, the cluster has a fixed number of workers that you decide. You can then add and remove workers manually, for more details: documentation . In the autoscaling setup, you configure conditions to add and remove workers and the cluster will automatically add and remove workers depending on the demand, for more details: documentation .","title":"Step 3 workers configuration"},{"location":"setup_installation/gcp/cluster_creation/#static-workers-configuration","text":"You can set the static configuration by selecting Disabled in the first drop-down (1). Then you select the number of workers you want to start the cluster with (2). And, select the Instance type (3) and Local storage size (4) for the worker nodes . Create a Hopsworks cluster, static workers configuration","title":"Static workers configuration"},{"location":"setup_installation/gcp/cluster_creation/#autoscaling-workers-configuration","text":"You can set the autoscaling configuration by selecting enabled in the first drop-down (1). You then have access to a two parts form, allowing you to configure the autoscaling. In the first part, you configure the autoscaling for general-purpose compute nodes. In the second part, you configure the autoscaling for nodes equipped with GPUs. In both parts you will have to set up the following: The instance type you want to use. You can decide to not enable the autoscaling for GPU nodes by selecting No GPU autoscale . The size of the instances' disk. The minimum number of workers. The maximum number of workers. The targeted number of standby workers. Setting some resources on standby ensures that there are always some free resources in your cluster. This ensures that requests for new resources are fulfilled promptly. You configure the standby by setting the number of workers you want to be on standby. For example, if you set a value of 0.5 the system will start a new worker every time the aggregated free cluster resources drop below 50% of a worker's resources. If you set this value to 0 new workers will only be started when a job or notebook requests the resources. The time to wait before removing unused resources. One often starts a new computation shortly after finishing the previous one. To avoid having to wait for workers to stop and start between each computation it is recommended to wait before shutting down workers. Here you set the amount of time in seconds resources need to be unused before they get removed from the system. Note The standby will not be taken into account if you set the minimum number of workers to 0 and no resources are used in the cluster. This ensures that the number of nodes can fall to 0 when no resources are used. The standby will start to take effect as soon as you start using resources. Create a Hopsworks cluster, autoscale workers configuration","title":"Autoscaling workers configuration"},{"location":"setup_installation/gcp/cluster_creation/#step-4-select-the-service-account","text":"Hopsworks cluster store their data in a storage bucket. To let the cluster instances access the bucket we need to attach a Service Account to the virtual machines. In this step, you set which Service Account to use by entering its Email . This Service Account needs to have access right to the bucket you selected in Step 2 . For more details on how to create the Service Account and give it access to the bucket refer to Creating and configuring a storage Set the instance service account","title":"Step 4 select the Service Account"},{"location":"setup_installation/gcp/cluster_creation/#step-5-set-the-backup-retention-policy","text":"To backup the storage bucket data when taking a cluster backup we need to set a retention policy for the bucket. In this step, you choose the retention period in days. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the backup retention policy","title":"Step 5 set the backup retention policy"},{"location":"setup_installation/gcp/cluster_creation/#step-6-vpc-and-subnet-selection","text":"You can select the VPC which will be used by the Hopsworks cluster. You can either select an existing VPC or let Hopsworks.ai create one for you. If you decide to use restricted hopsworks.ai permissions (see restrictive-permissions for more details) you will need to select an existing VPC here. Select the vpc If you selected an existing VPC in the previous step, this step lets you select which subnet of this VPC to use. If you did not select an existing virtual network in the previous step Hopsworks.ai will create a subnet for you. You can choose the CIDR block this subnet will use. Select the Subnet to be used by your cluster and press Next . Select the subnet","title":"Step 6 VPC and Subnet selection"},{"location":"setup_installation/gcp/cluster_creation/#step-7-user-management-selection","text":"In this step, you can choose which user management system to use. You have three choices: Managed : Hopsworks.ai automatically adds and removes users from the Hopsworks cluster when you add and remove users from your organization (more details here ). OAuth2 : integrate the cluster with your organization's OAuth2 identity provider. See Use OAuth2 for user management for more detail. LDAP : integrate the cluster with your organization's LDAP/ActiveDirectory server. See Use LDAP for user management for more detail. Disabled : let you manage users manually from within Hopsworks. Choose user management type","title":"Step 7 User management selection"},{"location":"setup_installation/gcp/cluster_creation/#step-8-managed-rondb","text":"Hopsworks uses RonDB as a database engine for its online Feature Store. By default database will run on its own VM. Premium users can scale-out database services to multiple VMs to handle increased workload. For details on how to configure RonDB check our guide here . Configure RonDB If you need to deploy a RonDB cluster instead of a single node please contact us .","title":"Step 8 Managed RonDB"},{"location":"setup_installation/gcp/cluster_creation/#step-9-add-tags-to-your-instances","text":"In this step, you can define tags that will be added to the cluster virtual machines. Add tags","title":"Step 9 add tags to your instances."},{"location":"setup_installation/gcp/cluster_creation/#step-10-add-an-init-script-to-your-instances","text":"In this step, you can enter an initialization script that will be run at startup on every instance. You can select whether this script will run before or after the VM configuration. Be cautious if you select to run it before the VM configuration as this might affect Cluster creation. Note this init script must be a bash script starting with #!/usr/bin/env bash Add initialization script","title":"Step 10 add an init script to your instances."},{"location":"setup_installation/gcp/cluster_creation/#step-11-review-and-create","text":"Review all information and select Create : Review cluster information The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart, or terminate the cluster. Running Hopsworks cluster","title":"Step 11 Review and create"},{"location":"setup_installation/gcp/getting_started/","text":"Getting started with Hopsworks.ai (Google Cloud Platform) # Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's Google Cloud Platform's (GCP) account. Prerequisite # To follow the instruction of this page you will need the following: - A GCP project in which the hopsworks cluster will be deployed. - The gcloud CLI - The gsutil tool Step 1: Connecting your GCP account # Hopsworks.ai deploys Hopsworks clusters to a project in your GCP account. Hopsworks.ai uses service account keys to connect to your GCP project. To enable this, you need to create a service account in your GCP project. Assign to the service account the required permissions. And, create a service account key JSON. For more details about creating and managing service accounts steps in GCP, see documentation . In Hopsworks.ai click on Connect to GCP or go to Settings and click on Configure next to GCP . This will direct you to a page with the instructions needed to create the service account and set up the connection. Follow the instructions. Note it is possible to limit the permissions that step up during this phase. For more details see restrictive-permissions . GCP configuration page Step 2: Creating and configuring a storage # The Hopsworks clusters deployed by Hopsworks.ai store their data in a bucket in your GCP account. To enable this you need to create a bucket and to create a service account with permissions to access the storage. Step 2.1: Creating a custom role for accessing storage # Create a file named hopsworksai_instances_role.yaml with the following content: title : Hopsworks AI Instances description : Role that allows Hopsworks AI Instances to access resources stage : GA includedPermissions : - storage.buckets.get - storage.buckets.update - storage.multipartUploads.abort - storage.multipartUploads.create - storage.multipartUploads.list - storage.multipartUploads.listParts - storage.objects.create - storage.objects.delete - storage.objects.get - storage.objects.list - storage.objects.update Note it is possible to limit the permissions that set up during this phase. For more details see restrictive-permissions . Execute the following gcloud command to create a custom role from the file. Replace [PROJECT_ID] with your GCP project id: gcloud iam roles create hopsworksai_instances \\ --project=[PROJECT_ID] \\ --file=hopsworksai_instances_role.yaml Step 2.2: Creating a service account # Execute the following gcloud command to create a service account for Hopsworks AI instances. Replace [PROJECT_ID] with your GCP project id: gcloud iam service-accounts create hopsworks-ai-instances \\ --project=[PROJECT_ID] \\ --description=\"Service account for Hopsworks AI instances\" \\ --display-name=\"Hopsworks AI instances\" Execute the following gcloud command to bind the custom role to the service account. Replace all occurrences [PROJECT_ID] with your GCP project id: gcloud projects add-iam-policy-binding [PROJECT_ID] \\ --member=\"serviceAccount:hopsworks-ai-instances@[PROJECT_ID].iam.gserviceaccount.com\" \\ --role=\"projects/[PROJECT_ID]/roles/hopsworksai_instances\" Step 2.3: Creating a Bucket # Execute the following gsutil command to create a bucket. Replace all occurrences [PROJECT_ID] with your GCP project id and [BUCKET_NAME] by the name you want to give to your bucket: gsutil mb -p [PROJECT_ID] gs://[BUCKET_NAME] Note The hopsworks cluster created by Hopsworks.ai must be in the same region as the bucket. The above command will create the bucket in the US so in the following steps, you must deploy your cluster in a US region. If you want to deploy your cluster in another part of the word us the -l option of gsutil md . For more detail about creating buckets with gsutil see the documentation Step 4: Deploying a Hopsworks cluster # In Hopsworks.ai , select Create cluster : Create a Hopsworks cluster Select the Project (1) in which you created your Bucket and Service Account (see above). Note If the Project does not appear in the drop-down, make sure that you properly Connected your GCP account for this project. Name your cluster (2). Choose the Region (3) and Zone (4) in which to deploy the cluster. Warning The cluster must be deployed in a region having access to the bucket you created above. Select the Instance type (5) and Local storage (6) size for the cluster Head node . Enter the name of the bucket you created above in Cloud Storage Bucket (7) Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Enter Email of the instances service account that you created above . If you followed the instruction it should be hopsworks-ai-instances@[PROJECT_ID].iam.gserviceaccount.com with [PROJECT_ID] the name of your project: Set the instance service account To backup the storage bucket data when taking a cluster backup we need to set a retention policy for the bucket. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit . Choose the backup retention policy Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster Step 5: Next steps # Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Getting Started"},{"location":"setup_installation/gcp/getting_started/#getting-started-with-hopsworksai-google-cloud-platform","text":"Hopsworks.ai is our managed platform for running Hopsworks and the Feature Store in the cloud. It integrates seamlessly with third-party platforms such as Databricks, SageMaker and KubeFlow. This guide shows how to set up Hopsworks.ai with your organization's Google Cloud Platform's (GCP) account.","title":"Getting started with Hopsworks.ai (Google Cloud Platform)"},{"location":"setup_installation/gcp/getting_started/#prerequisite","text":"To follow the instruction of this page you will need the following: - A GCP project in which the hopsworks cluster will be deployed. - The gcloud CLI - The gsutil tool","title":"Prerequisite"},{"location":"setup_installation/gcp/getting_started/#step-1-connecting-your-gcp-account","text":"Hopsworks.ai deploys Hopsworks clusters to a project in your GCP account. Hopsworks.ai uses service account keys to connect to your GCP project. To enable this, you need to create a service account in your GCP project. Assign to the service account the required permissions. And, create a service account key JSON. For more details about creating and managing service accounts steps in GCP, see documentation . In Hopsworks.ai click on Connect to GCP or go to Settings and click on Configure next to GCP . This will direct you to a page with the instructions needed to create the service account and set up the connection. Follow the instructions. Note it is possible to limit the permissions that step up during this phase. For more details see restrictive-permissions . GCP configuration page","title":"Step 1: Connecting your GCP account"},{"location":"setup_installation/gcp/getting_started/#step-2-creating-and-configuring-a-storage","text":"The Hopsworks clusters deployed by Hopsworks.ai store their data in a bucket in your GCP account. To enable this you need to create a bucket and to create a service account with permissions to access the storage.","title":"Step 2: Creating and configuring a storage"},{"location":"setup_installation/gcp/getting_started/#step-21-creating-a-custom-role-for-accessing-storage","text":"Create a file named hopsworksai_instances_role.yaml with the following content: title : Hopsworks AI Instances description : Role that allows Hopsworks AI Instances to access resources stage : GA includedPermissions : - storage.buckets.get - storage.buckets.update - storage.multipartUploads.abort - storage.multipartUploads.create - storage.multipartUploads.list - storage.multipartUploads.listParts - storage.objects.create - storage.objects.delete - storage.objects.get - storage.objects.list - storage.objects.update Note it is possible to limit the permissions that set up during this phase. For more details see restrictive-permissions . Execute the following gcloud command to create a custom role from the file. Replace [PROJECT_ID] with your GCP project id: gcloud iam roles create hopsworksai_instances \\ --project=[PROJECT_ID] \\ --file=hopsworksai_instances_role.yaml","title":"Step 2.1: Creating a custom role for accessing storage"},{"location":"setup_installation/gcp/getting_started/#step-22-creating-a-service-account","text":"Execute the following gcloud command to create a service account for Hopsworks AI instances. Replace [PROJECT_ID] with your GCP project id: gcloud iam service-accounts create hopsworks-ai-instances \\ --project=[PROJECT_ID] \\ --description=\"Service account for Hopsworks AI instances\" \\ --display-name=\"Hopsworks AI instances\" Execute the following gcloud command to bind the custom role to the service account. Replace all occurrences [PROJECT_ID] with your GCP project id: gcloud projects add-iam-policy-binding [PROJECT_ID] \\ --member=\"serviceAccount:hopsworks-ai-instances@[PROJECT_ID].iam.gserviceaccount.com\" \\ --role=\"projects/[PROJECT_ID]/roles/hopsworksai_instances\"","title":"Step 2.2: Creating a service account"},{"location":"setup_installation/gcp/getting_started/#step-23-creating-a-bucket","text":"Execute the following gsutil command to create a bucket. Replace all occurrences [PROJECT_ID] with your GCP project id and [BUCKET_NAME] by the name you want to give to your bucket: gsutil mb -p [PROJECT_ID] gs://[BUCKET_NAME] Note The hopsworks cluster created by Hopsworks.ai must be in the same region as the bucket. The above command will create the bucket in the US so in the following steps, you must deploy your cluster in a US region. If you want to deploy your cluster in another part of the word us the -l option of gsutil md . For more detail about creating buckets with gsutil see the documentation","title":"Step 2.3: Creating a Bucket"},{"location":"setup_installation/gcp/getting_started/#step-4-deploying-a-hopsworks-cluster","text":"In Hopsworks.ai , select Create cluster : Create a Hopsworks cluster Select the Project (1) in which you created your Bucket and Service Account (see above). Note If the Project does not appear in the drop-down, make sure that you properly Connected your GCP account for this project. Name your cluster (2). Choose the Region (3) and Zone (4) in which to deploy the cluster. Warning The cluster must be deployed in a region having access to the bucket you created above. Select the Instance type (5) and Local storage (6) size for the cluster Head node . Enter the name of the bucket you created above in Cloud Storage Bucket (7) Press Next : General configuration Select the number of workers you want to start the cluster with (2). Select the Instance type (3) and Local storage size (4) for the worker nodes . Note It is possible to add or remove workers or to enable autoscaling once the cluster is running. Press Next : Create a Hopsworks cluster, static workers configuration Enter Email of the instances service account that you created above . If you followed the instruction it should be hopsworks-ai-instances@[PROJECT_ID].iam.gserviceaccount.com with [PROJECT_ID] the name of your project: Set the instance service account To backup the storage bucket data when taking a cluster backup we need to set a retention policy for the bucket. You can deactivate the retention policy by setting this value to 0 but this will block you from taking any backup of your cluster. Choose the retention period in days and click on Review and submit . Choose the backup retention policy Review all information and select Create : Review cluster information Note We skipped cluster creation steps that are not mandatory. The cluster will start. This will take a few minutes: Booting Hopsworks cluster As soon as the cluster has started, you will be able to log in to your new Hopsworks cluster. You will also be able to stop, restart or terminate the cluster. Running Hopsworks cluster","title":"Step 4: Deploying a Hopsworks cluster"},{"location":"setup_installation/gcp/getting_started/#step-5-next-steps","text":"Check out our other guides for how to get started with Hopsworks and the Feature Store: Make Hopsworks services accessible from outside services Get started with the Hopsworks Feature Store Get started with Machine Learning on Hopsworks: HopsML Get started with Hopsworks: User Guide Code examples and notebooks: hops-examples","title":"Step 5: Next steps"},{"location":"setup_installation/gcp/restrictive_permissions/","text":"Limiting GCP permissions # Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s GCP project. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege. Limiting the Account Service Account permissions # Some of the permissions set up when connection your GCP account to Hopsworks.ai ( here ) can be removed under certain conditions. Backup permissions # The following permissions are only needed for the backup feature. If you are not going to create backups or if you do not have access to this Enterprise feature, you can limit the permission of the Service Account by removing them. - compute.disks.createSnapshot - compute.snapshots.create - compute.snapshots.delete - compute.snapshots.setLabels - compute.snapshots.get - compute.snapshots.useReadOnly Instance type modification permissions # The following permission is only needed to be able to change the head node and RonDB nodes instance type on an existing cluster ( documentation ). If you are not going to use this feature, you can limit the permission of the Service Account by removing it. - compute.instances.setMachineType Create a VPC permissions # The following permissions are only needed if you want Hopsworks.ai to create VPC and subnet for you. If you choose an existing VPC and subnet, you can limit the permission of the Service Account by removing them. - compute.networks.create - compute.networks.delete - compute.networks.get ... - compute.subnetworks.create - compute.subnetworks.delete - compute.subnetworks.get Limiting the Instances Service Account permissions # Some of the permissions set up for the instances service account used during cluster creation ( here ) can be removed under certain conditions. Backups # If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your instances service account. For this remove the following permission from your instances service account : storage.buckets.update","title":"Limiting Permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#limiting-gcp-permissions","text":"Hopsworks.ai requires a set of permissions to be able to manage resources in the user\u2019s GCP project. By default, these permissions are set to easily allow a wide range of different configurations and allow us to automate as many steps as possible. While we ensure to never access resources we shouldn\u2019t, we do understand that this might not be enough for your organization or security policy. This guide explains how to lock down access permissions following the IT security policy principle of least privilege.","title":"Limiting GCP permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#limiting-the-account-service-account-permissions","text":"Some of the permissions set up when connection your GCP account to Hopsworks.ai ( here ) can be removed under certain conditions.","title":"Limiting the Account Service Account permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#backup-permissions","text":"The following permissions are only needed for the backup feature. If you are not going to create backups or if you do not have access to this Enterprise feature, you can limit the permission of the Service Account by removing them. - compute.disks.createSnapshot - compute.snapshots.create - compute.snapshots.delete - compute.snapshots.setLabels - compute.snapshots.get - compute.snapshots.useReadOnly","title":"Backup permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#instance-type-modification-permissions","text":"The following permission is only needed to be able to change the head node and RonDB nodes instance type on an existing cluster ( documentation ). If you are not going to use this feature, you can limit the permission of the Service Account by removing it. - compute.instances.setMachineType","title":"Instance type modification permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#create-a-vpc-permissions","text":"The following permissions are only needed if you want Hopsworks.ai to create VPC and subnet for you. If you choose an existing VPC and subnet, you can limit the permission of the Service Account by removing them. - compute.networks.create - compute.networks.delete - compute.networks.get ... - compute.subnetworks.create - compute.subnetworks.delete - compute.subnetworks.get","title":"Create a VPC permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#limiting-the-instances-service-account-permissions","text":"Some of the permissions set up for the instances service account used during cluster creation ( here ) can be removed under certain conditions.","title":"Limiting the Instances Service Account permissions"},{"location":"setup_installation/gcp/restrictive_permissions/#backups","text":"If you do not intend to take backups or if you do not have access to this Enterprise feature you can remove the permissions that are only used by the backup feature when configuring your instances service account. For this remove the following permission from your instances service account : storage.buckets.update","title":"Backups"},{"location":"setup_installation/on_prem/hopsworks_installer/","text":"Hopsworks Installer # The hopsworks-installer.sh script downloads, configures, and installs Hopsworks. It is typically run interactively, prompting the user about details of what to be is installed and where. It can also be run non-interactively (no user prompts) using the '-ni' switch. Requirements # You need at least one server or virtual machine on which Hopsworks will be installed with at least the following specification: Centos/RHEL 7.x or Ubuntu 18.04; at least 32GB RAM, at least 8 CPUs, 100 GB of free hard-disk space, outside Internet access (if this server is air-gapped, contact us for support), a UNIX user account with sudo privileges. Quickstart # To install on your single server or VM, run the following bash commands from your user account with sudo privileges: wget https://raw.githubusercontent.com/logicalclocks/karamel-chef/master/hopsworks-installer.sh chmod +x hopsworks-installer.sh ./hopsworks-installer.sh Installation takes roughly 1 hr, or slower if your server/VM has a low-bandwidth Internet connection. Once your installation is finished, you can stop (and start) Hopsworks using the commands: sudo /srv/hops/kagent/kagent/bin/shutdown-all-local-services.sh sudo /srv/hops/kagent/kagent/bin/start-all-local-services.sh Note that all services in Hopsworks are systemd services that are enabled by default, that is, they will restart when VM/server is rebooted. Multi-node installation # Configure installer ssh access # You need to identify a set of servers/VMs and create the same user account with sudo privileges on all the servers. You should identify one server as the head server from which you will perform the installation. You need to ensure that the nodes can communicate with each other on every port and to configure password-less SSH Access from the Head node to Worker nodes. First, on the head node, you should create an openssh keypair without a password: cat /dev/zero | ssh-keygen -m PEM -q -N \"\" cat ~/.ssh/id_rsa.pub The second line above will print the public key for the sudo account on the head node. Copy that public key, and append it to the ~/.ssh/authorized_keys files on all worker nodes, so that that the sudo account on the head node can SSH into the worker nodes without a password. It may be that you need to configure your sshd daemon (sshd_config and sshlogin) to allow openssh-key based login, depending on how your server is configured: How to setup passwordless ssh For both Ubuntu and Centos/RHEL, and assuming the sudo account is 'ubuntu' and our three worker nodes have hostnames 'vm1', 'vm2', and 'vm3', then you could run the following: ssh-copy-id -i $HOME /.ssh/id_rsa.pub ubuntu@vm1 ssh-copy-id -i $HOME /.ssh/id_rsa.pub ubuntu@vm2 ssh-copy-id -i $HOME /.ssh/id_rsa.pub ubuntu@vm3 Test that you now have passwordless SSH access to all the worker nodes from the head node (assuming 'ubuntu' is the sudo account): # from the head VM ssh ubuntu@vm1 ssh ubuntu@vm2 ssh ubuntu@vm3 Start installation # On the head node, in the sudo account, download and run this script that installs Hopsworks on all hosts. It will ask you to enter the IP address of all the workers during installation: wget https://raw.githubusercontent.com/logicalclocks/karamel-chef/master/hopsworks-installer.sh chmod +x hopsworks-installer.sh ./hopsworks-installer.sh The above script will download and install Karamel on the same server that runs the script. Karamel will install Hopsworks across all hosts. Installation takes roughly 1 hr, slightly longer for large clusters. To find out more about Karamel, read more below. Purge an Existing Cluster Installation # ./hopsworks-installer.sh -i purge -ni Installation from behind an HTTP Proxy (firewall) # Installation will not work if your http proxy has a self-signed certificate. You can explicitly specify the http proxy by passing the '-p' switch to the installer. ./hopsworks-installer.sh -p https://1.2.3.4:3283 If you have set the environment variable http_proxy or https_proxy, hopsworks-installer.sh will use it, even if you don't specify the '-p-' switch. The '-p' switch overrides the environment variable, if both are set. If both http_proxy and https_proxy environment variables are set, it will favour the http_proxy environment variable. You can change this behavior using the following arguments '-p $https_proxy'. Air-gapped installation # Hopsworks can be installed in an air-gapped environment. We recommend that you submit a contact form for help in installing in an environment without outbound Internet access. Installation Script Options # usage: [ sudo ] ./hopsworks-installer.sh [ -h | --help ] help message [ -i | --install-action localhost | localhost-tls | cluster | enterprise | karamel | purge | purge-all ] 'localhost' installs a localhost Hopsworks cluster 'localhost-tls' installs a localhost Hopsworks cluster with TLS enabled 'cluster' installs a multi-host Hopsworks cluster 'enterprise' installs a multi-host Enterprise Hopsworks cluster 'kubernetes' installs a multi-host Enterprise Hopsworks cluster with Kubernetes 'karamel' installs and starts Karamel 'purge' removes Hopsworks completely from this host 'purge-all' removes Hopsworks completely from ALL hosts [ -cl | --clean ] removes the karamel installation [ -dr | --dry-run ] does not run karamel, just generates YML file [ -nvme | --nvme num_disks ] Number of NVMe disks on worker nodes ( for NDB/HopsFS ) [ -c | --cloud on-premises | gcp | aws | azure ] [ -w | --workers IP1,IP2,...,IPN | none ] install on workers with IPs in supplied list ( or none ) . Uses default mem/cpu/gpus for the workers. [ -de | --download-enterprise-url url ] downloads enterprise binaries from this URL. [ -dc | --download-opensource-url url ] downloads open-source binaries from this URL. [ -du | --download-user username ] Username for downloading enterprise binaries. [ -dp | --download-password password ] Password for downloading enterprise binaries. [ -gs | --gem-server ] Run a local gem server for chef-solo ( for air-gapped installations ) . [ -ni | --non-interactive )] skip license/terms acceptance and all confirmation screens. [ -p | --http-proxy ) url ] URL of the http ( s ) proxy server. Only https proxies with valid certs supported. [ -pwd | --password password ] sudo password for user running chef recipes. [ -y | --yml yaml_file ] yaml file to run Karamel against.","title":"Hopsworks Installer"},{"location":"setup_installation/on_prem/hopsworks_installer/#hopsworks-installer","text":"The hopsworks-installer.sh script downloads, configures, and installs Hopsworks. It is typically run interactively, prompting the user about details of what to be is installed and where. It can also be run non-interactively (no user prompts) using the '-ni' switch.","title":"Hopsworks Installer"},{"location":"setup_installation/on_prem/hopsworks_installer/#requirements","text":"You need at least one server or virtual machine on which Hopsworks will be installed with at least the following specification: Centos/RHEL 7.x or Ubuntu 18.04; at least 32GB RAM, at least 8 CPUs, 100 GB of free hard-disk space, outside Internet access (if this server is air-gapped, contact us for support), a UNIX user account with sudo privileges.","title":"Requirements"},{"location":"setup_installation/on_prem/hopsworks_installer/#quickstart","text":"To install on your single server or VM, run the following bash commands from your user account with sudo privileges: wget https://raw.githubusercontent.com/logicalclocks/karamel-chef/master/hopsworks-installer.sh chmod +x hopsworks-installer.sh ./hopsworks-installer.sh Installation takes roughly 1 hr, or slower if your server/VM has a low-bandwidth Internet connection. Once your installation is finished, you can stop (and start) Hopsworks using the commands: sudo /srv/hops/kagent/kagent/bin/shutdown-all-local-services.sh sudo /srv/hops/kagent/kagent/bin/start-all-local-services.sh Note that all services in Hopsworks are systemd services that are enabled by default, that is, they will restart when VM/server is rebooted.","title":"Quickstart"},{"location":"setup_installation/on_prem/hopsworks_installer/#multi-node-installation","text":"","title":"Multi-node installation"},{"location":"setup_installation/on_prem/hopsworks_installer/#configure-installer-ssh-access","text":"You need to identify a set of servers/VMs and create the same user account with sudo privileges on all the servers. You should identify one server as the head server from which you will perform the installation. You need to ensure that the nodes can communicate with each other on every port and to configure password-less SSH Access from the Head node to Worker nodes. First, on the head node, you should create an openssh keypair without a password: cat /dev/zero | ssh-keygen -m PEM -q -N \"\" cat ~/.ssh/id_rsa.pub The second line above will print the public key for the sudo account on the head node. Copy that public key, and append it to the ~/.ssh/authorized_keys files on all worker nodes, so that that the sudo account on the head node can SSH into the worker nodes without a password. It may be that you need to configure your sshd daemon (sshd_config and sshlogin) to allow openssh-key based login, depending on how your server is configured: How to setup passwordless ssh For both Ubuntu and Centos/RHEL, and assuming the sudo account is 'ubuntu' and our three worker nodes have hostnames 'vm1', 'vm2', and 'vm3', then you could run the following: ssh-copy-id -i $HOME /.ssh/id_rsa.pub ubuntu@vm1 ssh-copy-id -i $HOME /.ssh/id_rsa.pub ubuntu@vm2 ssh-copy-id -i $HOME /.ssh/id_rsa.pub ubuntu@vm3 Test that you now have passwordless SSH access to all the worker nodes from the head node (assuming 'ubuntu' is the sudo account): # from the head VM ssh ubuntu@vm1 ssh ubuntu@vm2 ssh ubuntu@vm3","title":"Configure installer ssh access"},{"location":"setup_installation/on_prem/hopsworks_installer/#start-installation","text":"On the head node, in the sudo account, download and run this script that installs Hopsworks on all hosts. It will ask you to enter the IP address of all the workers during installation: wget https://raw.githubusercontent.com/logicalclocks/karamel-chef/master/hopsworks-installer.sh chmod +x hopsworks-installer.sh ./hopsworks-installer.sh The above script will download and install Karamel on the same server that runs the script. Karamel will install Hopsworks across all hosts. Installation takes roughly 1 hr, slightly longer for large clusters. To find out more about Karamel, read more below.","title":"Start installation"},{"location":"setup_installation/on_prem/hopsworks_installer/#purge-an-existing-cluster-installation","text":"./hopsworks-installer.sh -i purge -ni","title":"Purge an Existing Cluster Installation"},{"location":"setup_installation/on_prem/hopsworks_installer/#installation-from-behind-an-http-proxy-firewall","text":"Installation will not work if your http proxy has a self-signed certificate. You can explicitly specify the http proxy by passing the '-p' switch to the installer. ./hopsworks-installer.sh -p https://1.2.3.4:3283 If you have set the environment variable http_proxy or https_proxy, hopsworks-installer.sh will use it, even if you don't specify the '-p-' switch. The '-p' switch overrides the environment variable, if both are set. If both http_proxy and https_proxy environment variables are set, it will favour the http_proxy environment variable. You can change this behavior using the following arguments '-p $https_proxy'.","title":"Installation from behind an HTTP Proxy (firewall)"},{"location":"setup_installation/on_prem/hopsworks_installer/#air-gapped-installation","text":"Hopsworks can be installed in an air-gapped environment. We recommend that you submit a contact form for help in installing in an environment without outbound Internet access.","title":"Air-gapped installation"},{"location":"setup_installation/on_prem/hopsworks_installer/#installation-script-options","text":"usage: [ sudo ] ./hopsworks-installer.sh [ -h | --help ] help message [ -i | --install-action localhost | localhost-tls | cluster | enterprise | karamel | purge | purge-all ] 'localhost' installs a localhost Hopsworks cluster 'localhost-tls' installs a localhost Hopsworks cluster with TLS enabled 'cluster' installs a multi-host Hopsworks cluster 'enterprise' installs a multi-host Enterprise Hopsworks cluster 'kubernetes' installs a multi-host Enterprise Hopsworks cluster with Kubernetes 'karamel' installs and starts Karamel 'purge' removes Hopsworks completely from this host 'purge-all' removes Hopsworks completely from ALL hosts [ -cl | --clean ] removes the karamel installation [ -dr | --dry-run ] does not run karamel, just generates YML file [ -nvme | --nvme num_disks ] Number of NVMe disks on worker nodes ( for NDB/HopsFS ) [ -c | --cloud on-premises | gcp | aws | azure ] [ -w | --workers IP1,IP2,...,IPN | none ] install on workers with IPs in supplied list ( or none ) . Uses default mem/cpu/gpus for the workers. [ -de | --download-enterprise-url url ] downloads enterprise binaries from this URL. [ -dc | --download-opensource-url url ] downloads open-source binaries from this URL. [ -du | --download-user username ] Username for downloading enterprise binaries. [ -dp | --download-password password ] Password for downloading enterprise binaries. [ -gs | --gem-server ] Run a local gem server for chef-solo ( for air-gapped installations ) . [ -ni | --non-interactive )] skip license/terms acceptance and all confirmation screens. [ -p | --http-proxy ) url ] URL of the http ( s ) proxy server. Only https proxies with valid certs supported. [ -pwd | --password password ] sudo password for user running chef recipes. [ -y | --yml yaml_file ] yaml file to run Karamel against.","title":"Installation Script Options"},{"location":"user_guides/","text":"How-To Guides # This section serves to provide guides and examples for the common usage of abstractions and functionality of the Hopsworks platform through the Hopsworks UI and APIs. Feature Store : Learn about the common usage of the core Hopsworks Feature Store abstractions, such as Feature Groups , Feature Views and Storage Connectors . MLOps : Learn about the common usage of Hopsworks MLOps abstractions, such as the Model Registry or Model Serving. Integrations : Hopsworks is an open platform aiming to be accessible from a variety of tools. Learn in this section how to connect to connect to Hopsworks from local python environments, Databricks or more. Projects : The core abstraction on Hopsworks are Projects . Learn in this section how to manage your projects and the services therein.","title":"How-To Guides"},{"location":"user_guides/#how-to-guides","text":"This section serves to provide guides and examples for the common usage of abstractions and functionality of the Hopsworks platform through the Hopsworks UI and APIs. Feature Store : Learn about the common usage of the core Hopsworks Feature Store abstractions, such as Feature Groups , Feature Views and Storage Connectors . MLOps : Learn about the common usage of Hopsworks MLOps abstractions, such as the Model Registry or Model Serving. Integrations : Hopsworks is an open platform aiming to be accessible from a variety of tools. Learn in this section how to connect to connect to Hopsworks from local python environments, Databricks or more. Projects : The core abstraction on Hopsworks are Projects . Learn in this section how to manage your projects and the services therein.","title":"How-To Guides"},{"location":"user_guides/fs/","text":"Feature Store Guides # This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Store through the Hopsworks UI and APIs. Storage Connectors Feature Groups Feature Views Tags and Keywords","title":"Feature Store Guides"},{"location":"user_guides/fs/#feature-store-guides","text":"This section serves to provide guides and examples for the common usage of abstractions and functionality of the Feature Store through the Hopsworks UI and APIs. Storage Connectors Feature Groups Feature Views Tags and Keywords","title":"Feature Store Guides"},{"location":"user_guides/fs/dummy/","text":"How To [Install/Configure/Do Something] on [Distribution] # Introduction # Introductory paragraph about the topic that explains what this topic is about and why the reader should care; what problem does it solve? In this guide, you will [configure/set up/build/deploy] [some thing]... When you're finished, you'll be able to... Prerequisites # Before you begin this guide you'll need the following: [number of servers] server(s) A non-root user with sudo privileges ( ) explains how to set this up.) (Optional) If software such as Nginx needs to be installed, link to the proper article describing how to install it. (Optional) If the reader needs a fully-qualified domain name (FQDN), mention it here as well. (Optional) List any other accounts needed, such as GitHub, Facebook, or other services. Step 1 \u2014 Doing Something # Introduction to the step. What are we going to do and why are we doing it? First.... Next... Finally... Display the status of your firewall with the following command: sudo ufw status You'll see the following output: [secondary_label Output] Status: active To Action From -- ------ ---- OpenSSH ALLOW Anywhere <^>Nginx HTTP ALLOW Anywhere<^> OpenSSH (v6) ALLOW Anywhere (v6) <^>Nginx HTTP (v6) ALLOW Anywhere (v6)<^> Open the file /etc/nginx.conf in your editor: sudo nano /etc/nginx.com Change the root field so it points to /var/www/your_domain : [label /etc/nginx/sites-available/default] server { listen 80 default_server ; listen [::]:80 default_server ipv6only=on ; root <^>/var/www/your_domain<^> ; index index.html index.htm ; server_name localhost ; location / { try_files $uri $uri/ = 404 ; } } Now transition to the next step by telling the reader what's next. Step 2 \u2014 Title Case # Another introduction Your content Transition to the next step. Step 3 \u2014 Title Case # Another introduction Your content Transition to the next step. Conclusion # In this article you [configured/set up/built/deployed] [something]. Now you can....","title":"Tags and Keywords"},{"location":"user_guides/fs/dummy/#how-to-installconfiguredo-something-on-distribution","text":"","title":"How To [Install/Configure/Do Something] on [Distribution]"},{"location":"user_guides/fs/dummy/#introduction","text":"Introductory paragraph about the topic that explains what this topic is about and why the reader should care; what problem does it solve? In this guide, you will [configure/set up/build/deploy] [some thing]... When you're finished, you'll be able to...","title":"Introduction"},{"location":"user_guides/fs/dummy/#prerequisites","text":"Before you begin this guide you'll need the following: [number of servers] server(s) A non-root user with sudo privileges ( ) explains how to set this up.) (Optional) If software such as Nginx needs to be installed, link to the proper article describing how to install it. (Optional) If the reader needs a fully-qualified domain name (FQDN), mention it here as well. (Optional) List any other accounts needed, such as GitHub, Facebook, or other services.","title":"Prerequisites"},{"location":"user_guides/fs/dummy/#step-1-doing-something","text":"Introduction to the step. What are we going to do and why are we doing it? First.... Next... Finally... Display the status of your firewall with the following command: sudo ufw status You'll see the following output: [secondary_label Output] Status: active To Action From -- ------ ---- OpenSSH ALLOW Anywhere <^>Nginx HTTP ALLOW Anywhere<^> OpenSSH (v6) ALLOW Anywhere (v6) <^>Nginx HTTP (v6) ALLOW Anywhere (v6)<^> Open the file /etc/nginx.conf in your editor: sudo nano /etc/nginx.com Change the root field so it points to /var/www/your_domain : [label /etc/nginx/sites-available/default] server { listen 80 default_server ; listen [::]:80 default_server ipv6only=on ; root <^>/var/www/your_domain<^> ; index index.html index.htm ; server_name localhost ; location / { try_files $uri $uri/ = 404 ; } } Now transition to the next step by telling the reader what's next.","title":"Step 1 \u2014 Doing Something"},{"location":"user_guides/fs/dummy/#step-2-title-case","text":"Another introduction Your content Transition to the next step.","title":"Step 2 \u2014 Title Case"},{"location":"user_guides/fs/dummy/#step-3-title-case","text":"Another introduction Your content Transition to the next step.","title":"Step 3 \u2014 Title Case"},{"location":"user_guides/fs/dummy/#conclusion","text":"In this article you [configured/set up/built/deployed] [something]. Now you can....","title":"Conclusion"},{"location":"user_guides/fs/feature_group/","text":"","title":"Index"},{"location":"user_guides/fs/feature_group/create/","text":"How to create a Feature Group # Introduction # In this guide you will learn how to create and register a feature group with Hopsworks. This guide covers creating a feature group using the HSFS APIs as well as the user interface. Prerequisites # Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. Create using the HSFS APIs # To create a feature group using the HSFS APIs, you need to provide a Pandas or Spark DataFrame. The DataFrame will contain all the features you want to register within the feature group, as well as the primary key, event time and partition key. Create a Feature Group # The first step to create a feature group is to create the API metadata object representing a feature group. Using the HSFS API you can execute: Streaming Write API # Python PySpark fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' ) fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , stream = True ) Batch Write API # PySpark fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , ) The full method documentation is available here . name is the only mandatory parameter of the create_feature_group and represents the name of the feature group. In the example above we created the first version of a feature group named weather , we provide a description to make it searchable to the other project members, as well as making the feature group available online. Additionally we specify which columns of the DataFrame will be used as primary key, partition key and event time. Composite primary key and multi level partitioning is also supported. The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the the highest existing version number plus one. The last parameter used in the examples above is stream . The stream parameter controls whether to enable the streaming write APIs to the online and offline feature store. When using the APIs in a Python environment this behavior is the default. Register the metadata and save the feature data # The snippet above only created the metadata object on the Python interpreter running the code. To register the feature group metadata and to save the feature data with Hopsworks, you should invoke the save method: fg . save ( df ) The save method takes in input a Pandas or Spark DataFrame. HSFS will use the DataFrame columns and types to determine the name and types of features, primary key, partition key and event time. The DataFrame must contains the columns specified as primary keys, partition key and event time in the create_feature_group call. If a feature group is online enabled, the save method will store the feature data to both the online and offline storage. API Reference # FeatureGroup Create using the UI # You can also create a new feature group through the UI.","title":"Create"},{"location":"user_guides/fs/feature_group/create/#how-to-create-a-feature-group","text":"","title":"How to create a Feature Group"},{"location":"user_guides/fs/feature_group/create/#introduction","text":"In this guide you will learn how to create and register a feature group with Hopsworks. This guide covers creating a feature group using the HSFS APIs as well as the user interface.","title":"Introduction"},{"location":"user_guides/fs/feature_group/create/#prerequisites","text":"Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/create/#create-using-the-hsfs-apis","text":"To create a feature group using the HSFS APIs, you need to provide a Pandas or Spark DataFrame. The DataFrame will contain all the features you want to register within the feature group, as well as the primary key, event time and partition key.","title":"Create using the HSFS APIs"},{"location":"user_guides/fs/feature_group/create/#create-a-feature-group","text":"The first step to create a feature group is to create the API metadata object representing a feature group. Using the HSFS API you can execute:","title":"Create a Feature Group"},{"location":"user_guides/fs/feature_group/create/#streaming-write-api","text":"Python PySpark fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' ) fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , stream = True )","title":"Streaming Write API"},{"location":"user_guides/fs/feature_group/create/#batch-write-api","text":"PySpark fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , ) The full method documentation is available here . name is the only mandatory parameter of the create_feature_group and represents the name of the feature group. In the example above we created the first version of a feature group named weather , we provide a description to make it searchable to the other project members, as well as making the feature group available online. Additionally we specify which columns of the DataFrame will be used as primary key, partition key and event time. Composite primary key and multi level partitioning is also supported. The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the the highest existing version number plus one. The last parameter used in the examples above is stream . The stream parameter controls whether to enable the streaming write APIs to the online and offline feature store. When using the APIs in a Python environment this behavior is the default.","title":"Batch Write API"},{"location":"user_guides/fs/feature_group/create/#register-the-metadata-and-save-the-feature-data","text":"The snippet above only created the metadata object on the Python interpreter running the code. To register the feature group metadata and to save the feature data with Hopsworks, you should invoke the save method: fg . save ( df ) The save method takes in input a Pandas or Spark DataFrame. HSFS will use the DataFrame columns and types to determine the name and types of features, primary key, partition key and event time. The DataFrame must contains the columns specified as primary keys, partition key and event time in the create_feature_group call. If a feature group is online enabled, the save method will store the feature data to both the online and offline storage.","title":"Register the metadata and save the feature data"},{"location":"user_guides/fs/feature_group/create/#api-reference","text":"FeatureGroup","title":"API Reference"},{"location":"user_guides/fs/feature_group/create/#create-using-the-ui","text":"You can also create a new feature group through the UI.","title":"Create using the UI"},{"location":"user_guides/fs/feature_group/create_external/","text":"How to create an External Feature Group # Introduction # In this guide you will learn how to create and register an external feature group with Hopsworks. This guide covers creating an external feature group using the HSFS APIs as well as the user interface. Prerequisites # Before you begin this guide we suggest you read the External Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. Create using the HSFS APIs # Retrieve the storage connector # To create an external feature group using the HSFS APIs you need to provide an existing storage connector . Python connector = feature_store . get_storage_connector ( \"connector_name\" ) Create an External Feature Group # SQL based external feature group # Python query = \"\"\" SELECT TO_NUMERIC(ss_store_sk) AS ss_store_sk , AVG(ss_net_profit) AS avg_ss_net_profit , SUM(ss_net_profit) AS total_ss_net_profit , AVG(ss_list_price) AS avg_ss_list_price , AVG(ss_coupon_amt) AS avg_ss_coupon_amt , sale_date , ss_store_sk FROM STORE_SALES GROUP BY ss_store_sk, sales_date \"\"\" fg = feature_store . create_on_demand_feature_group ( name = \"sales\" , version = 1 description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) Data Lake based external feature group # Python fg = feature_store . create_on_demand_feature_group ( name = \"sales\" , version = 1 description = \"Physical shop sales features\" , data_format = \"parquet\" , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) The full method documentation is available here . name is a mandatory parameter of the create_on_demand_feature_group and represents the name of the feature group. The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the the highest existing version number plus one. If the storage connector is defined for a data warehouse (e.g. JDBC, Snowflake, Redshift) you need to provide a SQL statement that will be executed to compute the features. If the storage connector is defined for a data lake, the location of the data as well as the format need to be provided. Additionally we specify which columns of the DataFrame will be used as primary key, and event time. Composite primary keys are also supported. Register the metadata # The snippet above only created the metadata object on the Python interpreter running the code. To register the external feature group metadata with Hopsworks, you should invoke the save method: Python fg . save ( df ) Limitations # Hopsworks Feature Store does not support time-travel capabilities for on-demand feature groups. Moreover, as the data resides on external systems, on-demand feature groups cannot be made available online for low latency serving. To make data from an on-demand feature group available online, users need to define an online enabled feature group and hava a job that periodically reads data from the on-demand feature group and writes in the online feature group. Python support Currently the HSFS library does not support calling the read() or show() methods on on-demand feature groups. Likewise it is not possibile to call the read() or show() methods on queries containing on-demand feature groups. Nevertheless, on-demand feature groups can be used from a Python engine to create training datasets. API Reference # FeatureGroup Create using the UI # You can also create a new feature group through the UI.","title":"Create External"},{"location":"user_guides/fs/feature_group/create_external/#how-to-create-an-external-feature-group","text":"","title":"How to create an External Feature Group"},{"location":"user_guides/fs/feature_group/create_external/#introduction","text":"In this guide you will learn how to create and register an external feature group with Hopsworks. This guide covers creating an external feature group using the HSFS APIs as well as the user interface.","title":"Introduction"},{"location":"user_guides/fs/feature_group/create_external/#prerequisites","text":"Before you begin this guide we suggest you read the External Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline.","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/create_external/#create-using-the-hsfs-apis","text":"","title":"Create using the HSFS APIs"},{"location":"user_guides/fs/feature_group/create_external/#retrieve-the-storage-connector","text":"To create an external feature group using the HSFS APIs you need to provide an existing storage connector . Python connector = feature_store . get_storage_connector ( \"connector_name\" )","title":"Retrieve the storage connector"},{"location":"user_guides/fs/feature_group/create_external/#create-an-external-feature-group","text":"","title":"Create an External Feature Group"},{"location":"user_guides/fs/feature_group/create_external/#sql-based-external-feature-group","text":"Python query = \"\"\" SELECT TO_NUMERIC(ss_store_sk) AS ss_store_sk , AVG(ss_net_profit) AS avg_ss_net_profit , SUM(ss_net_profit) AS total_ss_net_profit , AVG(ss_list_price) AS avg_ss_list_price , AVG(ss_coupon_amt) AS avg_ss_coupon_amt , sale_date , ss_store_sk FROM STORE_SALES GROUP BY ss_store_sk, sales_date \"\"\" fg = feature_store . create_on_demand_feature_group ( name = \"sales\" , version = 1 description = \"Physical shop sales features\" , query = query , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' )","title":"SQL based external feature group"},{"location":"user_guides/fs/feature_group/create_external/#data-lake-based-external-feature-group","text":"Python fg = feature_store . create_on_demand_feature_group ( name = \"sales\" , version = 1 description = \"Physical shop sales features\" , data_format = \"parquet\" , storage_connector = connector , primary_key = [ 'ss_store_sk' ], event_time = 'sale_date' ) The full method documentation is available here . name is a mandatory parameter of the create_on_demand_feature_group and represents the name of the feature group. The version number is optional, if you don't specify the version number the APIs will create a new version by default with a version number equals to the the highest existing version number plus one. If the storage connector is defined for a data warehouse (e.g. JDBC, Snowflake, Redshift) you need to provide a SQL statement that will be executed to compute the features. If the storage connector is defined for a data lake, the location of the data as well as the format need to be provided. Additionally we specify which columns of the DataFrame will be used as primary key, and event time. Composite primary keys are also supported.","title":"Data Lake based external feature group"},{"location":"user_guides/fs/feature_group/create_external/#register-the-metadata","text":"The snippet above only created the metadata object on the Python interpreter running the code. To register the external feature group metadata with Hopsworks, you should invoke the save method: Python fg . save ( df )","title":"Register the metadata"},{"location":"user_guides/fs/feature_group/create_external/#limitations","text":"Hopsworks Feature Store does not support time-travel capabilities for on-demand feature groups. Moreover, as the data resides on external systems, on-demand feature groups cannot be made available online for low latency serving. To make data from an on-demand feature group available online, users need to define an online enabled feature group and hava a job that periodically reads data from the on-demand feature group and writes in the online feature group. Python support Currently the HSFS library does not support calling the read() or show() methods on on-demand feature groups. Likewise it is not possibile to call the read() or show() methods on queries containing on-demand feature groups. Nevertheless, on-demand feature groups can be used from a Python engine to create training datasets.","title":"Limitations"},{"location":"user_guides/fs/feature_group/create_external/#api-reference","text":"FeatureGroup","title":"API Reference"},{"location":"user_guides/fs/feature_group/create_external/#create-using-the-ui","text":"You can also create a new feature group through the UI.","title":"Create using the UI"},{"location":"user_guides/fs/feature_group/data_types/","text":"How to manage schema and feature data types # Introduction # In this guide you will learn how to manage the feature group schema and control the data type of the features in a feature group. Prerequisites # Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize with the APIs to create a feature group . Feature group schema # When a feature is stored in the both the online and offline feature stores, it will be stored in a data type native to each store. Offline data type : The data type of the feature when stored on the offline feature store Online data type : The data type of the feature when stored on the online feature store. The offline data type is always required, even if the feature group is stored only online. On the other hand, if the feature group is not online_enabled , its features will not have an online data type. The offline and online types for each feature are identified based on the types of the columns in the Spark or Pandas DataFrame, and those types are then mapped to the online and offline data types. In the case of a Spark DataFrame, the Spark types will be mapped to the corresponding Hive Metastore type and used as offline data type. When registering a Pandas DataFrame as a feature group, the following mapping rules are applied: Pandas Type Offline Feature Type int32 INT int64 BIGINT float32 FLOAT float64 DOUBLE datetime64[ns] TIMESTAMP object STRING If the feature group is online enabled, Hopsworks will then map the offline data type to the corresponding online data type. The mapping is based on the following rules: If the offline data type is also supported on the online feature store (e.g. INT, FLOAT, DATE, TIMESTAMP), the online data type will be the same as the offline data type If the offline data type is boolean , the online data type is going to be set as tinyint If the offline data type is string , the online data type is going to be set as varchar(100) If the offline data type is not supported by the online feature store and it is not one of the above exception, the online data type will be set as varbinary(100) to handle complex types. Offline data types # The offline feature store is based on Apache Hudi and Hive Metastore, as such, any Hive Data Type can be leveraged. Potential offline types are: \"TINYINT\" , \"SMALLINT\" , \"INT\" , \"BIGINT\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"TIMESTAMP\" , \"DATE\" , \"STRING\" , \"BOOLEAN\" , \"BINARY\" , \"ARRAY <TINYINT>\" , \"ARRAY <SMALLINT>\" , \"ARRAY <INT>\" , \"ARRAY <BIGINT>\" , \"ARRAY <FLOAT>\" , \"ARRAY <DOUBLE>\" , \"ARRAY <DECIMAL>\" , \"ARRAY <TIMESTAMP>\" , \"ARRAY <DATE>\" , \"ARRAY <STRING>\" , \"ARRAY <BOOLEAN>\" , \"ARRAY <BINARY>\" , \"ARRAY <ARRAY <FLOAT> >\" , \"ARRAY <ARRAY <INT> >\" , \"ARRAY <ARRAY <STRING> >\" , \"MAP <FLOAT, FLOAT>\" , \"MAP <FLOAT, STRING>\" , \"MAP <FLOAT, INT>\" , \"MAP <FLOAT, BINARY>\" , \"MAP <INT, INT>\" , \"MAP <INT, STRING>\" , \"MAP <INT, BINARY>\" , \"MAP <INT, FLOAT>\" , \"MAP <INT, ARRAY <FLOAT> >\" , \"STRUCT < label: STRING, index: INT >\" , \"UNIONTYPE < STRING, INT>\" Online data types # The online storage is based on RonDB and hence, any MySQL Data Type can be leveraged. Potential online types are: \"None\" , \"INT(11)\" , \"TINYINT(1)\" , \"SMALLINT(5)\" , \"MEDIUMINT(7)\" , \"BIGINT(20)\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"DATE\" , \"DATETIME\" , \"TIMESTAMP\" , \"TIME\" , \"YEAR\" , \"CHAR\" , \"VARCHAR(n)\" , \"BINARY\" , \"VARBINARY(n)\" , \"BLOB\" , \"TEXT\" , \"TINYBLOB\" , \"TINYTEXT\" , \"MEDIUMBLOB\" , \"MEDIUMTEXT\" , \"LONGBLOB\" , \"LONGTEXT\" , \"JSON\" Complex online data types # Additionally to the online types above, Hopsworks allows users to store complex types (e.g. ARRAY ) on the online feature store. Hopsworks serializes the complex features transparently and stores them as VARBINARY in the online feature store. The serialization happens when calling the save() , insert() or insert_stream() methods. The deserialization will be executed when calling the get_serving_vector() method to retrieve data from the online feature store. If users query directly the online feature store, for instance using the fs.sql(\"SELECT ...\", online=True) statement, it will return a binary blob. On the feature store UI, the online feature type for complex features will be reported as VARBINARY . Online restrictions for primary key data types: # When a feature is being used as a primary key, certain types are not allowed. Examples of such types are Float , Double , Date , Text , Blob and Complex Types (e.g. Array<>). Additionally the size of the sum of the primary key online data types storage requirements should not exceed 3KB. Explicit schema definition # When creating a feature group it is possible for the user to control both the offline and online data type of each column. If users explicitly define the schema for the feature group, Hopsworks is going to use that schema to create the feature group, without performing any type mapping. You can explicitly define the feature group schema as follows: Python from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . create_feature_group ( name = \"fg_manual_schema\" , features = features , online_enabled = True ) fg . save ( df ) Append features to existing feature groups # Hopsworks supports appending additional features to an existing feature group. Adding a additional features to an existing feature group is not considered a breaking change. Python from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . get_feature_group ( name = \"example\" , version = 1 ) fg . append_features ( features ) When adding additional features to a feature group, you can provide a default values for existing entries in the feature group. You can also backfill the new features for existing entries by running an insert() operation and update all existing combinations of primary key - event time .","title":"Data Types and Schema management"},{"location":"user_guides/fs/feature_group/data_types/#how-to-manage-schema-and-feature-data-types","text":"","title":"How to manage schema and feature data types"},{"location":"user_guides/fs/feature_group/data_types/#introduction","text":"In this guide you will learn how to manage the feature group schema and control the data type of the features in a feature group.","title":"Introduction"},{"location":"user_guides/fs/feature_group/data_types/#prerequisites","text":"Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize with the APIs to create a feature group .","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/data_types/#feature-group-schema","text":"When a feature is stored in the both the online and offline feature stores, it will be stored in a data type native to each store. Offline data type : The data type of the feature when stored on the offline feature store Online data type : The data type of the feature when stored on the online feature store. The offline data type is always required, even if the feature group is stored only online. On the other hand, if the feature group is not online_enabled , its features will not have an online data type. The offline and online types for each feature are identified based on the types of the columns in the Spark or Pandas DataFrame, and those types are then mapped to the online and offline data types. In the case of a Spark DataFrame, the Spark types will be mapped to the corresponding Hive Metastore type and used as offline data type. When registering a Pandas DataFrame as a feature group, the following mapping rules are applied: Pandas Type Offline Feature Type int32 INT int64 BIGINT float32 FLOAT float64 DOUBLE datetime64[ns] TIMESTAMP object STRING If the feature group is online enabled, Hopsworks will then map the offline data type to the corresponding online data type. The mapping is based on the following rules: If the offline data type is also supported on the online feature store (e.g. INT, FLOAT, DATE, TIMESTAMP), the online data type will be the same as the offline data type If the offline data type is boolean , the online data type is going to be set as tinyint If the offline data type is string , the online data type is going to be set as varchar(100) If the offline data type is not supported by the online feature store and it is not one of the above exception, the online data type will be set as varbinary(100) to handle complex types.","title":"Feature group schema"},{"location":"user_guides/fs/feature_group/data_types/#offline-data-types","text":"The offline feature store is based on Apache Hudi and Hive Metastore, as such, any Hive Data Type can be leveraged. Potential offline types are: \"TINYINT\" , \"SMALLINT\" , \"INT\" , \"BIGINT\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"TIMESTAMP\" , \"DATE\" , \"STRING\" , \"BOOLEAN\" , \"BINARY\" , \"ARRAY <TINYINT>\" , \"ARRAY <SMALLINT>\" , \"ARRAY <INT>\" , \"ARRAY <BIGINT>\" , \"ARRAY <FLOAT>\" , \"ARRAY <DOUBLE>\" , \"ARRAY <DECIMAL>\" , \"ARRAY <TIMESTAMP>\" , \"ARRAY <DATE>\" , \"ARRAY <STRING>\" , \"ARRAY <BOOLEAN>\" , \"ARRAY <BINARY>\" , \"ARRAY <ARRAY <FLOAT> >\" , \"ARRAY <ARRAY <INT> >\" , \"ARRAY <ARRAY <STRING> >\" , \"MAP <FLOAT, FLOAT>\" , \"MAP <FLOAT, STRING>\" , \"MAP <FLOAT, INT>\" , \"MAP <FLOAT, BINARY>\" , \"MAP <INT, INT>\" , \"MAP <INT, STRING>\" , \"MAP <INT, BINARY>\" , \"MAP <INT, FLOAT>\" , \"MAP <INT, ARRAY <FLOAT> >\" , \"STRUCT < label: STRING, index: INT >\" , \"UNIONTYPE < STRING, INT>\"","title":"Offline data types"},{"location":"user_guides/fs/feature_group/data_types/#online-data-types","text":"The online storage is based on RonDB and hence, any MySQL Data Type can be leveraged. Potential online types are: \"None\" , \"INT(11)\" , \"TINYINT(1)\" , \"SMALLINT(5)\" , \"MEDIUMINT(7)\" , \"BIGINT(20)\" , \"FLOAT\" , \"DOUBLE\" , \"DECIMAL\" , \"DATE\" , \"DATETIME\" , \"TIMESTAMP\" , \"TIME\" , \"YEAR\" , \"CHAR\" , \"VARCHAR(n)\" , \"BINARY\" , \"VARBINARY(n)\" , \"BLOB\" , \"TEXT\" , \"TINYBLOB\" , \"TINYTEXT\" , \"MEDIUMBLOB\" , \"MEDIUMTEXT\" , \"LONGBLOB\" , \"LONGTEXT\" , \"JSON\"","title":"Online data types"},{"location":"user_guides/fs/feature_group/data_types/#complex-online-data-types","text":"Additionally to the online types above, Hopsworks allows users to store complex types (e.g. ARRAY ) on the online feature store. Hopsworks serializes the complex features transparently and stores them as VARBINARY in the online feature store. The serialization happens when calling the save() , insert() or insert_stream() methods. The deserialization will be executed when calling the get_serving_vector() method to retrieve data from the online feature store. If users query directly the online feature store, for instance using the fs.sql(\"SELECT ...\", online=True) statement, it will return a binary blob. On the feature store UI, the online feature type for complex features will be reported as VARBINARY .","title":"Complex online data types"},{"location":"user_guides/fs/feature_group/data_types/#online-restrictions-for-primary-key-data-types","text":"When a feature is being used as a primary key, certain types are not allowed. Examples of such types are Float , Double , Date , Text , Blob and Complex Types (e.g. Array<>). Additionally the size of the sum of the primary key online data types storage requirements should not exceed 3KB.","title":"Online restrictions for primary key data types:"},{"location":"user_guides/fs/feature_group/data_types/#explicit-schema-definition","text":"When creating a feature group it is possible for the user to control both the offline and online data type of each column. If users explicitly define the schema for the feature group, Hopsworks is going to use that schema to create the feature group, without performing any type mapping. You can explicitly define the feature group schema as follows: Python from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . create_feature_group ( name = \"fg_manual_schema\" , features = features , online_enabled = True ) fg . save ( df )","title":"Explicit schema definition"},{"location":"user_guides/fs/feature_group/data_types/#append-features-to-existing-feature-groups","text":"Hopsworks supports appending additional features to an existing feature group. Adding a additional features to an existing feature group is not considered a breaking change. Python from hsfs.feature import Feature features = [ Feature ( name = \"id\" , type = \"int\" , online_type = \"int\" ), Feature ( name = \"name\" , type = \"string\" , online_type = \"varchar(20)\" ) ] fg = fs . get_feature_group ( name = \"example\" , version = 1 ) fg . append_features ( features ) When adding additional features to a feature group, you can provide a default values for existing entries in the feature group. You can also backfill the new features for existing entries by running an insert() operation and update all existing combinations of primary key - event time .","title":"Append features to existing feature groups"},{"location":"user_guides/fs/feature_group/statistics/","text":"How to compute statistics on feature data # Introduction # In this guide you will learn how to configure, compute and visualize statistics for the features registered with Hopsworks. Hopsworks groups features in four categories: Descriptive : These are the basic statistics Hopsworks computes. They include an approximate count of the distinctive values and the completeness (i.e. the percentage of non null values). For numerical features Hopsworks also computes the minimum, maximum, mean, standard deviation and the sum of each feature. Enabled by default. Histograms : Hopsworks computes the distribution of the values of a feature. Exact histograms are computed as long as the number of distinct values is less than 20. If a feature has a numerical data type (e.g. integer, float, double, ...) and has more than 20 unique values, then the values are bucketed in 20 buckets and the histogram represents the distribution of values in those buckets. By default histograms are disabled. Correlation : If enabled, Hopsworks computes the Pearson correlation between features of numerical data type within a feature group. By default correlation is disabled. Exact Statistics : Exact statistics are an enhancement of the descriptive statistics that provide an exact count of distinctive values, entropy, uniqueness and distinctivness of the value of a feature. These statistics are more expensive to compute as they take into consideration all the values and they don't use approximations. By default they are disabled. When statistics are enabled, they are computed every time new data is written into the offline storage of a feature group. Statistics are then displayed on the Hopsworks UI and users can track how data has changed over time. Prerequisites # Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize with the APIs to create a feature group . Enable statistics when creating a feature group # As mentioned above, by default only descriptive statistics are enabled when creating a feature group. To enable histograms, correlations or exact statistics the statistics_config configuration parameter can be provided in the create statement. The statistics_config parameter takes a dictionary with the keys: enabled , correlations , histograms and exact_uniqueness and, as values, a boolean to describe whether or not to compute the specific class of statistics. Additionally it is possible to restrict the statistics computation to only a subset of columns. This is configurable by adding a columns key to the statistics_config parameter. The key should contain the list of columns for which to compute statistics. By default the value is empty list [] and the statistics are computed for all columns in the feature group. Python fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True , \"exact_uniqueness\" : False , \"columns\" : [] } ) Enable statistics after creating a feature group # It is possible users to change the statistics configuration after a feature group was created. Either to add or remove a class of statistics, or to change the set of features for which to compute statistics. Python fg . statistics_config = { \"enabled\" : True , \"histograms\" : False , \"correlations\" : False , \"exact_uniqueness\" : False \"columns\" : [ 'location_id' , 'min_temp' , 'max_temp' ] } fg . update_statistics_config () Explicitly compute statistics # As mentioned above, the statistics are computed every time new data is written into the offline storage of a feature group. By invoking the compute_statistics method, users can trigger explicitly the statistics computation for the data available in a feature group. This is useful when a feature group is receiving frequent updates. Users can schedule periodic statistics computation that take into consideration several data commits. By default, the compute_statistics method computes statistics on the most recent version of the data available in a feature group. Users can provide a specific time using the wallclock_time parameter, to compute the statistics for a previous version of the data. Hopsworks can compute statistics of external feature groups. As external feature groups are read only from an Hopsworks prospective, statistics computation can be triggered using the compute_statistics method. Python fg . compute_statistics ( wallclock_time = '20220611 20:00' ) Inspect statisitcs # You can also create a new feature group through the UI.","title":"Statistics"},{"location":"user_guides/fs/feature_group/statistics/#how-to-compute-statistics-on-feature-data","text":"","title":"How to compute statistics on feature data"},{"location":"user_guides/fs/feature_group/statistics/#introduction","text":"In this guide you will learn how to configure, compute and visualize statistics for the features registered with Hopsworks. Hopsworks groups features in four categories: Descriptive : These are the basic statistics Hopsworks computes. They include an approximate count of the distinctive values and the completeness (i.e. the percentage of non null values). For numerical features Hopsworks also computes the minimum, maximum, mean, standard deviation and the sum of each feature. Enabled by default. Histograms : Hopsworks computes the distribution of the values of a feature. Exact histograms are computed as long as the number of distinct values is less than 20. If a feature has a numerical data type (e.g. integer, float, double, ...) and has more than 20 unique values, then the values are bucketed in 20 buckets and the histogram represents the distribution of values in those buckets. By default histograms are disabled. Correlation : If enabled, Hopsworks computes the Pearson correlation between features of numerical data type within a feature group. By default correlation is disabled. Exact Statistics : Exact statistics are an enhancement of the descriptive statistics that provide an exact count of distinctive values, entropy, uniqueness and distinctivness of the value of a feature. These statistics are more expensive to compute as they take into consideration all the values and they don't use approximations. By default they are disabled. When statistics are enabled, they are computed every time new data is written into the offline storage of a feature group. Statistics are then displayed on the Hopsworks UI and users can track how data has changed over time.","title":"Introduction"},{"location":"user_guides/fs/feature_group/statistics/#prerequisites","text":"Before you begin this guide we suggest you read the Feature Group concept page to understand what a feature group is and how it fits in the ML pipeline. We also suggest you familiarize with the APIs to create a feature group .","title":"Prerequisites"},{"location":"user_guides/fs/feature_group/statistics/#enable-statistics-when-creating-a-feature-group","text":"As mentioned above, by default only descriptive statistics are enabled when creating a feature group. To enable histograms, correlations or exact statistics the statistics_config configuration parameter can be provided in the create statement. The statistics_config parameter takes a dictionary with the keys: enabled , correlations , histograms and exact_uniqueness and, as values, a boolean to describe whether or not to compute the specific class of statistics. Additionally it is possible to restrict the statistics computation to only a subset of columns. This is configurable by adding a columns key to the statistics_config parameter. The key should contain the list of columns for which to compute statistics. By default the value is empty list [] and the statistics are computed for all columns in the feature group. Python fg = feature_store . create_feature_group ( name = \"weather\" , version = 1 , description = \"Weather Features\" , online_enabled = True , primary_key = [ 'location_id' ], partition_key = [ 'day' ], event_time = 'event_time' , statistics_config = { \"enabled\" : True , \"histograms\" : True , \"correlations\" : True , \"exact_uniqueness\" : False , \"columns\" : [] } )","title":"Enable statistics when creating a feature group"},{"location":"user_guides/fs/feature_group/statistics/#enable-statistics-after-creating-a-feature-group","text":"It is possible users to change the statistics configuration after a feature group was created. Either to add or remove a class of statistics, or to change the set of features for which to compute statistics. Python fg . statistics_config = { \"enabled\" : True , \"histograms\" : False , \"correlations\" : False , \"exact_uniqueness\" : False \"columns\" : [ 'location_id' , 'min_temp' , 'max_temp' ] } fg . update_statistics_config ()","title":"Enable statistics after creating a feature group"},{"location":"user_guides/fs/feature_group/statistics/#explicitly-compute-statistics","text":"As mentioned above, the statistics are computed every time new data is written into the offline storage of a feature group. By invoking the compute_statistics method, users can trigger explicitly the statistics computation for the data available in a feature group. This is useful when a feature group is receiving frequent updates. Users can schedule periodic statistics computation that take into consideration several data commits. By default, the compute_statistics method computes statistics on the most recent version of the data available in a feature group. Users can provide a specific time using the wallclock_time parameter, to compute the statistics for a previous version of the data. Hopsworks can compute statistics of external feature groups. As external feature groups are read only from an Hopsworks prospective, statistics computation can be triggered using the compute_statistics method. Python fg . compute_statistics ( wallclock_time = '20220611 20:00' )","title":"Explicitly compute statistics"},{"location":"user_guides/fs/feature_group/statistics/#inspect-statisitcs","text":"You can also create a new feature group through the UI.","title":"Inspect statisitcs"},{"location":"user_guides/fs/feature_view/","text":"","title":"Index"},{"location":"user_guides/fs/feature_view/batch-data/","text":"Batch data (analytical ML systems) # Creation # It is very common that ML models are deployed in a \"batch\" setting where ML pipelines score incoming new data at a regular interval, for example, daily or weekly. Feature views support batch prediction by returning batch data as a DataFrame over a time range, by start_time and end_time . The resultant DataFrame (or batch-scoring DataFrame) can then be fed to models to make predictions. Python Java # get batch data df = feature_view . get_batch_data ( start_time = \"20220620\" , end_time = \"20220627\" ) # return a dataframe Dataset < Row > ds = featureView . getBatchData ( \"20220620\" , \"20220627\" ) Creation with transformation # If you have specified transformation functions when creating a feature view, you will get back transformed batch data as well. If your transformation functions require statistics of training dataset, you must also provide the training data version. init_batch_scoring will then fetch the statistics and initialize the functions with required statistics. Then you can follow the above examples and create the batch data. Please note that transformed batch data can only be returned in the python client but not in the java client. feature_view . init_batch_scoring ( training_dataset_version = 1 )","title":"Batch data"},{"location":"user_guides/fs/feature_view/batch-data/#batch-data-analytical-ml-systems","text":"","title":"Batch data (analytical ML systems)"},{"location":"user_guides/fs/feature_view/batch-data/#creation","text":"It is very common that ML models are deployed in a \"batch\" setting where ML pipelines score incoming new data at a regular interval, for example, daily or weekly. Feature views support batch prediction by returning batch data as a DataFrame over a time range, by start_time and end_time . The resultant DataFrame (or batch-scoring DataFrame) can then be fed to models to make predictions. Python Java # get batch data df = feature_view . get_batch_data ( start_time = \"20220620\" , end_time = \"20220627\" ) # return a dataframe Dataset < Row > ds = featureView . getBatchData ( \"20220620\" , \"20220627\" )","title":"Creation"},{"location":"user_guides/fs/feature_view/batch-data/#creation-with-transformation","text":"If you have specified transformation functions when creating a feature view, you will get back transformed batch data as well. If your transformation functions require statistics of training dataset, you must also provide the training data version. init_batch_scoring will then fetch the statistics and initialize the functions with required statistics. Then you can follow the above examples and create the batch data. Please note that transformed batch data can only be returned in the python client but not in the java client. feature_view . init_batch_scoring ( training_dataset_version = 1 )","title":"Creation with transformation"},{"location":"user_guides/fs/feature_view/feature-vectors/","text":"Feature Vectors # Once you have trained a model, it is time to deploy it. You can get back all the features required to feed into an ML model with a single method call. A feature view provides great flexibility for you to retrieve a vector (or row) of features from any environment, whether you are either inside the Hopsworks platform, a model serving platform, or in an external environment, such as your application server. Harnessing the powerful RonDB , feature vectors are served at in-memory latency. If you want to understand more about the concept of feature vectors, you can refer to here . Retrieval # You can get back feature vectors from either python or java client by providing the primary key value(s) for the feature view. Python Java # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) // get a single vector Map < String , Object > entry1 = Maps . newHashMap (); entry1 . put ( \"pk1\" , 1 ); entry1 . put ( \"pk2\" , 2 ); featureView . getFeatureVector ( entry1 ); // get multiple vectors Map < String , Object > entry2 = Maps . newHashMap (); entry2 . put ( \"pk1\" , 3 ); entry2 . put ( \"pk2\" , 4 ); featureView . getFeatureVectors ( Lists . newArrayList ( entry1 , entry2 ); Retrieval with transformation # If you have specified transformation functions when creating a feature view, you receive transformed feature vectors. If your transformation functions require statistics of training dataset, you must also provide the training data version. init_serving will then fetch the statistics and initialize the functions with the required statistics. Then you can follow the above examples and retrieve the feature vectors. Please note that transformed feature vectors can only be returned in the python client but not in the java client. feature_view . init_serving ( training_dataset_version = 1 ) Preview # In order to enable ML engineers to test feature serving easily, a feature view can return a sample of feature vectors without specifying any primary keys. Python Java # get a single vector feature_view . preview_feature_vector () # get multiple vectors feature_view . preview_feature_vectors ( n = 3 ) # n = size of feature vectors // get a single vector featureView . previewFeatureVector (); // get multiple vectors featureView . previewFeatureVectors ( 3 ); Passed features # fabio's part","title":"Feature vectors"},{"location":"user_guides/fs/feature_view/feature-vectors/#feature-vectors","text":"Once you have trained a model, it is time to deploy it. You can get back all the features required to feed into an ML model with a single method call. A feature view provides great flexibility for you to retrieve a vector (or row) of features from any environment, whether you are either inside the Hopsworks platform, a model serving platform, or in an external environment, such as your application server. Harnessing the powerful RonDB , feature vectors are served at in-memory latency. If you want to understand more about the concept of feature vectors, you can refer to here .","title":"Feature Vectors"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieval","text":"You can get back feature vectors from either python or java client by providing the primary key value(s) for the feature view. Python Java # get a single vector feature_view . get_feature_vector ( entry = { \"pk1\" : 1 , \"pk2\" : 2 } ) # get multiple vectors feature_view . get_feature_vectors ( entry = [ { \"pk1\" : 1 , \"pk2\" : 2 }, { \"pk1\" : 3 , \"pk2\" : 4 }, { \"pk1\" : 5 , \"pk2\" : 6 } ] ) // get a single vector Map < String , Object > entry1 = Maps . newHashMap (); entry1 . put ( \"pk1\" , 1 ); entry1 . put ( \"pk2\" , 2 ); featureView . getFeatureVector ( entry1 ); // get multiple vectors Map < String , Object > entry2 = Maps . newHashMap (); entry2 . put ( \"pk1\" , 3 ); entry2 . put ( \"pk2\" , 4 ); featureView . getFeatureVectors ( Lists . newArrayList ( entry1 , entry2 );","title":"Retrieval"},{"location":"user_guides/fs/feature_view/feature-vectors/#retrieval-with-transformation","text":"If you have specified transformation functions when creating a feature view, you receive transformed feature vectors. If your transformation functions require statistics of training dataset, you must also provide the training data version. init_serving will then fetch the statistics and initialize the functions with the required statistics. Then you can follow the above examples and retrieve the feature vectors. Please note that transformed feature vectors can only be returned in the python client but not in the java client. feature_view . init_serving ( training_dataset_version = 1 )","title":"Retrieval with transformation"},{"location":"user_guides/fs/feature_view/feature-vectors/#preview","text":"In order to enable ML engineers to test feature serving easily, a feature view can return a sample of feature vectors without specifying any primary keys. Python Java # get a single vector feature_view . preview_feature_vector () # get multiple vectors feature_view . preview_feature_vectors ( n = 3 ) # n = size of feature vectors // get a single vector featureView . previewFeatureVector (); // get multiple vectors featureView . previewFeatureVectors ( 3 );","title":"Preview"},{"location":"user_guides/fs/feature_view/feature-vectors/#passed-features","text":"fabio's part","title":"Passed features"},{"location":"user_guides/fs/feature_view/overview/","text":"Feature View # A feature view is a set of features that come from one or more feature groups. It is a logical view over the feature groups, as the feature data is only stored in feature groups. Feature views are used to read feature data for both training and serving (online and batch). You can create training datasets , create batch data and get feature vectors . If you want to understand more about the concept of feature view, you can refer to here . Creation # Query and transformation function are the building blocks of a feature view. You can define your set of features by building a query . You can also define which columns in your feature view are the labels , which is useful for supervised machine learning tasks. Furthermore, in python client, each feature can be attached to its own transformation function. This way, when a feature is read (for training or scoring), the transformation is executed on-demand - just before the feature data is returned. For example, when a client reads a numerical feature, the feature value could be normalized by a StandardScalar transformation function before it is returned to the client. Python Java # create a simple feature view feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query ) # create a feature view with transformation and label feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"amount\" : fs . get_transformation_function ( name = \"standard_scaler\" , version = 1 ) } ) // create a simple feature view FeatureView featureView = featureStore . createFeatureView () . name ( \"transactions_view) .query(query) .build(); // create a feature view with label FeatureView featureView = featureStore.createFeatureView() .name(\" transactions_view ) . query ( query ) . labels ( Lists . newArrayList ( \"fraud_label\" ) . build (); You can refer to query and transformation function for creating query and transformation_function . Retrieval # Once you have created a feature view, you can retrieve it by its name and version. Python Java feature_view = fs . get_feature_view ( name = \"transactions_view\" , version = 1 ) FeatureView featureView = featureStore . getFeatureView ( \"transactions_view\" , 1 ) Deletion # If there are some feature view instances which you do not use anymore, you can delete a feature view. It is important to mention that all training datasets (include all materialised hopsfs training data) will be deleted along with the feature view. Python Java feature_view . delete () featureView . delete () Tags # Feature views also support tags. You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work. Python Java # attach feature_view . add_tag ( name = \"tag_schema\" , value = { \"key\" , \"value\" } # get feature_view . get_tag ( name = \"tag_schema\" ) #remove feature_view . delete_tag ( name = \"tag_schema\" ) // attach Map < String , String > tag = Maps . newHashMap (); tag . put ( \"key\" , \"value\" ); featureView . addTag ( \"tag_schema\" , tag ) // get featureView . getTag ( \"tag_schema\" ) // remove featureView . deleteTag ( \"tag_schema\" ) Next # Once you have created a feature view, you can now create training data","title":"Feature View"},{"location":"user_guides/fs/feature_view/overview/#feature-view","text":"A feature view is a set of features that come from one or more feature groups. It is a logical view over the feature groups, as the feature data is only stored in feature groups. Feature views are used to read feature data for both training and serving (online and batch). You can create training datasets , create batch data and get feature vectors . If you want to understand more about the concept of feature view, you can refer to here .","title":"Feature View"},{"location":"user_guides/fs/feature_view/overview/#creation","text":"Query and transformation function are the building blocks of a feature view. You can define your set of features by building a query . You can also define which columns in your feature view are the labels , which is useful for supervised machine learning tasks. Furthermore, in python client, each feature can be attached to its own transformation function. This way, when a feature is read (for training or scoring), the transformation is executed on-demand - just before the feature data is returned. For example, when a client reads a numerical feature, the feature value could be normalized by a StandardScalar transformation function before it is returned to the client. Python Java # create a simple feature view feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query ) # create a feature view with transformation and label feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"amount\" : fs . get_transformation_function ( name = \"standard_scaler\" , version = 1 ) } ) // create a simple feature view FeatureView featureView = featureStore . createFeatureView () . name ( \"transactions_view) .query(query) .build(); // create a feature view with label FeatureView featureView = featureStore.createFeatureView() .name(\" transactions_view ) . query ( query ) . labels ( Lists . newArrayList ( \"fraud_label\" ) . build (); You can refer to query and transformation function for creating query and transformation_function .","title":"Creation"},{"location":"user_guides/fs/feature_view/overview/#retrieval","text":"Once you have created a feature view, you can retrieve it by its name and version. Python Java feature_view = fs . get_feature_view ( name = \"transactions_view\" , version = 1 ) FeatureView featureView = featureStore . getFeatureView ( \"transactions_view\" , 1 )","title":"Retrieval"},{"location":"user_guides/fs/feature_view/overview/#deletion","text":"If there are some feature view instances which you do not use anymore, you can delete a feature view. It is important to mention that all training datasets (include all materialised hopsfs training data) will be deleted along with the feature view. Python Java feature_view . delete () featureView . delete ()","title":"Deletion"},{"location":"user_guides/fs/feature_view/overview/#tags","text":"Feature views also support tags. You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work. Python Java # attach feature_view . add_tag ( name = \"tag_schema\" , value = { \"key\" , \"value\" } # get feature_view . get_tag ( name = \"tag_schema\" ) #remove feature_view . delete_tag ( name = \"tag_schema\" ) // attach Map < String , String > tag = Maps . newHashMap (); tag . put ( \"key\" , \"value\" ); featureView . addTag ( \"tag_schema\" , tag ) // get featureView . getTag ( \"tag_schema\" ) // remove featureView . deleteTag ( \"tag_schema\" )","title":"Tags"},{"location":"user_guides/fs/feature_view/overview/#next","text":"Once you have created a feature view, you can now create training data","title":"Next"},{"location":"user_guides/fs/feature_view/query/","text":"Query vs DataFrame # HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters, and point in time queries. The Query object enables you to select features from different feature groups to join together to be used in a feature view. The joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python Scala # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) # save the query to feature view feature_view = fs . create_feature_view ( name = 'rain_dataset' , query = feature_join ) # retrieve the query back from the feature view feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) query = feature_view . query // create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val featureView = featureStore . createFeatureView () . name ( \"rain_dataset\" ) . query ( featureJoin ) . build (); // retrieve the query back from the feature view val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , 1 ) val query = featureView . getQuery () If a data scientist wants to modify a new feature that is not available in the feature store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the feature store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources. The Query Abstraction # Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation. Examples # Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python Scala rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" )) Join # Similarly, joins return query objects. The simplest join in one where we join all of the features together from two different feature groups without specifying a join key - HSFS will infer the join key as a common primary key between the two feature groups. By default, Hopsworks will use the maximal matching subset of the primary keys of the two feature groups as joining key(s), if not specified otherwise. Python Scala # Returns Query feature_join = rain_fg . join ( temperature_fg ) // Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joined feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the names of the features to join on. Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure. Filter # In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and using the Bitwise Operators & and | to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python Scala filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )))","title":"Query"},{"location":"user_guides/fs/feature_view/query/#query-vs-dataframe","text":"HSFS provides a DataFrame API to ingest data into the Hopsworks Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models. The idea of the Feature Store is to have pre-computed features available for both training and serving models. The key functionality required to generate training datasets from reusable features are: feature selection, joins, filters, and point in time queries. The Query object enables you to select features from different feature groups to join together to be used in a feature view. The joining functionality is heavily inspired by the APIs used by Pandas to merge DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions. Python Scala # create a query feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all ()) # save the query to feature view feature_view = fs . create_feature_view ( name = 'rain_dataset' , query = feature_join ) # retrieve the query back from the feature view feature_view = fs . get_feature_view ( \u201c rain_dataset \u201d , version = 1 ) query = feature_view . query // create a query val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), on = Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll ())) val featureView = featureStore . createFeatureView () . name ( \"rain_dataset\" ) . query ( featureJoin ) . build (); // retrieve the query back from the feature view val featureView = fs . getFeatureView ( \u201c rain_dataset \u201d , 1 ) val query = featureView . getQuery () If a data scientist wants to modify a new feature that is not available in the feature store, she can write code to compute the new feature (using existing features or external data) and ingest the new feature values into the feature store. If the new feature is based solely on existing feature values in the Feature Store, we call it a derived feature. The same HSFS APIs can be used to compute derived features as well as features using external data sources.","title":"Query vs DataFrame"},{"location":"user_guides/fs/feature_view/query/#the-query-abstraction","text":"Most operations performed on FeatureGroup metadata objects will return a Query with the applied operation.","title":"The Query Abstraction"},{"location":"user_guides/fs/feature_view/query/#examples","text":"Selecting features from a feature group is a lazy operation, returning a query with the selected features only: Python Scala rain_fg = fs . get_feature_group ( \"rain_fg\" ) # Returns Query feature_join = rain_fg . select ([ \"location_id\" , \"weekly_rainfall\" ]) val rainFg = fs . getFeatureGroup ( \"rain_fg\" ) # Returns Query val featureJoin = rainFg . select ( Seq ( \"location_id\" , \"weekly_rainfall\" ))","title":"Examples"},{"location":"user_guides/fs/feature_view/query/#join","text":"Similarly, joins return query objects. The simplest join in one where we join all of the features together from two different feature groups without specifying a join key - HSFS will infer the join key as a common primary key between the two feature groups. By default, Hopsworks will use the maximal matching subset of the primary keys of the two feature groups as joining key(s), if not specified otherwise. Python Scala # Returns Query feature_join = rain_fg . join ( temperature_fg ) // Returns Query val featureJoin = rainFg . join ( temperatureFg ) More complex joins are possible by selecting subsets of features from the joined feature groups and by specifying a join key and type. Possible join types are \"inner\", \"left\" or \"right\". Furthermore, it is possible to specify different features for the join key of the left and right feature group. The join key lists should contain the names of the features to join on. Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" )) Nested Joins The API currently does not support nested joins. That is joins of joins. You can fall back to Spark DataFrames to cover these cases. However, if you have to use joins of joins, most likely there is potential to optimise your feature group structure.","title":"Join"},{"location":"user_guides/fs/feature_view/query/#filter","text":"In the same way as joins, applying filters to feature groups creates a query with the applied filter. Filters are constructed with Python Operators == , >= , <= , != , > , < and using the Bitwise Operators & and | to construct conjunctions. For the Scala part of the API, equivalent methods are available in the Feature and Filter classes. Python Scala filtered_rain = rain_fg . filter ( rain_fg . location_id == 10 ) val filteredRain = rainFg . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )) Filters are fully compatible with joins: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all (), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter (( rain_fg . location_id == 10 ) | ( rain_fg . location_id == 20 )) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 ). or ( rainFg . getFeature ( \"location_id\" ). eq ( 20 )))) The filters can be applied at any point of the query: Python Scala feature_join = rain_fg . select_all () \\ . join ( temperature_fg . select_all () . filter ( temperature_fg . avg_temp >= 22 ), on = [ \"date\" , \"location_id\" ]) \\ . join ( location_fg . select_all (), left_on = [ \"location_id\" ], right_on = [ \"id\" ], how = \"left\" ) \\ . filter ( rain_fg . location_id == 10 ) val featureJoin = ( rainFg . selectAll () . join ( temperatureFg . selectAll (). filter ( temperatureFg . getFeature ( \"avg_temp\" ). ge ( 22 )), Seq ( \"date\" , \"location_id\" )) . join ( locationFg . selectAll (), Seq ( \"location_id\" ), Seq ( \"id\" ), \"left\" ) . filter ( rainFg . getFeature ( \"location_id\" ). eq ( 10 )))","title":"Filter"},{"location":"user_guides/fs/feature_view/training-data/","text":"Training data # Training data can be created from the feature view and used by different ML libraries for training different models. You can read training data concepts for more details. Creation # It can be created as in-memory DataFrames or materialised as tfrecords , parquet , csv , or tsv files to HopsFS or in all other locations, for example, S3, GCS. If you materialise a training dataset, a PySparkJob will be launched. By default, create_training_data waits for the job to finish. However, you can run the job asynchronously by passing write_options={\"wait_for_job\": False} . You can monitor the job status in the jobs overview UI . # create a training dataset as dataframe df = feature_view . training_data ( description = 'transactions_dataset_jan_feb' , ) # materialise a training dataset version , job = feature_view . create_training_data ( description = 'transactions_dataset_jan_feb' , data_format = 'csv' , write_options = { \"wait_for_job\" : False } ) # By default, it is materialised to HopsFS print ( job . id ) # get the job's id and view the job status in the UI Train/Validation/Test Splits # In most cases, ML practitioners want to slice a dataset into multiple splits, most commonly train-test splits or train-validation-test splits, so that they can train and test their models. Feature view provides a sklearn-like API for this purpose, so it is very easy to create a training dataset with different splits. Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train and test splits. # create a training dataset X_train , y_train , X_test , y_test = feature_view . train_test_split ( test_size = 0.2 ) # materialise a training dataset version , job = feature_view . create_train_test_split ( test_size = 0.2 , description = 'transactions_dataset_jan_feb' , data_format = 'csv' ) Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train, validation, and test splits. # create a training dataset as DataFrame X_train , y_train , X_val , y_val , X_test , y_test = feature_view . train_validation_test_splits ( val_size = 0.3 , test_size = 0.2 ) # materialise a training dataset version , job = feature_view . create_train_validation_test_splits ( val_size = 0.3 , test_size = 0.2 description = 'transactions_dataset_jan_feb' , data_format = 'csv' ) Read Training Data # Once you have created a training dataset, all its metadata are saved in Hopsworks. This enables you to reproduce exactly the same dataset at a later point in time. This holds for training data as both DataFrames or files. That is, you can delete the training data files (for example, to reduce storage costs), but still reproduce the training data files later on if you need to. # get a training dataset df = feature_view . get_training_data ( training_dataset_version = 1 ) # get a training dataset with train and test splits X_train , y_train , X_test , y_test = feature_view . get_train_test_split ( training_dataset_version = 1 ) # get a training dataset with train, validation and test splits X_train , y_train , X_val , y_val , X_test , y_test = feature_view . get_train_validation_test_splits ( training_dataset_version = 1 ) Deletion # To clean up unused training data, you can delete all training data or for a particular version. Note that all metadata of training data and materialised files stored in HopsFS will be deleted and cannot be recreated anymore. # delete a training data version feature_view . delete_training_dataset ( version = 1 ) # delete all training datasets feature_view . delete_training_dataset () It is also possible to keep the metadata and delete only the materialised files. Then you can recreate the deleted files by just specifying a version, and you get back the exact same dataset again. This is useful when you are running out of storage. # delete files of a training data version feature_view . purge_training_data ( version = 1 ) # delete files of all training datasets feature_view . purge_all_training_data () To recreate a training dataset: feature_view . recreate_training_dataset ( version = 1 ) Tags # Similar to feature view, You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work. # attach feature_view . add_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" , value = { \"key\" , \"value\" } ) # get feature_view . get_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" ) #remove feature_view . delete_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" ) Next # Once you have created a training dataset and trained your model, you can deploy your model in a \"batch\" or \"online\" setting. Next, you can learn how to create batch data and get feature vectors .","title":"Training data"},{"location":"user_guides/fs/feature_view/training-data/#training-data","text":"Training data can be created from the feature view and used by different ML libraries for training different models. You can read training data concepts for more details.","title":"Training data"},{"location":"user_guides/fs/feature_view/training-data/#creation","text":"It can be created as in-memory DataFrames or materialised as tfrecords , parquet , csv , or tsv files to HopsFS or in all other locations, for example, S3, GCS. If you materialise a training dataset, a PySparkJob will be launched. By default, create_training_data waits for the job to finish. However, you can run the job asynchronously by passing write_options={\"wait_for_job\": False} . You can monitor the job status in the jobs overview UI . # create a training dataset as dataframe df = feature_view . training_data ( description = 'transactions_dataset_jan_feb' , ) # materialise a training dataset version , job = feature_view . create_training_data ( description = 'transactions_dataset_jan_feb' , data_format = 'csv' , write_options = { \"wait_for_job\" : False } ) # By default, it is materialised to HopsFS print ( job . id ) # get the job's id and view the job status in the UI","title":"Creation"},{"location":"user_guides/fs/feature_view/training-data/#trainvalidationtest-splits","text":"In most cases, ML practitioners want to slice a dataset into multiple splits, most commonly train-test splits or train-validation-test splits, so that they can train and test their models. Feature view provides a sklearn-like API for this purpose, so it is very easy to create a training dataset with different splits. Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train and test splits. # create a training dataset X_train , y_train , X_test , y_test = feature_view . train_test_split ( test_size = 0.2 ) # materialise a training dataset version , job = feature_view . create_train_test_split ( test_size = 0.2 , description = 'transactions_dataset_jan_feb' , data_format = 'csv' ) Create a training dataset (as in-memory DataFrames) or materialise a training dataset with train, validation, and test splits. # create a training dataset as DataFrame X_train , y_train , X_val , y_val , X_test , y_test = feature_view . train_validation_test_splits ( val_size = 0.3 , test_size = 0.2 ) # materialise a training dataset version , job = feature_view . create_train_validation_test_splits ( val_size = 0.3 , test_size = 0.2 description = 'transactions_dataset_jan_feb' , data_format = 'csv' )","title":"Train/Validation/Test Splits"},{"location":"user_guides/fs/feature_view/training-data/#read-training-data","text":"Once you have created a training dataset, all its metadata are saved in Hopsworks. This enables you to reproduce exactly the same dataset at a later point in time. This holds for training data as both DataFrames or files. That is, you can delete the training data files (for example, to reduce storage costs), but still reproduce the training data files later on if you need to. # get a training dataset df = feature_view . get_training_data ( training_dataset_version = 1 ) # get a training dataset with train and test splits X_train , y_train , X_test , y_test = feature_view . get_train_test_split ( training_dataset_version = 1 ) # get a training dataset with train, validation and test splits X_train , y_train , X_val , y_val , X_test , y_test = feature_view . get_train_validation_test_splits ( training_dataset_version = 1 )","title":"Read Training Data"},{"location":"user_guides/fs/feature_view/training-data/#deletion","text":"To clean up unused training data, you can delete all training data or for a particular version. Note that all metadata of training data and materialised files stored in HopsFS will be deleted and cannot be recreated anymore. # delete a training data version feature_view . delete_training_dataset ( version = 1 ) # delete all training datasets feature_view . delete_training_dataset () It is also possible to keep the metadata and delete only the materialised files. Then you can recreate the deleted files by just specifying a version, and you get back the exact same dataset again. This is useful when you are running out of storage. # delete files of a training data version feature_view . purge_training_data ( version = 1 ) # delete files of all training datasets feature_view . purge_all_training_data () To recreate a training dataset: feature_view . recreate_training_dataset ( version = 1 )","title":"Deletion"},{"location":"user_guides/fs/feature_view/training-data/#tags","text":"Similar to feature view, You can attach, get, and remove tags. You can refer to here if you want to learn more about how tags work. # attach feature_view . add_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" , value = { \"key\" , \"value\" } ) # get feature_view . get_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" ) #remove feature_view . delete_training_dataset_tag ( training_dataset_version = 1 , name = \"tag_schema\" )","title":"Tags"},{"location":"user_guides/fs/feature_view/training-data/#next","text":"Once you have created a training dataset and trained your model, you can deploy your model in a \"batch\" or \"online\" setting. Next, you can learn how to create batch data and get feature vectors .","title":"Next"},{"location":"user_guides/fs/feature_view/transformation-function/","text":"Transformation Functions # HSFS provides functionality to attach transformation functions to feature views . User defined, custom transformation functions need to be registered in the feature store to make them accessible for feature view creation. To register them in the feature store, they either have to be part of the library installed in Hopsworks or attached when starting a Jupyter notebook or Hopsworks job . Pyspark decorators Don't decorate transformation functions with Pyspark @udf or @pandas_udf , and also make sure not to use any Pyspark dependencies. That is because, the transformation functions may be executed by Python clients. HSFS will decorate transformation function for you only if it is used inside Pyspark application. Creation # Hopsworks ships built-in transformation functions such as min_max_scaler , standard_scaler , robust_scaler and label_encoder . You can also create new functions. Let's assume that you have already installed Python library transformation_fn_template containing the transformation function plus_one . Python Register transformation function plus_one in the Hopsworks feature store. from custom_functions import transformations plus_one_meta = fs . create_transformation_function ( transformation_function = transformations . plus_one , output_type = int , version = 1 ) plus_one_meta . save () Retrieval # To retrieve all transformation functions from the feature store, use get_transformation_functions which will return the list of available TransformationFunction objects. A specific transformation function can be retrieved with the get_transformation_function method where you can provide its name and version of the transformation function. If only the function name is provided then it will default to version 1. Python Retrieving transformation functions from the feature store # get all transformation functions fs . get_transformation_functions () # get transformation function by name. This will default to version 1 plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) # get built-in transformation function min max scaler min_max_scaler_fn = fs . get_transformation_function ( name = \"min_max_scaler\" ) # get transformation function by name and version. plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" , version = 2 ) Apply transformation functions to features # You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function name. Then the transformation functions are applied when you read training data , read batch data , or get feature vectors . Python Attaching transformation functions to the feature view plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" , version = 1 ) feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"amount_spent\" : plus_one_fn } ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Python Attaching built-in transformation functions to the feature view min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"category\" : label_encoder , \"amount\" : robust_scaler , \"loc_delta\" : min_max_scaler , \"age_at_transaction\" : standard_scaler } ) Java/Scala support Creating and attaching Transformation functions to feature views are not supported for HSFS Java or Scala client. If feature view with transformation function was created using python client, you cannot get training data or get feature vectors from HSFS Java or Scala client.","title":"Transformation Functions"},{"location":"user_guides/fs/feature_view/transformation-function/#transformation-functions","text":"HSFS provides functionality to attach transformation functions to feature views . User defined, custom transformation functions need to be registered in the feature store to make them accessible for feature view creation. To register them in the feature store, they either have to be part of the library installed in Hopsworks or attached when starting a Jupyter notebook or Hopsworks job . Pyspark decorators Don't decorate transformation functions with Pyspark @udf or @pandas_udf , and also make sure not to use any Pyspark dependencies. That is because, the transformation functions may be executed by Python clients. HSFS will decorate transformation function for you only if it is used inside Pyspark application.","title":"Transformation Functions"},{"location":"user_guides/fs/feature_view/transformation-function/#creation","text":"Hopsworks ships built-in transformation functions such as min_max_scaler , standard_scaler , robust_scaler and label_encoder . You can also create new functions. Let's assume that you have already installed Python library transformation_fn_template containing the transformation function plus_one . Python Register transformation function plus_one in the Hopsworks feature store. from custom_functions import transformations plus_one_meta = fs . create_transformation_function ( transformation_function = transformations . plus_one , output_type = int , version = 1 ) plus_one_meta . save ()","title":"Creation"},{"location":"user_guides/fs/feature_view/transformation-function/#retrieval","text":"To retrieve all transformation functions from the feature store, use get_transformation_functions which will return the list of available TransformationFunction objects. A specific transformation function can be retrieved with the get_transformation_function method where you can provide its name and version of the transformation function. If only the function name is provided then it will default to version 1. Python Retrieving transformation functions from the feature store # get all transformation functions fs . get_transformation_functions () # get transformation function by name. This will default to version 1 plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" ) # get built-in transformation function min max scaler min_max_scaler_fn = fs . get_transformation_function ( name = \"min_max_scaler\" ) # get transformation function by name and version. plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" , version = 2 )","title":"Retrieval"},{"location":"user_guides/fs/feature_view/transformation-function/#apply-transformation-functions-to-features","text":"You can define in the feature view transformation functions as dict, where key is feature name and value is online transformation function name. Then the transformation functions are applied when you read training data , read batch data , or get feature vectors . Python Attaching transformation functions to the feature view plus_one_fn = fs . get_transformation_function ( name = \"plus_one\" , version = 1 ) feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"amount_spent\" : plus_one_fn } ) Built-in transformation functions are attached in the same way. The only difference is that it will compute the necessary statistics for the specific function in the background. For example min and max values for min_max_scaler ; mean and standard deviation for standard_scaler etc. Python Attaching built-in transformation functions to the feature view min_max_scaler = fs . get_transformation_function ( name = \"min_max_scaler\" ) standard_scaler = fs . get_transformation_function ( name = \"standard_scaler\" ) robust_scaler = fs . get_transformation_function ( name = \"robust_scaler\" ) label_encoder = fs . get_transformation_function ( name = \"label_encoder\" ) feature_view = fs . create_feature_view ( name = 'transactions_view' , query = query , labels = [ \"fraud_label\" ], transformation_functions = { \"category\" : label_encoder , \"amount\" : robust_scaler , \"loc_delta\" : min_max_scaler , \"age_at_transaction\" : standard_scaler } ) Java/Scala support Creating and attaching Transformation functions to feature views are not supported for HSFS Java or Scala client. If feature view with transformation function was created using python client, you cannot get training data or get feature vectors from HSFS Java or Scala client.","title":"Apply transformation functions to features"},{"location":"user_guides/fs/storage_connector/","text":"Storage Connector Guides # You can define storage connectors in Hopsworks for batch and streaming data sources. Storage connectors securely store the authentication information about how to connect to an external data store. They can be used from programs within Hopsworks or externally. There are three main use cases for Storage Connectors: Simply use it to read data from the storage into a dataframe. External (on-demand) Feature Groups can be defined with storage connectors as data source. This way, Hopsworks stores only the metadata about the features, but does not keep a copy of the data itself. This is also called the Connector API. Write training data to an external storage system to make it accessible by third parties. Storage connectors provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project. By default, each project is created with three default Storage Connectors: A JDBC connector to the online feature store, a HopsFS connector to the Training Datasets directory of the project and a JDBC connector to the offline feature store. The Storage Connector View in the User Interface Cloud Agnostic # Cloud agnostic storage systems: JDBC : Connect to JDBC compatible databases and query them using SQL. Snowflake : Query Snowflake databases and tables using SQL. Kafka : Read data from a Kafka cluster into a Spark Structured Streaming Dataframe. HopsFS : Easily connect and read from directories of Hopsworks' internal File System. AWS # For AWS the following storage systems are supported: S3 : Read data from a variety of file based storage in S3 such as parquet or CSV. Redshift : Query Redshift databases and tables using SQL. Azure # For AWS the following storage systems are supported: ADLS : Read data from a variety of file based storage in ADLS such as parquet or CSV. GCP # For GCP the following storage systems are supported: BigQuery : Query BigQuery databases and tables using SQL. GCS : Read data from a variety of file based storage in Google Cloud Storage such as parquet or CSV. Next Steps # Move on to the Configuration and Creation Guides to learn how to set up a storage connector.","title":"Storage Connector Guides"},{"location":"user_guides/fs/storage_connector/#storage-connector-guides","text":"You can define storage connectors in Hopsworks for batch and streaming data sources. Storage connectors securely store the authentication information about how to connect to an external data store. They can be used from programs within Hopsworks or externally. There are three main use cases for Storage Connectors: Simply use it to read data from the storage into a dataframe. External (on-demand) Feature Groups can be defined with storage connectors as data source. This way, Hopsworks stores only the metadata about the features, but does not keep a copy of the data itself. This is also called the Connector API. Write training data to an external storage system to make it accessible by third parties. Storage connectors provide two main mechanisms for authentication: using credentials or an authentication role (IAM Role on AWS or Managed Identity on Azure). Hopsworks supports both a single IAM role (AWS) or Managed Identity (Azure) for the whole Hopsworks cluster or multiple IAM roles (AWS) or Managed Identities (Azure) that can only be assumed by users with a specific role in a specific project. By default, each project is created with three default Storage Connectors: A JDBC connector to the online feature store, a HopsFS connector to the Training Datasets directory of the project and a JDBC connector to the offline feature store. The Storage Connector View in the User Interface","title":"Storage Connector Guides"},{"location":"user_guides/fs/storage_connector/#cloud-agnostic","text":"Cloud agnostic storage systems: JDBC : Connect to JDBC compatible databases and query them using SQL. Snowflake : Query Snowflake databases and tables using SQL. Kafka : Read data from a Kafka cluster into a Spark Structured Streaming Dataframe. HopsFS : Easily connect and read from directories of Hopsworks' internal File System.","title":"Cloud Agnostic"},{"location":"user_guides/fs/storage_connector/#aws","text":"For AWS the following storage systems are supported: S3 : Read data from a variety of file based storage in S3 such as parquet or CSV. Redshift : Query Redshift databases and tables using SQL.","title":"AWS"},{"location":"user_guides/fs/storage_connector/#azure","text":"For AWS the following storage systems are supported: ADLS : Read data from a variety of file based storage in ADLS such as parquet or CSV.","title":"Azure"},{"location":"user_guides/fs/storage_connector/#gcp","text":"For GCP the following storage systems are supported: BigQuery : Query BigQuery databases and tables using SQL. GCS : Read data from a variety of file based storage in Google Cloud Storage such as parquet or CSV.","title":"GCP"},{"location":"user_guides/fs/storage_connector/#next-steps","text":"Move on to the Configuration and Creation Guides to learn how to set up a storage connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/usage/","text":"Storage Connector Usage # Retrieving a Storage Connector # Reading a Spark Dataframe from a Storage Connector # Creating an External Feature Group # Writing Training Data #","title":"Usage"},{"location":"user_guides/fs/storage_connector/usage/#storage-connector-usage","text":"","title":"Storage Connector Usage"},{"location":"user_guides/fs/storage_connector/usage/#retrieving-a-storage-connector","text":"","title":"Retrieving a Storage Connector"},{"location":"user_guides/fs/storage_connector/usage/#reading-a-spark-dataframe-from-a-storage-connector","text":"","title":"Reading a Spark Dataframe from a Storage Connector"},{"location":"user_guides/fs/storage_connector/usage/#creating-an-external-feature-group","text":"","title":"Creating an External Feature Group"},{"location":"user_guides/fs/storage_connector/usage/#writing-training-data","text":"","title":"Writing Training Data"},{"location":"user_guides/fs/storage_connector/creation/adls/","text":"How-To set up a ADLS Storage Connector # Introduction # Azure Data Lake Storage (ADLS) Gen2 is a HDFS-compatible filesystem on Azure for data analytics. The ADLS Gen2 filesystem stores its data in Azure Blob storage, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access ADLS Gen2 by defining a Storage Connector and creating and granting persmissions to a service principal. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Azure ADLS filesystem. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your Azure ADLS account: Data Lake Storage Gen2 Account: Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace . Note that your storage account must belong to an Azure resource group. Azure AD application and service principal: Create an Azure AD application and service principal that can access your ADLS storage account and its resource group. Service Principal Registration: Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account. Info When you specify the 'container name' in the ADLS storage connector, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter ADLS Information # Enter the details for your ADLS connector. Start by giving it a name and an optional description . ADLS Connector Creation Form Step 3: Azure Create an ADLS Resource # When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps: Select Azure Active Directory. From App registrations in Azure AD, select your application. Copy the Directory (tenant) ID and store it in your application code. You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS storage connector \"Directory id\" text field. Copy the Application ID and store it in your application code. >You need to copy the Application id and paste it to the Hopsworks ADLS storage connector \"Application id\" text field. Create an Application Secret and copy it into the Service Credential field. You need to copy the Application Secret and paste it to the Hopsworks ADLS storage connector \"Service Credential\" text field. Common Problems # If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" buton to add a \"role assignment\". If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container. References # How to create a service principal on Azure Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created ADLS connector.","title":"ADLS"},{"location":"user_guides/fs/storage_connector/creation/adls/#how-to-set-up-a-adls-storage-connector","text":"","title":"How-To set up a ADLS Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/adls/#introduction","text":"Azure Data Lake Storage (ADLS) Gen2 is a HDFS-compatible filesystem on Azure for data analytics. The ADLS Gen2 filesystem stores its data in Azure Blob storage, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access ADLS Gen2 by defining a Storage Connector and creating and granting persmissions to a service principal. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Azure ADLS filesystem. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/adls/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your Azure ADLS account: Data Lake Storage Gen2 Account: Create an Azure Data Lake Storage Gen2 account and initialize a filesystem, enabling the hierarchical namespace . Note that your storage account must belong to an Azure resource group. Azure AD application and service principal: Create an Azure AD application and service principal that can access your ADLS storage account and its resource group. Service Principal Registration: Register the service principal, granting it a role assignment such as Storage Blob Data Contributor, on the Azure Data Lake Storage Gen2 account. Info When you specify the 'container name' in the ADLS storage connector, you need to have previously created that container - the Hopsworks Feature Store will not create that storage container for you.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/adls/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/adls/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/adls/#step-2-enter-adls-information","text":"Enter the details for your ADLS connector. Start by giving it a name and an optional description . ADLS Connector Creation Form","title":"Step 2: Enter ADLS Information"},{"location":"user_guides/fs/storage_connector/creation/adls/#step-3-azure-create-an-adls-resource","text":"When programmatically signing in, you need to pass the tenant ID with your authentication request and the application ID. You also need a certificate or an authentication key (described in the following section). To get those values, use the following steps: Select Azure Active Directory. From App registrations in Azure AD, select your application. Copy the Directory (tenant) ID and store it in your application code. You need to copy the Directory (tenant) id and paste it to the Hopsworks ADLS storage connector \"Directory id\" text field. Copy the Application ID and store it in your application code. >You need to copy the Application id and paste it to the Hopsworks ADLS storage connector \"Application id\" text field. Create an Application Secret and copy it into the Service Credential field. You need to copy the Application Secret and paste it to the Hopsworks ADLS storage connector \"Service Credential\" text field.","title":"Step 3: Azure Create an ADLS Resource"},{"location":"user_guides/fs/storage_connector/creation/adls/#common-problems","text":"If you get a permission denied error when writing or reading to/from a ADLS container, it is often because the storage principal (app) does not have the correct permissions. Have you added the \"Storage Blob Data Owner\" or \"Storage Blob Data Contributor\" role to the resource group for your storage account (or the subscription for your storage group, if you apply roles at the subscription level)? Go to your resource group, then in \"Access Control (IAM)\", click the \"Add\" buton to add a \"role assignment\". If you get an error \"StatusCode=404 StatusDescription=The specified filesystem does not exist.\", then maybe you have not created the storage account or the storage container.","title":"Common Problems"},{"location":"user_guides/fs/storage_connector/creation/adls/#references","text":"How to create a service principal on Azure","title":"References"},{"location":"user_guides/fs/storage_connector/creation/adls/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created ADLS connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/bigquery/","text":"How-To set up a BigQuery Storage Connector # Introduction # A BigQuery storage connector provides integration to Google Cloud BigQuery. BigQuery is Google Cloud's managed data warehouse supporting that lets you run analytics and execute SQL queries over large scale data. Such data warehouses are often the source of raw data for feature engineering pipelines. In this guide, you will configure a Storage Connector in Hopsworks to connect to your BigQuery project by saving the necessary information. When you're finished, you'll be able to execute queries and read results of BigQuery using Spark through HSFS APIs. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information about your GCP account: BigQuery Project: You need a BigQuery project, dataset and table created and have read access to it. Or, if you wish to query a public dataset you need its corresponding details. Authentication Method: Authentication to GCP account is handled by uploading the JSON keyfile for service account to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter connector details # Enter the details for your BigQuery connector. Start by giving it a unique name and an optional description . BigQuery Connector Creation Form Choose Google BigQuery from the connector options. Next, set the name of the parent BigQuery project. This is used for billing by GCP. Authentication: Here you should upload your JSON keyfile for service account used for authentication. You can choose to either upload from your local using Upload new file or choose an existing file within project using From Project . Read Options: There are two ways to read via BigQuery, using the BigQuery Table or BigQuery Query option: The table option reads directly from the BigQuery table reference. In the UI set the below fields, BigQuery Project : The BigQuery project BigQuery Dataset : The dataset of the table BigQuery Table : The table to read The second option is to read by executing a SQL query at runtime, by selecting BigQuery Query and setting, Materiliazation Dataset : Temporary dataset used by BigQuery for writing Spark Options: Optionally, you can set additional spark options using the Key - Value pairs Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created BigQuery connector.","title":"BigQuery"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#how-to-set-up-a-bigquery-storage-connector","text":"","title":"How-To set up a BigQuery Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#introduction","text":"A BigQuery storage connector provides integration to Google Cloud BigQuery. BigQuery is Google Cloud's managed data warehouse supporting that lets you run analytics and execute SQL queries over large scale data. Such data warehouses are often the source of raw data for feature engineering pipelines. In this guide, you will configure a Storage Connector in Hopsworks to connect to your BigQuery project by saving the necessary information. When you're finished, you'll be able to execute queries and read results of BigQuery using Spark through HSFS APIs. The storage connector uses the Google spark-bigquery-connector behind the scenes. To read more about the spark connector, like the spark options or usage, check Apache Spark SQL connector for Google BigQuery. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information about your GCP account: BigQuery Project: You need a BigQuery project, dataset and table created and have read access to it. Or, if you wish to query a public dataset you need its corresponding details. Authentication Method: Authentication to GCP account is handled by uploading the JSON keyfile for service account to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#step-2-enter-connector-details","text":"Enter the details for your BigQuery connector. Start by giving it a unique name and an optional description . BigQuery Connector Creation Form Choose Google BigQuery from the connector options. Next, set the name of the parent BigQuery project. This is used for billing by GCP. Authentication: Here you should upload your JSON keyfile for service account used for authentication. You can choose to either upload from your local using Upload new file or choose an existing file within project using From Project . Read Options: There are two ways to read via BigQuery, using the BigQuery Table or BigQuery Query option: The table option reads directly from the BigQuery table reference. In the UI set the below fields, BigQuery Project : The BigQuery project BigQuery Dataset : The dataset of the table BigQuery Table : The table to read The second option is to read by executing a SQL query at runtime, by selecting BigQuery Query and setting, Materiliazation Dataset : Temporary dataset used by BigQuery for writing Spark Options: Optionally, you can set additional spark options using the Key - Value pairs","title":"Step 2: Enter connector details"},{"location":"user_guides/fs/storage_connector/creation/bigquery/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created BigQuery connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/gcs/","text":"How-To set up a GCS Storage Connector # Introduction # This particular type of storage connector provides integration to Google Cloud Storage (GCS). GCS is an object storage service offered by Google Cloud. An object could be simply any piece of immutable data consisting of a file of any format, for example a CSV or PARQUET . These objects are stored in containers called as buckets . These types of storages are often the source for raw data from which features can be engineered. In this guide, you will configure a Storage Connector in Hopsworks to connect to your GCS bucket by saving the necessary information. When you're finished, you'll be able to read files from the GCS bucket using Spark through HSFS APIs. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information about your GCP account and bucket: Bucket: You need a GCS bucket created and have read access to it. The bucket is identified by its name. Authentication Method: Authentication to GCP account is handled by uploading the JSON keyfile for service account to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. Server-side Encryption GCS encrypts the data on server side by default. The connector additionally supports the optional encryption method Customer Supplied Encryption Key by GCP. You can choose the encryption option AES-256 and provide AES-256 key and hash, encoded in standard Base64. The encryption details are stored as Secrets in the Hopsworks for keeping it secure. Read more about encryption on Google Documentation. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter connector details # Enter the details for your GCS connector. Start by giving it a unique name and an optional description . GCS Connector Creation Form Choose Google Cloud Storage from the connector options. Next, set the name of the GCS Bucket you wish to connect with. Authentication: Here you should upload your JSON keyfile for service account used for authentication. You can choose to either upload from your local using Upload new file or choose an existing file within project using From Project . GCS Server Side Encryption: You can leave this to Default Encryption if you do not wish to provide explicit encrypting keys. Otherwise, optionally you can set the encryption setting for AES-256 and provide the encryption key and hash when selected. Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created GCS connector.","title":"GCS"},{"location":"user_guides/fs/storage_connector/creation/gcs/#how-to-set-up-a-gcs-storage-connector","text":"","title":"How-To set up a GCS Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/gcs/#introduction","text":"This particular type of storage connector provides integration to Google Cloud Storage (GCS). GCS is an object storage service offered by Google Cloud. An object could be simply any piece of immutable data consisting of a file of any format, for example a CSV or PARQUET . These objects are stored in containers called as buckets . These types of storages are often the source for raw data from which features can be engineered. In this guide, you will configure a Storage Connector in Hopsworks to connect to your GCS bucket by saving the necessary information. When you're finished, you'll be able to read files from the GCS bucket using Spark through HSFS APIs. The storage connector uses the Google gcs-connector-hadoop behind the scenes. For more information, check out Google Cloud Storage Connector for Spark and Hadoop Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/gcs/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information about your GCP account and bucket: Bucket: You need a GCS bucket created and have read access to it. The bucket is identified by its name. Authentication Method: Authentication to GCP account is handled by uploading the JSON keyfile for service account to the Hopsworks Project. You will need to create this JSON keyfile from GCP. For more information on service accounts and creating keyfile in GCP, read Google Cloud documentation. Server-side Encryption GCS encrypts the data on server side by default. The connector additionally supports the optional encryption method Customer Supplied Encryption Key by GCP. You can choose the encryption option AES-256 and provide AES-256 key and hash, encoded in standard Base64. The encryption details are stored as Secrets in the Hopsworks for keeping it secure. Read more about encryption on Google Documentation.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/gcs/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/gcs/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/gcs/#step-2-enter-connector-details","text":"Enter the details for your GCS connector. Start by giving it a unique name and an optional description . GCS Connector Creation Form Choose Google Cloud Storage from the connector options. Next, set the name of the GCS Bucket you wish to connect with. Authentication: Here you should upload your JSON keyfile for service account used for authentication. You can choose to either upload from your local using Upload new file or choose an existing file within project using From Project . GCS Server Side Encryption: You can leave this to Default Encryption if you do not wish to provide explicit encrypting keys. Otherwise, optionally you can set the encryption setting for AES-256 and provide the encryption key and hash when selected.","title":"Step 2: Enter connector details"},{"location":"user_guides/fs/storage_connector/creation/gcs/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created GCS connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/","text":"How-To set up a HopsFS Storage Connector # Introduction # HopsFS is a HDFS-compatible filesystem on AWS/Azure/on-premises for data analytics. HopsFS stores its data on object storage in the cloud (S3 in AWs and Blob storage on Azure) and on commodity servers on-premises, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access HopsFS natively in programs (Spark, TensorFlow, etc) without the need to define a Storage Connector. By default, every Project has a Storage Connector for Training Datasets. When you create training datasets from features in the Feature Store the HopsFS connector is the default Storage Connector. However, if you want to output data to a different dataset, you can define a new Storage Connector for that dataset. In this guide, you will configure a HopsFS Storage Connector in Hopsworks which points at a different directory on the file system than the Training Datasets directory. When you're finished, you'll be able to write training data to different locations in your cluster through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to identify a directory on the filesystem of Hopsworks, to which you want to point the Storage Connector that you are going to create. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter HopsFS Settings # Enter the details for your HopsFS connector. Start by giving it a name and an optional description . Select \"HopsFS\" as connector protocol. Select the top-level directory to point the connector to. Click \"Setup storage connector\". HopsFS Connector Creation Form Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created HopsFS connector.","title":"HopsFS"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#how-to-set-up-a-hopsfs-storage-connector","text":"","title":"How-To set up a HopsFS Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#introduction","text":"HopsFS is a HDFS-compatible filesystem on AWS/Azure/on-premises for data analytics. HopsFS stores its data on object storage in the cloud (S3 in AWs and Blob storage on Azure) and on commodity servers on-premises, ensuring low-cost storage, high availability, and disaster recovery. In Hopsworks, you can access HopsFS natively in programs (Spark, TensorFlow, etc) without the need to define a Storage Connector. By default, every Project has a Storage Connector for Training Datasets. When you create training datasets from features in the Feature Store the HopsFS connector is the default Storage Connector. However, if you want to output data to a different dataset, you can define a new Storage Connector for that dataset. In this guide, you will configure a HopsFS Storage Connector in Hopsworks which points at a different directory on the file system than the Training Datasets directory. When you're finished, you'll be able to write training data to different locations in your cluster through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#prerequisites","text":"Before you begin this guide you'll need to identify a directory on the filesystem of Hopsworks, to which you want to point the Storage Connector that you are going to create.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#step-2-enter-hopsfs-settings","text":"Enter the details for your HopsFS connector. Start by giving it a name and an optional description . Select \"HopsFS\" as connector protocol. Select the top-level directory to point the connector to. Click \"Setup storage connector\". HopsFS Connector Creation Form","title":"Step 2: Enter HopsFS Settings"},{"location":"user_guides/fs/storage_connector/creation/hopsfs/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created HopsFS connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/jdbc/","text":"How-To set up a JDBC Storage Connector # Introduction # JDBC is an API provided by many database systems. Using JDBC connections one can query and update data in a database, usually oriented towards relational databases. Examples of databases you can connect to using JDBC are MySQL, Postgres, ORacle, DB2, MongoDB or Microsoft SQLServer. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a JDBC connection to your database of choice. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your JDBC compatible database: JDBC Connection URL: Consult the documentation of your target database to determine the correct JDBC URL and parameters. As an example, for MySQL the URL could be: jdbc:mysql://10.0.2.15:3306/[databaseName]?useSSL=false&allowPublicKeyRetrieval=true Username and Password: Typically, you will need to add username and password in your JDBC URL or as key/value parameters. So make sure you have retrieved a username and password with the suitable permissions for the database and table you want to query. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter JDBC Settings # Enter the details for your JDBC enabled database. Select \"JDBC\" as connector protocol. Enter the JDBC connection url. This can for example also contain the username and password. Add additional key/value arguments to be passed to the connection. These might differ by database. It can be the username and password. Click \"Setup storage connector\". JDBC Connector Creation Form Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created JDBC connector.","title":"JDBC"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#how-to-set-up-a-jdbc-storage-connector","text":"","title":"How-To set up a JDBC Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#introduction","text":"JDBC is an API provided by many database systems. Using JDBC connections one can query and update data in a database, usually oriented towards relational databases. Examples of databases you can connect to using JDBC are MySQL, Postgres, ORacle, DB2, MongoDB or Microsoft SQLServer. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a JDBC connection to your database of choice. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your JDBC compatible database: JDBC Connection URL: Consult the documentation of your target database to determine the correct JDBC URL and parameters. As an example, for MySQL the URL could be: jdbc:mysql://10.0.2.15:3306/[databaseName]?useSSL=false&allowPublicKeyRetrieval=true Username and Password: Typically, you will need to add username and password in your JDBC URL or as key/value parameters. So make sure you have retrieved a username and password with the suitable permissions for the database and table you want to query.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#step-2-enter-jdbc-settings","text":"Enter the details for your JDBC enabled database. Select \"JDBC\" as connector protocol. Enter the JDBC connection url. This can for example also contain the username and password. Add additional key/value arguments to be passed to the connection. These might differ by database. It can be the username and password. Click \"Setup storage connector\". JDBC Connector Creation Form","title":"Step 2: Enter JDBC Settings"},{"location":"user_guides/fs/storage_connector/creation/jdbc/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created JDBC connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/kafka/","text":"How-To set up a Kafka Storage Connector # Introduction # Apache Kafka is a distributed event store and stream-processing platform. It's very popular framework for handling realtime data streams and is often used as a message you for events coming from production systems until they are being processed and either loaded into a data warehouse our aggregated into features for Machine Learning. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Kafka cluster. When you're finished, you'll be able to read from Kafka topics in your cluster using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from Kafka cluster, the following options are mandatory : Kafka Bootstrap servers: It is the url of one of the Kafka brokers which you give to fetch the initial metadata about your Kafka cluster. The metadata consists of the topics, their partitions, the leader brokers for those partitions etc. Depending upon this metadata your producer or consumer produces or consumes the data. Security Protocol: The security protocol you want to use to authenticate with your Kafka cluster. Make sure the chosen protocol is supported by your cluster. For an overview of the available protocols, please see the Confluent Kafka Documentation . Certificates: Depending on the chosen security protocol, you might need TrustStore and KeyStore files along with the corresponding key password. Contact your Kafka administrator, if you don't know how to retrieve these. If you want to setup a storage connector to Hopsworks' internal Kafka cluster, you can download the needed certificates from the integration tab in your project settings. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter Kafka Settings # Enter the details for your Kafka connector. Start by giving it a name and an optional description . Select \"Kafka\" as connector protocol. Add all the bootstrap server addresses and ports that you want the consumers/producers to connect to. The client will make use of all servers irrespective of which servers are specified here for bootstrapping\u2014this list only impacts the initial hosts used to discover the full set of servers. Choose the Security protocol. TSL/SSL By default, Apache Kafka communicates in PLAINTEXT , which means that all data is sent in the clear. To encrypt communication, you should configure all the Confluent Platform components in your deployment to use TLS/SSL encryption. TLS uses private-key/certificate pairs, which are used during the TLS handshake process. Each broker needs its own private-key/certificate pair, and the client uses the certificate to authenticate the broker. Each logical client needs a private-key/certificate pair if client authentication is enabled, and the broker uses the certificate to authenticate the client. These are provided in the form of TrustStore and KeyStore JKS files together with a key password. For more information, refer to the official Apacha Kafka Guide for TSL/SSL authentication . SASL SSL or SASL plaintext Apache Kafka brokers support client authentication using SASL. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). This authentication method often requires extra arguments depending on your setup. Make use of the optional additional key/value arguments (5) to provide these. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). For more information, please refer to the offical Apache Kafka Guide for SASL authentication . The endpoint identification algorithm used by clients to validate server host name. The default value is https . Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Optional additional key/value arguments. Click \"Setup storage connector\". Kafka Connector Creation Form Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created Kafka connector.","title":"Kafka"},{"location":"user_guides/fs/storage_connector/creation/kafka/#how-to-set-up-a-kafka-storage-connector","text":"","title":"How-To set up a Kafka Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/kafka/#introduction","text":"Apache Kafka is a distributed event store and stream-processing platform. It's very popular framework for handling realtime data streams and is often used as a message you for events coming from production systems until they are being processed and either loaded into a data warehouse our aggregated into features for Machine Learning. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Kafka cluster. When you're finished, you'll be able to read from Kafka topics in your cluster using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/kafka/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from Kafka cluster, the following options are mandatory : Kafka Bootstrap servers: It is the url of one of the Kafka brokers which you give to fetch the initial metadata about your Kafka cluster. The metadata consists of the topics, their partitions, the leader brokers for those partitions etc. Depending upon this metadata your producer or consumer produces or consumes the data. Security Protocol: The security protocol you want to use to authenticate with your Kafka cluster. Make sure the chosen protocol is supported by your cluster. For an overview of the available protocols, please see the Confluent Kafka Documentation . Certificates: Depending on the chosen security protocol, you might need TrustStore and KeyStore files along with the corresponding key password. Contact your Kafka administrator, if you don't know how to retrieve these. If you want to setup a storage connector to Hopsworks' internal Kafka cluster, you can download the needed certificates from the integration tab in your project settings.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/kafka/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/kafka/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/kafka/#step-2-enter-kafka-settings","text":"Enter the details for your Kafka connector. Start by giving it a name and an optional description . Select \"Kafka\" as connector protocol. Add all the bootstrap server addresses and ports that you want the consumers/producers to connect to. The client will make use of all servers irrespective of which servers are specified here for bootstrapping\u2014this list only impacts the initial hosts used to discover the full set of servers. Choose the Security protocol. TSL/SSL By default, Apache Kafka communicates in PLAINTEXT , which means that all data is sent in the clear. To encrypt communication, you should configure all the Confluent Platform components in your deployment to use TLS/SSL encryption. TLS uses private-key/certificate pairs, which are used during the TLS handshake process. Each broker needs its own private-key/certificate pair, and the client uses the certificate to authenticate the broker. Each logical client needs a private-key/certificate pair if client authentication is enabled, and the broker uses the certificate to authenticate the client. These are provided in the form of TrustStore and KeyStore JKS files together with a key password. For more information, refer to the official Apacha Kafka Guide for TSL/SSL authentication . SASL SSL or SASL plaintext Apache Kafka brokers support client authentication using SASL. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). This authentication method often requires extra arguments depending on your setup. Make use of the optional additional key/value arguments (5) to provide these. SASL authentication can be enabled concurrently with TLS/SSL encryption (TLS/SSL client authentication will be disabled). For more information, please refer to the offical Apache Kafka Guide for SASL authentication . The endpoint identification algorithm used by clients to validate server host name. The default value is https . Clients including client connections created by the broker for inter-broker communication verify that the broker host name matches the host name in the broker\u2019s certificate. Optional additional key/value arguments. Click \"Setup storage connector\". Kafka Connector Creation Form","title":"Step 2: Enter Kafka Settings"},{"location":"user_guides/fs/storage_connector/creation/kafka/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created Kafka connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/redshift/","text":"How-To set up a Redshift Storage Connector # Introduction # Amazon Redshift is a popular managed data warehouse on AWS, used as a data warehouse in many enterprises. Data warehouses are often the source of raw data for feature engineering pipelines and Redshift supports scalable feature computation with SQL. However, Redshift is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS Redshift cluster. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your AWS account and Redshift database, the following options are mandatory : Cluster identifier: The name of the cluster. Database driver: You can use the default JDBC Redshift Driver com.amazon.redshift.jdbc42.Driver (More on this later) Database endpoint: The endpoint for the database. Should be in the format of [UUID].eu-west-1.redshift.amazonaws.com . Database name: The name of the database to query. Database port: The port of the cluster. Defaults to 5349. Authentication method: There are two options available for authenticating with the Redshift cluster. The first option is to configure a username and a password. The second option is to configure an IAM role. With IAM roles, Jobs or notebooks launched on Hopsworks do not need to explicitly authenticate with Redshift, as the HSFS library will transparently use the IAM role to acquire a temporary credential to authenticate the specified user. Read more about IAM roles in our AWS credentials passthrough guide Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter The Connector Information # Enter the details for your Redshift connector. Start by giving it a name and an optional description . Select \"Redshift\" as connector protocol. The name of the cluster. The name of the database driver class. The database endpoint. The database name. The database port. The database username, here you have the possibility to let Hopwsworks auto-create the username for you. Optionally provide the database group and table to point the connector to. Set the appropriate authentication method. Redshift Connector Creation Form Step 3: Upload the Redshift database driver # With regards to the database driver, the library to interact with Redshift is not included in Hopsworks - you need to upload the driver yourself. First, you need to download the library . Select the driver version without the AWS SDK. Upload the driver to Hopsworks # You then upload the driver files to the \u201cResources\u201d directory in your project, see the screenshot below. Open the File Browser. Move into the \"Resources\" directory. Upload the driver jar file. Redshift Driver Upload in the Filesystem Browser Add the driver to Jupyter Notebooks and Spark jobs # You can now add the driver file to the default job and Jupyter configuration. This way, all jobs and Jupyter instances in the project will have the driver attached so that Spark can access it. Go into the Project's settings. Select \"Compute configuration\". Select \"Spark\". Select and Upload the jar file in the \"additional jars\" section of the configuration. Attaching the Redshift Driver to all Jobs and Jupyter Instances of the Project Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created Redshift connector.","title":"Redshift"},{"location":"user_guides/fs/storage_connector/creation/redshift/#how-to-set-up-a-redshift-storage-connector","text":"","title":"How-To set up a Redshift Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/redshift/#introduction","text":"Amazon Redshift is a popular managed data warehouse on AWS, used as a data warehouse in many enterprises. Data warehouses are often the source of raw data for feature engineering pipelines and Redshift supports scalable feature computation with SQL. However, Redshift is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS Redshift cluster. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/redshift/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your AWS account and Redshift database, the following options are mandatory : Cluster identifier: The name of the cluster. Database driver: You can use the default JDBC Redshift Driver com.amazon.redshift.jdbc42.Driver (More on this later) Database endpoint: The endpoint for the database. Should be in the format of [UUID].eu-west-1.redshift.amazonaws.com . Database name: The name of the database to query. Database port: The port of the cluster. Defaults to 5349. Authentication method: There are two options available for authenticating with the Redshift cluster. The first option is to configure a username and a password. The second option is to configure an IAM role. With IAM roles, Jobs or notebooks launched on Hopsworks do not need to explicitly authenticate with Redshift, as the HSFS library will transparently use the IAM role to acquire a temporary credential to authenticate the specified user. Read more about IAM roles in our AWS credentials passthrough guide","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/redshift/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/redshift/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/redshift/#step-2-enter-the-connector-information","text":"Enter the details for your Redshift connector. Start by giving it a name and an optional description . Select \"Redshift\" as connector protocol. The name of the cluster. The name of the database driver class. The database endpoint. The database name. The database port. The database username, here you have the possibility to let Hopwsworks auto-create the username for you. Optionally provide the database group and table to point the connector to. Set the appropriate authentication method. Redshift Connector Creation Form","title":"Step 2: Enter The Connector Information"},{"location":"user_guides/fs/storage_connector/creation/redshift/#step-3-upload-the-redshift-database-driver","text":"With regards to the database driver, the library to interact with Redshift is not included in Hopsworks - you need to upload the driver yourself. First, you need to download the library . Select the driver version without the AWS SDK.","title":"Step 3: Upload the Redshift database driver"},{"location":"user_guides/fs/storage_connector/creation/redshift/#upload-the-driver-to-hopsworks","text":"You then upload the driver files to the \u201cResources\u201d directory in your project, see the screenshot below. Open the File Browser. Move into the \"Resources\" directory. Upload the driver jar file. Redshift Driver Upload in the Filesystem Browser","title":"Upload the driver to Hopsworks"},{"location":"user_guides/fs/storage_connector/creation/redshift/#add-the-driver-to-jupyter-notebooks-and-spark-jobs","text":"You can now add the driver file to the default job and Jupyter configuration. This way, all jobs and Jupyter instances in the project will have the driver attached so that Spark can access it. Go into the Project's settings. Select \"Compute configuration\". Select \"Spark\". Select and Upload the jar file in the \"additional jars\" section of the configuration. Attaching the Redshift Driver to all Jobs and Jupyter Instances of the Project","title":"Add the driver to Jupyter Notebooks and Spark jobs"},{"location":"user_guides/fs/storage_connector/creation/redshift/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created Redshift connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/s3/","text":"How-To set up a S3 Storage Connector # Introduction # Amazon S3 or Amazon Simple Storage Service is a service offered by AWS that provides object storage. That means you can store arbitrary objects associated with a key. These kind of storage systems are often used as Data Lakes with large volumes of unstructured data or file based storage. Popular file formats are CSV or PARQUET . There are so called Data Lake House technologies such as Delta Lake or Apache Hudi, building an additional layer on top of object based storage with files, to provide database semantics like ACID transactions among others. This has the advantage that cheap storage can be turned into a cloud native data warehouse. These kind of storages are often the source for raw data from which features can be engineered. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS S3 bucket. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your AWS S3 account and bucket: Bucket: You will need a S3 bucket that you have access to. The bucket is identified by its name. Authentication Method: You can authenticate using Access Key/Secret, or use IAM roles. If you want to use an IAM role it either needs to be attached to the entire Hopsworks cluster or Hopsworks needs to be able to assume the role. See IAM role documentation for more information. TODO: add link Server Side Encryption details: If your bucket has server side encryption (SSE) enabled, make sure you know which algorithm it is using (AES256 or SSE-KMS). If you are using SSE-KMS, you need the resource ARN of the managed key. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter Bucket Information # Enter the details for your S3 connector. Start by giving it a name and an optional description . And set the name of the S3 Bucket you want to point the connector to. S3 Connector Creation Form Step 3: Configure Authentication # Instance Role # Choose instance role if you have an EC2 instance role attached to your Hopsworks cluster which grants you access to the specified bucket. Temporary Credentials # Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . If you or your Hopsworks administrator followed the AWS Credentials Passthrough Guide you should be able to choose \"Temporary Credentials\" and see the roles, that your user is able to assume. Choose the role that gives you access to the specified bucket. Access Key/Secret # The most simple authentication method are Access Key/Secret, choose this option to get started quickly, if you are able retrieve the keys using the IAM user administration. Step 4: Configure Server Side Encryption # Additionally, you can specify if your Bucket has SSE enabled. AES256 # For AES256, there is nothing to do but enabling the encryption by toggling the AES256 option. This is using S3-Managed Keys, also called SSE-S3 . SSE-KMS # With this option the encryption key is managed by AWS KMS , with some additional benefits and charges for using this service. The difference is that you need to provide the resource ARN of the key. If you have SSE-KMS enabled for your bucket, you can find the key ARN in the \"Properties\" section of the bucket details on AWS. Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created S3 connector.","title":"S3"},{"location":"user_guides/fs/storage_connector/creation/s3/#how-to-set-up-a-s3-storage-connector","text":"","title":"How-To set up a S3 Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/s3/#introduction","text":"Amazon S3 or Amazon Simple Storage Service is a service offered by AWS that provides object storage. That means you can store arbitrary objects associated with a key. These kind of storage systems are often used as Data Lakes with large volumes of unstructured data or file based storage. Popular file formats are CSV or PARQUET . There are so called Data Lake House technologies such as Delta Lake or Apache Hudi, building an additional layer on top of object based storage with files, to provide database semantics like ACID transactions among others. This has the advantage that cheap storage can be turned into a cloud native data warehouse. These kind of storages are often the source for raw data from which features can be engineered. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your AWS S3 bucket. When you're finished, you'll be able to read files using Spark through HSFS APIs. You can also use the connector to write out training data from the Feature Store, in order to make it accessible by third parties. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/s3/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your AWS S3 account and bucket: Bucket: You will need a S3 bucket that you have access to. The bucket is identified by its name. Authentication Method: You can authenticate using Access Key/Secret, or use IAM roles. If you want to use an IAM role it either needs to be attached to the entire Hopsworks cluster or Hopsworks needs to be able to assume the role. See IAM role documentation for more information. TODO: add link Server Side Encryption details: If your bucket has server side encryption (SSE) enabled, make sure you know which algorithm it is using (AES256 or SSE-KMS). If you are using SSE-KMS, you need the resource ARN of the managed key.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/s3/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-2-enter-bucket-information","text":"Enter the details for your S3 connector. Start by giving it a name and an optional description . And set the name of the S3 Bucket you want to point the connector to. S3 Connector Creation Form","title":"Step 2: Enter Bucket Information"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-3-configure-authentication","text":"","title":"Step 3: Configure Authentication"},{"location":"user_guides/fs/storage_connector/creation/s3/#instance-role","text":"Choose instance role if you have an EC2 instance role attached to your Hopsworks cluster which grants you access to the specified bucket.","title":"Instance Role"},{"location":"user_guides/fs/storage_connector/creation/s3/#temporary-credentials","text":"Using an EC2 instance profile enables your Hopsworks cluster to access AWS resources. This forces all Hopsworks users to share the instance profile role and the resource access policies attached to that role. To allow for per project access policies you could have your users use AWS credentials directly in their programs which is not recommended so you should instead use Role chaining . If you or your Hopsworks administrator followed the AWS Credentials Passthrough Guide you should be able to choose \"Temporary Credentials\" and see the roles, that your user is able to assume. Choose the role that gives you access to the specified bucket.","title":"Temporary Credentials"},{"location":"user_guides/fs/storage_connector/creation/s3/#access-keysecret","text":"The most simple authentication method are Access Key/Secret, choose this option to get started quickly, if you are able retrieve the keys using the IAM user administration.","title":"Access Key/Secret"},{"location":"user_guides/fs/storage_connector/creation/s3/#step-4-configure-server-side-encryption","text":"Additionally, you can specify if your Bucket has SSE enabled.","title":"Step 4: Configure Server Side Encryption"},{"location":"user_guides/fs/storage_connector/creation/s3/#aes256","text":"For AES256, there is nothing to do but enabling the encryption by toggling the AES256 option. This is using S3-Managed Keys, also called SSE-S3 .","title":"AES256"},{"location":"user_guides/fs/storage_connector/creation/s3/#sse-kms","text":"With this option the encryption key is managed by AWS KMS , with some additional benefits and charges for using this service. The difference is that you need to provide the resource ARN of the key. If you have SSE-KMS enabled for your bucket, you can find the key ARN in the \"Properties\" section of the bucket details on AWS.","title":"SSE-KMS"},{"location":"user_guides/fs/storage_connector/creation/s3/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created S3 connector.","title":"Next Steps"},{"location":"user_guides/fs/storage_connector/creation/snowflake/","text":"How-To set up a Snowflake Storage Connector # Introduction # Snowflake provides a cloud-based data storage and analytics service, used as a data warehouse in many enterprises. Data warehouses are often the source of raw data for feature engineering pipelines and Snowflake supports scalable feature computation with SQL. However, Snowflake is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Snowflake database. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically. Prerequisites # Before you begin this guide you'll need to retrieve the following information from your Snowflake account and database, the following options are mandatory : Snowflake Connection URL: Consult the documentation of your target snowflake account to determine the correct connection URL. This is usually some form of your Snowflake account identifier . For example: <account_identifier>.snowflakecomputing.com Token-based authentication or password based The Snowflake storage connector supports both username and password authentication as well as token-based authentication. Currently token-based authentication is in beta phase. Users are advised to use username/password and/or create a service account for accessing Snowflake from Hopsworks. Username and Password: Login name for the Snowflake user and password. This is often also referred to as sfUser and sfPassword . Database: The database to use for the session after connecting. Schema: The schema to use for the session after connecting. These are a few additional optional arguments: Role: The role field can be used to specify which Snowflake security role to assume for the session after the connection is established. Application: The application field can also be specified to have better observability in Snowflake with regards to which application is running which query. The application field can be a simple string like \u201cHopsworks\u201d or, for instance, the project name, to track usage and queries from each Hopsworks project. Creation in the UI # Step 1: Set up new storage connector # Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface Step 2: Enter Snowflake Settings # Enter the details for your Snowflake connector. Start by giving it a name and an optional description . Select \"Snowflake\" as connector protocol. Specify the hostname for your account in the following format <account_identifier>.snowflakecomputing.com . Login name for the Snowflake user. Password for the Snowflake user or Token. The database to connect to. The schema to use for the connection to the database. Additional optional arguments. For example, you can point the connector to a specific table in the database only. Optional additional key/value arguments. Click \"Setup storage connector\". Snowflake Connector Creation Form Next Steps # Move on to the usage guide for storage connectors to see how you can use your newly created Snowflake connector.","title":"Snowflake"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#how-to-set-up-a-snowflake-storage-connector","text":"","title":"How-To set up a Snowflake Storage Connector"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#introduction","text":"Snowflake provides a cloud-based data storage and analytics service, used as a data warehouse in many enterprises. Data warehouses are often the source of raw data for feature engineering pipelines and Snowflake supports scalable feature computation with SQL. However, Snowflake is not viable as an online feature store that serves features to models in production, with its columnar database layout its latency is too high compared to OLTP databases or key-value stores. In this guide, you will configure a Storage Connector in Hopsworks to save all the authentication information needed in order to set up a connection to your Snowflake database. When you're finished, you'll be able to query the database using Spark through HSFS APIs. Note Currently, it is only possible to create storage connectors in the Hopsworks UI. You cannot create a storage connector programmatically.","title":"Introduction"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#prerequisites","text":"Before you begin this guide you'll need to retrieve the following information from your Snowflake account and database, the following options are mandatory : Snowflake Connection URL: Consult the documentation of your target snowflake account to determine the correct connection URL. This is usually some form of your Snowflake account identifier . For example: <account_identifier>.snowflakecomputing.com Token-based authentication or password based The Snowflake storage connector supports both username and password authentication as well as token-based authentication. Currently token-based authentication is in beta phase. Users are advised to use username/password and/or create a service account for accessing Snowflake from Hopsworks. Username and Password: Login name for the Snowflake user and password. This is often also referred to as sfUser and sfPassword . Database: The database to use for the session after connecting. Schema: The schema to use for the session after connecting. These are a few additional optional arguments: Role: The role field can be used to specify which Snowflake security role to assume for the session after the connection is established. Application: The application field can also be specified to have better observability in Snowflake with regards to which application is running which query. The application field can be a simple string like \u201cHopsworks\u201d or, for instance, the project name, to track usage and queries from each Hopsworks project.","title":"Prerequisites"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#creation-in-the-ui","text":"","title":"Creation in the UI"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#step-1-set-up-new-storage-connector","text":"Head to the Storage Connector View on Hopsworks (1) and set up a new storage connector (2). The Storage Connector View in the User Interface","title":"Step 1: Set up new storage connector"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#step-2-enter-snowflake-settings","text":"Enter the details for your Snowflake connector. Start by giving it a name and an optional description . Select \"Snowflake\" as connector protocol. Specify the hostname for your account in the following format <account_identifier>.snowflakecomputing.com . Login name for the Snowflake user. Password for the Snowflake user or Token. The database to connect to. The schema to use for the connection to the database. Additional optional arguments. For example, you can point the connector to a specific table in the database only. Optional additional key/value arguments. Click \"Setup storage connector\". Snowflake Connector Creation Form","title":"Step 2: Enter Snowflake Settings"},{"location":"user_guides/fs/storage_connector/creation/snowflake/#next-steps","text":"Move on to the usage guide for storage connectors to see how you can use your newly created Snowflake connector.","title":"Next Steps"},{"location":"user_guides/integrations/dummy/","text":"","title":"Dummy"},{"location":"user_guides/mlops/","text":"Model Registry & Serving Guides # This section serves to provide guides and examples for the common usage of abstractions and functionality of Models and Deployments through the Hopsworks UI and APIs. Model Registry Model Serving Vector Database","title":"Model Registry & Serving Guides"},{"location":"user_guides/mlops/#model-registry-serving-guides","text":"This section serves to provide guides and examples for the common usage of abstractions and functionality of Models and Deployments through the Hopsworks UI and APIs. Model Registry Model Serving Vector Database","title":"Model Registry &amp; Serving Guides"},{"location":"user_guides/mlops/registry/","text":"Model Registry Guides # Hopsworks Model Registry is a centralized repository, within an organization, to manage machine learning models. A model is the product of training a machine learning algorithm with training data. This section provides guides for creating models and publish them to the Model Registry to make them available to make them available for download for batch predictions, or deployed to serve realtime applications. Exporting a model # Follow these framework-specific guides to export a Model to the Model Registry. TensorFlow Scikit-learn Other frameworks Model Schema # A Model schema describes the input and outputs for a model. It provides a functional description of the model which makes it simpler to get started working with it. For example if the model inputs a tensor, the model schema can define the shape and data type of the tensor. Input Example # An Input example provides an instance of a valid model input. Input examples are stored with the model as separate artifacts.","title":"Model Registry Guides"},{"location":"user_guides/mlops/registry/#model-registry-guides","text":"Hopsworks Model Registry is a centralized repository, within an organization, to manage machine learning models. A model is the product of training a machine learning algorithm with training data. This section provides guides for creating models and publish them to the Model Registry to make them available to make them available for download for batch predictions, or deployed to serve realtime applications.","title":"Model Registry Guides"},{"location":"user_guides/mlops/registry/#exporting-a-model","text":"Follow these framework-specific guides to export a Model to the Model Registry. TensorFlow Scikit-learn Other frameworks","title":"Exporting a model"},{"location":"user_guides/mlops/registry/#model-schema","text":"A Model schema describes the input and outputs for a model. It provides a functional description of the model which makes it simpler to get started working with it. For example if the model inputs a tensor, the model schema can define the shape and data type of the tensor.","title":"Model Schema"},{"location":"user_guides/mlops/registry/#input-example","text":"An Input example provides an instance of a valid model input. Input examples are stored with the model as separate artifacts.","title":"Input Example"},{"location":"user_guides/mlops/registry/input_example/","text":"How To Attach An Input Example # Introduction # In this guide you will learn how to attach an input example to a model. An input example is simply an instance of a valid model input. Attaching an input example to your model will give other users a better understanding of what data it expects. Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Generate an input example # Generate an input example which corresponds to a valid input to your model. Currently we support pandas.DataFrame, pandas.Series, numpy.ndarray, list to be passed as input example. import numpy as np input_example = np . random . randint ( 0 , high = 256 , size = 784 , dtype = np . uint8 ) Step 3: Set input_example parameter # Set the input_example parameter in the create_model function and call save() to attaching it to the model and register it in the registry. model = mr . tensorflow . create_model ( name = \"mnist\" , input_example = input_example ) model . save ( \"./model\" ) Conclusion # In this guide you learned how to attach an input example to your model.","title":"Input Example"},{"location":"user_guides/mlops/registry/input_example/#how-to-attach-an-input-example","text":"","title":"How To Attach An Input Example"},{"location":"user_guides/mlops/registry/input_example/#introduction","text":"In this guide you will learn how to attach an input example to a model. An input example is simply an instance of a valid model input. Attaching an input example to your model will give other users a better understanding of what data it expects.","title":"Introduction"},{"location":"user_guides/mlops/registry/input_example/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/input_example/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/input_example/#step-2-generate-an-input-example","text":"Generate an input example which corresponds to a valid input to your model. Currently we support pandas.DataFrame, pandas.Series, numpy.ndarray, list to be passed as input example. import numpy as np input_example = np . random . randint ( 0 , high = 256 , size = 784 , dtype = np . uint8 )","title":"Step 2: Generate an input example"},{"location":"user_guides/mlops/registry/input_example/#step-3-set-input_example-parameter","text":"Set the input_example parameter in the create_model function and call save() to attaching it to the model and register it in the registry. model = mr . tensorflow . create_model ( name = \"mnist\" , input_example = input_example ) model . save ( \"./model\" )","title":"Step 3: Set input_example parameter"},{"location":"user_guides/mlops/registry/input_example/#conclusion","text":"In this guide you learned how to attach an input example to your model.","title":"Conclusion"},{"location":"user_guides/mlops/registry/model_schema/","text":"How To Attach A Model Schema # Introduction # In this guide you will learn how to attach a model schema to your model. A model schema, describes the type and shape of inputs and outputs (predictions) for your model. Attaching a model schema to your model will give other users a better understanding of what data it expects. Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Create ModelSchema # Create a ModelSchema for your inputs and outputs by passing in an example that your model is trained on and a valid prediction. Currently, we support pandas.DataFrame, pandas.Series, numpy.ndarray, list . # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs for MNIST dataset inputs = [{ 'type' : 'uint8' , 'shape' : [ 28 , 28 , 1 ], 'description' : 'grayscale representation of 28x28 MNIST images' }] # Build the input schema input_schema = Schema ( inputs ) # Model outputs outputs = [{ 'type' : 'float32' , 'shape' : [ 10 ]}] # Build the output schema output_schema = Schema ( outputs ) # Create ModelSchema object model_schema = ModelSchema ( input_schema = input_schema , output_schema = output_schema ) Step 3: Set model_schema parameter # Set the model_schema parameter in the create_model function and call save() to attaching it to the model and register it in the registry. model = mr . tensorflow . create_model ( name = \"mnist\" , model_schema = model_schema ) model . save ( \"./model\" ) Conclusion # In this guide you learned how to attach an input example to your model.","title":"Model Schema"},{"location":"user_guides/mlops/registry/model_schema/#how-to-attach-a-model-schema","text":"","title":"How To Attach A Model Schema"},{"location":"user_guides/mlops/registry/model_schema/#introduction","text":"In this guide you will learn how to attach a model schema to your model. A model schema, describes the type and shape of inputs and outputs (predictions) for your model. Attaching a model schema to your model will give other users a better understanding of what data it expects.","title":"Introduction"},{"location":"user_guides/mlops/registry/model_schema/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/model_schema/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/model_schema/#step-2-create-modelschema","text":"Create a ModelSchema for your inputs and outputs by passing in an example that your model is trained on and a valid prediction. Currently, we support pandas.DataFrame, pandas.Series, numpy.ndarray, list . # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs for MNIST dataset inputs = [{ 'type' : 'uint8' , 'shape' : [ 28 , 28 , 1 ], 'description' : 'grayscale representation of 28x28 MNIST images' }] # Build the input schema input_schema = Schema ( inputs ) # Model outputs outputs = [{ 'type' : 'float32' , 'shape' : [ 10 ]}] # Build the output schema output_schema = Schema ( outputs ) # Create ModelSchema object model_schema = ModelSchema ( input_schema = input_schema , output_schema = output_schema )","title":"Step 2: Create ModelSchema"},{"location":"user_guides/mlops/registry/model_schema/#step-3-set-model_schema-parameter","text":"Set the model_schema parameter in the create_model function and call save() to attaching it to the model and register it in the registry. model = mr . tensorflow . create_model ( name = \"mnist\" , model_schema = model_schema ) model . save ( \"./model\" )","title":"Step 3: Set model_schema parameter"},{"location":"user_guides/mlops/registry/model_schema/#conclusion","text":"In this guide you learned how to attach an input example to your model.","title":"Conclusion"},{"location":"user_guides/mlops/registry/frameworks/python/","text":"How To Export a Python Model # Introduction # In this guide you will learn how to export a generic Python model and register it in the Model Registry. Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Train # Define your XGBoost model and run the training loop. # Define a model model = XGBClassifier () # Train model model . fit ( X_train , y_train ) Step 3: Export to local path # Export the XGBoost model to a directory on the local filesystem. model_file = \"model.json\" model . save_model ( model_file ) Step 4: Register model in registry # Use the ModelRegistry.python.create_model(..) function to register a model as a Python model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } py_model = mr . python . create_model ( \"py_model\" , metrics = metrics ) py_model . save ( model_dir ) Conclusion # In this guide you learned how to export a Python model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Python"},{"location":"user_guides/mlops/registry/frameworks/python/#how-to-export-a-python-model","text":"","title":"How To Export a Python Model"},{"location":"user_guides/mlops/registry/frameworks/python/#introduction","text":"In this guide you will learn how to export a generic Python model and register it in the Model Registry.","title":"Introduction"},{"location":"user_guides/mlops/registry/frameworks/python/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/frameworks/python/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/frameworks/python/#step-2-train","text":"Define your XGBoost model and run the training loop. # Define a model model = XGBClassifier () # Train model model . fit ( X_train , y_train )","title":"Step 2: Train"},{"location":"user_guides/mlops/registry/frameworks/python/#step-3-export-to-local-path","text":"Export the XGBoost model to a directory on the local filesystem. model_file = \"model.json\" model . save_model ( model_file )","title":"Step 3: Export to local path"},{"location":"user_guides/mlops/registry/frameworks/python/#step-4-register-model-in-registry","text":"Use the ModelRegistry.python.create_model(..) function to register a model as a Python model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } py_model = mr . python . create_model ( \"py_model\" , metrics = metrics ) py_model . save ( model_dir )","title":"Step 4: Register model in registry"},{"location":"user_guides/mlops/registry/frameworks/python/#conclusion","text":"In this guide you learned how to export a Python model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Conclusion"},{"location":"user_guides/mlops/registry/frameworks/skl/","text":"How To Export a Scikit-learn Model # Introduction # In this guide you will learn how to export a Scikit-learn model and register it in the Model Registry. Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Train # Define your Scikit-learn model and run the training loop. # Define a model iris_knn = KNeighborsClassifier ( .. ) iris_knn . fit ( .. ) Step 3: Export to local path # Export the Scikit-learn model to a directory on the local filesystem. model_file = \"skl_knn.pkl\" joblib . dump ( iris_knn , model_file ) Step 4: Register model in registry # Use the ModelRegistry.sklearn.create_model(..) function to register a model as a Scikit-learn model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } skl_model = mr . sklearn . create_model ( \"skl_model\" , metrics = metrics ) skl_model . save ( model_file ) Conclusion # In this guide you learned how to export a Scikit-learn model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Scikit-learn"},{"location":"user_guides/mlops/registry/frameworks/skl/#how-to-export-a-scikit-learn-model","text":"","title":"How To Export a Scikit-learn Model"},{"location":"user_guides/mlops/registry/frameworks/skl/#introduction","text":"In this guide you will learn how to export a Scikit-learn model and register it in the Model Registry.","title":"Introduction"},{"location":"user_guides/mlops/registry/frameworks/skl/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-2-train","text":"Define your Scikit-learn model and run the training loop. # Define a model iris_knn = KNeighborsClassifier ( .. ) iris_knn . fit ( .. )","title":"Step 2: Train"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-3-export-to-local-path","text":"Export the Scikit-learn model to a directory on the local filesystem. model_file = \"skl_knn.pkl\" joblib . dump ( iris_knn , model_file )","title":"Step 3: Export to local path"},{"location":"user_guides/mlops/registry/frameworks/skl/#step-4-register-model-in-registry","text":"Use the ModelRegistry.sklearn.create_model(..) function to register a model as a Scikit-learn model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } skl_model = mr . sklearn . create_model ( \"skl_model\" , metrics = metrics ) skl_model . save ( model_file )","title":"Step 4: Register model in registry"},{"location":"user_guides/mlops/registry/frameworks/skl/#conclusion","text":"In this guide you learned how to export a Scikit-learn model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Conclusion"},{"location":"user_guides/mlops/registry/frameworks/tf/","text":"How To Export a TensorFlow Model # Introduction # In this guide you will learn how to export a TensorFlow model and register it in the Model Registry. Save in SavedModel format Make sure the model is saved in the SavedModel format to be able to deploy it on TensorFlow Serving. Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Train # Define your TensorFlow model and run the training loop. # Define a model model = tf . keras . Sequential () # Add layers model . add ( .. ) # Compile the model. model . compile ( .. ) # Train the model model . fit ( .. ) Step 3: Export to local path # Export the TensorFlow model to a directory on the local filesystem. model_dir = \"./model\" tf . saved_model . save ( model , model_dir ) Step 4: Register model in registry # Use the ModelRegistry.tensorflow.create_model(..) function to register a model as a TensorFlow model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } tf_model = mr . tensorflow . create_model ( \"tf_model\" , metrics = metrics ) tf_model . save ( model_dir ) Conclusion # In this guide you learned how to export a TensorFlow model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"TensorFlow"},{"location":"user_guides/mlops/registry/frameworks/tf/#how-to-export-a-tensorflow-model","text":"","title":"How To Export a TensorFlow Model"},{"location":"user_guides/mlops/registry/frameworks/tf/#introduction","text":"In this guide you will learn how to export a TensorFlow model and register it in the Model Registry. Save in SavedModel format Make sure the model is saved in the SavedModel format to be able to deploy it on TensorFlow Serving.","title":"Introduction"},{"location":"user_guides/mlops/registry/frameworks/tf/#code","text":"","title":"Code"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-2-train","text":"Define your TensorFlow model and run the training loop. # Define a model model = tf . keras . Sequential () # Add layers model . add ( .. ) # Compile the model. model . compile ( .. ) # Train the model model . fit ( .. )","title":"Step 2: Train"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-3-export-to-local-path","text":"Export the TensorFlow model to a directory on the local filesystem. model_dir = \"./model\" tf . saved_model . save ( model , model_dir )","title":"Step 3: Export to local path"},{"location":"user_guides/mlops/registry/frameworks/tf/#step-4-register-model-in-registry","text":"Use the ModelRegistry.tensorflow.create_model(..) function to register a model as a TensorFlow model. Define a name, and attach optional metrics for your model, then invoke the save() function with the parameter being the path to the local directory where the model was exported to. # Model evaluation metrics metrics = { 'accuracy' : 0.92 } tf_model = mr . tensorflow . create_model ( \"tf_model\" , metrics = metrics ) tf_model . save ( model_dir )","title":"Step 4: Register model in registry"},{"location":"user_guides/mlops/registry/frameworks/tf/#conclusion","text":"In this guide you learned how to export a TensorFlow model to the Model Registry. You can also try attaching an Input Example and a Model Schema to your model to document the shape and type of the data the model was trained on.","title":"Conclusion"},{"location":"user_guides/mlops/serving/","text":"Model Serving Guide # Deployment # Assuming you have already created a model in the Model Registry, a deployment can now be created to prepare a model artifact for this model and make it accessible for running predictions behind a REST endpoint. Follow the listed guides to create a Deployment for your model. Predictor # Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions, see the Predictor Guide . Transformer # Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model, see the Transformer Guide . Inference Batcher # Configure the predictor to batch inference requests, see the Inference Batcher Guide . Inference Logger # Configure the predictor to log inference requests and predictions, see the Inference Logger Guide .","title":"Model Serving Guide"},{"location":"user_guides/mlops/serving/#model-serving-guide","text":"","title":"Model Serving Guide"},{"location":"user_guides/mlops/serving/#deployment","text":"Assuming you have already created a model in the Model Registry, a deployment can now be created to prepare a model artifact for this model and make it accessible for running predictions behind a REST endpoint. Follow the listed guides to create a Deployment for your model.","title":"Deployment"},{"location":"user_guides/mlops/serving/#predictor","text":"Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions, see the Predictor Guide .","title":"Predictor"},{"location":"user_guides/mlops/serving/#transformer","text":"Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model, see the Transformer Guide .","title":"Transformer"},{"location":"user_guides/mlops/serving/#inference-batcher","text":"Configure the predictor to batch inference requests, see the Inference Batcher Guide .","title":"Inference Batcher"},{"location":"user_guides/mlops/serving/#inference-logger","text":"Configure the predictor to log inference requests and predictions, see the Inference Logger Guide .","title":"Inference Logger"},{"location":"user_guides/mlops/serving/custom-predictor/","text":"To configure a custom predictor, users must provide a python script implementing the following class. Python class Predict ( object ): def __init__ ( self ): \"\"\" Initialization code goes here: - Download the model artifact - Load the model \"\"\" pass def predict ( self , inputs ): \"\"\" Serve predictions using the trained model\"\"\" pass The predictor script should be available via a local file system path or a path on HopsFS. The path to this script then has to be provided when calling deploy() or create_predictor() methods.","title":"Custom predictor"},{"location":"user_guides/mlops/serving/deployment/","text":"How To Create A Deployment # Introduction # In this guide, you will learn how to create a new deployment for a trained model. Warning This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide Deployments are used to unify the different components involved in making one or more trained models online and accessible to compute predictions on demand. In each deployment, there are three main components to consider: Model artifact Predictor Transformer GUI # Step 1: Create a deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form. Step 2: Simplified deployment form # A simplified creation form will appear including the most common deployment fields among all the configuration possible. We provide default values for the rest of the fields, adjusted to the type of deployment you want to create. In the simplified form, select the model framework used to train your model (a) . Then, select the model you want to deploy from the list of available models in the Model Registry (b) . After selecting the model, the rest of fields are filled automatically. We pick the last model version (c) and model artifact version (d) available in the Model Registry. Moreover, we infer the deployment name (e) from the name given to the model. Deployment name validation rules A valid deployment name can only contain characters a-z, A-Z and 0-9. Predictor script for Python models For Python models, you can select a custom predictor script to load and run the trained model by clicking on Select predictor (f) . If you prefer, change the name of the deployment, model version or artifact version . Then, click on Create deployment to create the deployment for your model. CHANGE IMAGE List of created API Keys Step 3 (Optional): Advanced deployment form # Optionally, you can access and adjust other parameters of the deployment configuration by clicking on Advanced configuration . CHANGE IMAGE List of created API Keys You will be redirected to a full-page deployment creation form where you can see all the default configuration values we selected for your deployment and adjust them according to your use case. Apart from the aforementioned simplified configuration, in this form you can setup the following components: Deployment advanced configuration Predictor Transformer Inference logger Inference batcher Resources CHANGE IMAGE List of created API Keys Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model. Step 4: Deployment creation # Wait for the deployment creation process to finish. CHANGE IMAGE List of created API Keys Step 5: Deployment overview # Once the deployment is created, you will be redirected to the list of all your existing deployments in the project. You can use the filters on the top of the page to easily locate your new deployment. CHANGE IMAGE List of created API Keys After that, click on the new deployment to access the overview page. CHANGE IMAGE List of created API Keys Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () Step 2: Create deployment # Retrieve the trained model you want to deploy. my_model = mr . get_model ( \"my_model\" , version = 1 ) Option A: Using the model object # my_deployment = my_model . deploy () Option B: Using the Model Serving handle # # get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () API Reference # Model Serving Model Artifact # A model artifact is a package containing all of the necessary files for the deployment of a model. It includes the model file(s) and/or custom scripts for loading the model (predictor script) or transforming the model inputs at inference time (the transformer script). When a new deployment is created, a model artifact is generated in two cases: the artifact version in the predictor is set to CREATE (see Artifact Version ) no model artifact with the same files has been created before. Predictor # Predictors are responsible for running the model server that loads the trained model, listens to inference requests and returns prediction results. To learn more about predictors, see the Predictor Guide Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon. Info Model artifacts are assigned an incremental version number, being 0 the version reserved for model artifacts that do not contain predictor or transformer scripts (i.e., shared artifacts containing only the model files). Transformer # Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Warning Transformers are only supported in KServe deployments. Inference logger # Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide Inference batcher # Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide . Resources # Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide . Conclusion # In this guide you learned how to create a deployment.","title":"How To Create A Deployment"},{"location":"user_guides/mlops/serving/deployment/#how-to-create-a-deployment","text":"","title":"How To Create A Deployment"},{"location":"user_guides/mlops/serving/deployment/#introduction","text":"In this guide, you will learn how to create a new deployment for a trained model. Warning This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide Deployments are used to unify the different components involved in making one or more trained models online and accessible to compute predictions on demand. In each deployment, there are three main components to consider: Model artifact Predictor Transformer","title":"Introduction"},{"location":"user_guides/mlops/serving/deployment/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/deployment/#step-1-create-a-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form.","title":"Step 1: Create a deployment"},{"location":"user_guides/mlops/serving/deployment/#step-2-simplified-deployment-form","text":"A simplified creation form will appear including the most common deployment fields among all the configuration possible. We provide default values for the rest of the fields, adjusted to the type of deployment you want to create. In the simplified form, select the model framework used to train your model (a) . Then, select the model you want to deploy from the list of available models in the Model Registry (b) . After selecting the model, the rest of fields are filled automatically. We pick the last model version (c) and model artifact version (d) available in the Model Registry. Moreover, we infer the deployment name (e) from the name given to the model. Deployment name validation rules A valid deployment name can only contain characters a-z, A-Z and 0-9. Predictor script for Python models For Python models, you can select a custom predictor script to load and run the trained model by clicking on Select predictor (f) . If you prefer, change the name of the deployment, model version or artifact version . Then, click on Create deployment to create the deployment for your model. CHANGE IMAGE List of created API Keys","title":"Step 2: Simplified deployment form"},{"location":"user_guides/mlops/serving/deployment/#step-3-optional-advanced-deployment-form","text":"Optionally, you can access and adjust other parameters of the deployment configuration by clicking on Advanced configuration . CHANGE IMAGE List of created API Keys You will be redirected to a full-page deployment creation form where you can see all the default configuration values we selected for your deployment and adjust them according to your use case. Apart from the aforementioned simplified configuration, in this form you can setup the following components: Deployment advanced configuration Predictor Transformer Inference logger Inference batcher Resources CHANGE IMAGE List of created API Keys Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model.","title":"Step 3 (Optional): Advanced deployment form"},{"location":"user_guides/mlops/serving/deployment/#step-4-deployment-creation","text":"Wait for the deployment creation process to finish. CHANGE IMAGE List of created API Keys","title":"Step 4: Deployment creation"},{"location":"user_guides/mlops/serving/deployment/#step-5-deployment-overview","text":"Once the deployment is created, you will be redirected to the list of all your existing deployments in the project. You can use the filters on the top of the page to easily locate your new deployment. CHANGE IMAGE List of created API Keys After that, click on the new deployment to access the overview page. CHANGE IMAGE List of created API Keys","title":"Step 5: Deployment overview"},{"location":"user_guides/mlops/serving/deployment/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/deployment/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/deployment/#step-2-create-deployment","text":"Retrieve the trained model you want to deploy. my_model = mr . get_model ( \"my_model\" , version = 1 )","title":"Step 2: Create deployment"},{"location":"user_guides/mlops/serving/deployment/#option-a-using-the-model-object","text":"my_deployment = my_model . deploy ()","title":"Option A: Using the model object"},{"location":"user_guides/mlops/serving/deployment/#option-b-using-the-model-serving-handle","text":"# get Hopsworks Model Serving handle ms = project . get_model_serving () my_predictor = ms . create_predictor ( my_model ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save ()","title":"Option B: Using the Model Serving handle"},{"location":"user_guides/mlops/serving/deployment/#api-reference","text":"Model Serving","title":"API Reference"},{"location":"user_guides/mlops/serving/deployment/#model-artifact","text":"A model artifact is a package containing all of the necessary files for the deployment of a model. It includes the model file(s) and/or custom scripts for loading the model (predictor script) or transforming the model inputs at inference time (the transformer script). When a new deployment is created, a model artifact is generated in two cases: the artifact version in the predictor is set to CREATE (see Artifact Version ) no model artifact with the same files has been created before.","title":"Model Artifact"},{"location":"user_guides/mlops/serving/deployment/#predictor","text":"Predictors are responsible for running the model server that loads the trained model, listens to inference requests and returns prediction results. To learn more about predictors, see the Predictor Guide Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon. Info Model artifacts are assigned an incremental version number, being 0 the version reserved for model artifacts that do not contain predictor or transformer scripts (i.e., shared artifacts containing only the model files).","title":"Predictor"},{"location":"user_guides/mlops/serving/deployment/#transformer","text":"Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Warning Transformers are only supported in KServe deployments.","title":"Transformer"},{"location":"user_guides/mlops/serving/deployment/#inference-logger","text":"Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide","title":"Inference logger"},{"location":"user_guides/mlops/serving/deployment/#inference-batcher","text":"Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide .","title":"Inference batcher"},{"location":"user_guides/mlops/serving/deployment/#resources","text":"Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide .","title":"Resources"},{"location":"user_guides/mlops/serving/deployment/#conclusion","text":"In this guide you learned how to create a deployment.","title":"Conclusion"},{"location":"user_guides/mlops/serving/inference-batcher/","text":"How To Configure Inference Batcher # Introduction # Inference batching can be enabled to increase inference request throughput at the cost of higher latencies. The configuration of the inference batcher depends on the serving tool and the model server used in the deployment. See the compatibility matrix . GUI # Step 1: Create a deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form. Step 2: Simplified deployment form # A simplified creation form will appear including the most common deployment fields among all the configuration possible. Inference batching is part of the advanced configuration of a deployment. To navigate to the advanced creation form, click on Advanced configuration . CHANGE IMAGE List of created API Keys Step 3: Advanced deployment form # To enable inference batching, click on the Request batching checkbox. CHANGE IMAGE List of created API Keys If your deployment uses KServe, you can optionally set three additional parameters for the inference batcher: maximum batch size, maximum latency (ms) and timeout (s). CHANGE IMAGE List of created API Keys Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model. CODE # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2: Define an inference logger # from hsml.inference_batcher import InferenceBatcher my_batcher = InferenceBatcher ( enabled = True , # optional max_batch_size = 32 , max_latency = 5000 , # milliseconds timeout = 5 # seconds ) Step 3: Create a deployment with the inference batcher # my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , inference_batcher = my_batcher ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () API Reference # Inference Batcher Compatibility matrix # Show supported inference batcher configuration Serving tool Model server Inference batching Fine-grained configuration Docker Flask \u274c - TensorFlow Serving \u2705 \u274c Kubernetes Flask \u274c - TensorFlow Serving \u2705 \u274c KServe Flask \u2705 \u2705 TensorFlow Serving \u2705 \u2705","title":"Inference Batcher"},{"location":"user_guides/mlops/serving/inference-batcher/#how-to-configure-inference-batcher","text":"","title":"How To Configure Inference Batcher"},{"location":"user_guides/mlops/serving/inference-batcher/#introduction","text":"Inference batching can be enabled to increase inference request throughput at the cost of higher latencies. The configuration of the inference batcher depends on the serving tool and the model server used in the deployment. See the compatibility matrix .","title":"Introduction"},{"location":"user_guides/mlops/serving/inference-batcher/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/inference-batcher/#step-1-create-a-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form.","title":"Step 1: Create a deployment"},{"location":"user_guides/mlops/serving/inference-batcher/#step-2-simplified-deployment-form","text":"A simplified creation form will appear including the most common deployment fields among all the configuration possible. Inference batching is part of the advanced configuration of a deployment. To navigate to the advanced creation form, click on Advanced configuration . CHANGE IMAGE List of created API Keys","title":"Step 2: Simplified deployment form"},{"location":"user_guides/mlops/serving/inference-batcher/#step-3-advanced-deployment-form","text":"To enable inference batching, click on the Request batching checkbox. CHANGE IMAGE List of created API Keys If your deployment uses KServe, you can optionally set three additional parameters for the inference batcher: maximum batch size, maximum latency (ms) and timeout (s). CHANGE IMAGE List of created API Keys Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model.","title":"Step 3: Advanced deployment form"},{"location":"user_guides/mlops/serving/inference-batcher/#code","text":"","title":"CODE"},{"location":"user_guides/mlops/serving/inference-batcher/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/inference-batcher/#step-2-define-an-inference-logger","text":"from hsml.inference_batcher import InferenceBatcher my_batcher = InferenceBatcher ( enabled = True , # optional max_batch_size = 32 , max_latency = 5000 , # milliseconds timeout = 5 # seconds )","title":"Step 2: Define an inference logger"},{"location":"user_guides/mlops/serving/inference-batcher/#step-3-create-a-deployment-with-the-inference-batcher","text":"my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , inference_batcher = my_batcher ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save ()","title":"Step 3: Create a deployment with the inference batcher"},{"location":"user_guides/mlops/serving/inference-batcher/#api-reference","text":"Inference Batcher","title":"API Reference"},{"location":"user_guides/mlops/serving/inference-batcher/#compatibility-matrix","text":"Show supported inference batcher configuration Serving tool Model server Inference batching Fine-grained configuration Docker Flask \u274c - TensorFlow Serving \u2705 \u274c Kubernetes Flask \u274c - TensorFlow Serving \u2705 \u274c KServe Flask \u2705 \u2705 TensorFlow Serving \u2705 \u2705","title":"Compatibility matrix"},{"location":"user_guides/mlops/serving/inference-logger/","text":"How To Configure Inference Logging # Introduction # Once a model is deployed and starts making predictions as inference requests arrive, logging model inputs and predictions becomes essential to monitor the health of the model and take action if the model's performance degrades over time. Hopsworks supports logging both inference requests and predictions as events to a Kafka topic for analysis. Topic schemas vary depending on the serving tool. See below GUI # Step 1: Create a deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form. Step 2: Simplified deployment form # A simplified creation form will appear including the most common deployment fields among all the configuration possible. Inference logging is part of the advanced configuration of a deployment. To navigate to the advanced creation form, click on Advanced configuration . CHANGE IMAGE List of created API Keys Step 3: Advanced deployment form # To enable inference logging, choose \u00b4CREATE\u00b4 as Kafka topic name to create a new topic, or select an existing topic. If you prefer, you can disable inference logging by selecting NONE . If you decide to create a new topic, select the number of partitions and number of replicas for your topic, or use the default values. If the deployment is created with KServe enabled, you can specify which inference logs you want to send to the Kafka topic (i.e., MODEL_INPUTS , PREDICTIONS or both) CHANGE IMAGE List of created API Keys Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model. CODE # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2: Define an inference logger # from hsml.inference_logger import InferenceLogger from hsml.kafka_topic import KafkaTopic new_topic = KafkaTopic ( name = \"CREATE\" , # optional num_partitions = 1 , num_replicas = 1 ) my_logger = InferenceLogger ( kafka_topic = new_topic , mode = \"ALL\" ) Use dict for simpler code Similarly, you can create the same logger with: my_logger = InferenceLogger ( kafka_topic = { \"name\" : \"CREATE\" }, mode = \"ALL\" ) Step 3: Create a deployment with the inference logger # my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , inference_logger = my_logger ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () API Reference # Inference Logger Topic schema # The schema of Kafka events varies depending on the serving tool. In KServe deployments, model inputs and predictions are logged in separate events, but sharing the same requestId field. In non-KServe deployments, the same event contains both the model input and prediction related to the same inference request. Show kafka topic schemas KServe Docker / Kubernetes { \"fields\" : [ { \"name\" : \"servingId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceId\" , \"type\" : \"string\" }, { \"name\" : \"messageType\" , \"type\" : \"string\" }, { \"name\" : \"payload\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" } { \"fields\" : [ { \"name\" : \"modelId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceRequest\" , \"type\" : \"string\" }, { \"name\" : \"inferenceResponse\" , \"type\" : \"string\" }, { \"name\" : \"modelServer\" , \"type\" : \"string\" }, { \"name\" : \"servingTool\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" }","title":"Inference Logger"},{"location":"user_guides/mlops/serving/inference-logger/#how-to-configure-inference-logging","text":"","title":"How To Configure Inference Logging"},{"location":"user_guides/mlops/serving/inference-logger/#introduction","text":"Once a model is deployed and starts making predictions as inference requests arrive, logging model inputs and predictions becomes essential to monitor the health of the model and take action if the model's performance degrades over time. Hopsworks supports logging both inference requests and predictions as events to a Kafka topic for analysis. Topic schemas vary depending on the serving tool. See below","title":"Introduction"},{"location":"user_guides/mlops/serving/inference-logger/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/inference-logger/#step-1-create-a-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form.","title":"Step 1: Create a deployment"},{"location":"user_guides/mlops/serving/inference-logger/#step-2-simplified-deployment-form","text":"A simplified creation form will appear including the most common deployment fields among all the configuration possible. Inference logging is part of the advanced configuration of a deployment. To navigate to the advanced creation form, click on Advanced configuration . CHANGE IMAGE List of created API Keys","title":"Step 2: Simplified deployment form"},{"location":"user_guides/mlops/serving/inference-logger/#step-3-advanced-deployment-form","text":"To enable inference logging, choose \u00b4CREATE\u00b4 as Kafka topic name to create a new topic, or select an existing topic. If you prefer, you can disable inference logging by selecting NONE . If you decide to create a new topic, select the number of partitions and number of replicas for your topic, or use the default values. If the deployment is created with KServe enabled, you can specify which inference logs you want to send to the Kafka topic (i.e., MODEL_INPUTS , PREDICTIONS or both) CHANGE IMAGE List of created API Keys Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model.","title":"Step 3: Advanced deployment form"},{"location":"user_guides/mlops/serving/inference-logger/#code","text":"","title":"CODE"},{"location":"user_guides/mlops/serving/inference-logger/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/inference-logger/#step-2-define-an-inference-logger","text":"from hsml.inference_logger import InferenceLogger from hsml.kafka_topic import KafkaTopic new_topic = KafkaTopic ( name = \"CREATE\" , # optional num_partitions = 1 , num_replicas = 1 ) my_logger = InferenceLogger ( kafka_topic = new_topic , mode = \"ALL\" ) Use dict for simpler code Similarly, you can create the same logger with: my_logger = InferenceLogger ( kafka_topic = { \"name\" : \"CREATE\" }, mode = \"ALL\" )","title":"Step 2: Define an inference logger"},{"location":"user_guides/mlops/serving/inference-logger/#step-3-create-a-deployment-with-the-inference-logger","text":"my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , inference_logger = my_logger ) my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save ()","title":"Step 3: Create a deployment with the inference logger"},{"location":"user_guides/mlops/serving/inference-logger/#api-reference","text":"Inference Logger","title":"API Reference"},{"location":"user_guides/mlops/serving/inference-logger/#topic-schema","text":"The schema of Kafka events varies depending on the serving tool. In KServe deployments, model inputs and predictions are logged in separate events, but sharing the same requestId field. In non-KServe deployments, the same event contains both the model input and prediction related to the same inference request. Show kafka topic schemas KServe Docker / Kubernetes { \"fields\" : [ { \"name\" : \"servingId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceId\" , \"type\" : \"string\" }, { \"name\" : \"messageType\" , \"type\" : \"string\" }, { \"name\" : \"payload\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" } { \"fields\" : [ { \"name\" : \"modelId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceRequest\" , \"type\" : \"string\" }, { \"name\" : \"inferenceResponse\" , \"type\" : \"string\" }, { \"name\" : \"modelServer\" , \"type\" : \"string\" }, { \"name\" : \"servingTool\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" }","title":"Topic schema"},{"location":"user_guides/mlops/serving/predictor/","text":"How To Configure A Predictor # Introduction # In this guide, you will learn how to configure a predictor for a trained model. Warning This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide Predictors are the main component of deployments. They are responsible for running a model server that loads a trained model, handles inference requests and returns predictions. They can be configured to use different model servers, serving tools, log specific inference data or scale differently. In each predictor, you can configure the following components: Model server Serving tool Custom script Transformer Inference Logger Inference Batcher Resources GUI # Step 1: Create a deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form. Step 2: Simplified deployment form # A simplified creation form will appear, including the most common deployment fields among all the configuration possible. These fields include the model server (a) and custom script (b) (for python models). If you want to use your own predictor script , click on Select file and navigate through the file system to find it, or click on Upload file to upload the predictor script now. CHANGE IMAGE List of created API Keys Other configuration such as the serving tool, is part of the advanced configuration of a deployment. To navigate to the advanced creation form, click on Advanced configuration . Step 3: Advanced deployment form # Here, you change the serving tool for your deployment by enabling or disabling the KServe checkbox. CHANGE IMAGE List of created API Keys Additionally, you can adjust the default values of the rest of components: Predictor components Transformer Inference logger Inference batcher Resources Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model. Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2 (Optional): Implement predictor script # Python class Predict ( object ): def __init__ ( self ): \"\"\" Initialization code goes here: - Download the model artifact - Load the model \"\"\" pass def predict ( self , inputs ): \"\"\" Serve predictions using the trained model\"\"\" pass Jupyter magic In a jupyter notebook, you can add %%writefile my_predictor.py at the top of the cell to save it as a local file. Step 3 (Optional): Upload the script to your project # You can also use the UI to upload your predictor script. See above uploaded_file_path = dataset_api . upload ( \"my_predictor.py\" , \"Resources\" , overwrite = True ) predictor_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path ) Step 4: Define predictor # my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , # optional model_server = \"PYTHON\" , serving_tool = \"KSERVE\" , script_file = predictor_script_path ) Step 3: Create a deployment with the predictor # my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save () API Reference # Predictor Model Server # Hopsworks Model Serving currently supports deploying models with a Flask server for python-based models or TensorFlow Serving for TensorFlow / Keras models. Support for TorchServe for running PyTorch models is coming soon. Today, you can deploy PyTorch models as python-based models. Show supported model servers Model Server Supported ML Frameworks Flask \u2705 python-based (scikit-learn, xgboost, pytorch...) TensorFlow Serving \u2705 keras, tensorflow TorchServe \u274c pytorch Serving tool # In Hopsworks, model servers can be deployed in three different ways: directly on Docker, on Kubernetes deployments or using KServe inference services. Although the same models can be deployed in either of our two serving tools (Python or KServe), the use of KServe is highly recommended. The following is a comparitive table showing the features supported by each of them. Show serving tools comparison Feature / requirement Docker Kubernetes (enterprise) KServe (enterprise) Autoscaling (scale-out) \u274c \u2705 \u2705 Resource allocation \u2796 fixed \u2796 min. resources \u2705 min / max. resources Inference logging \u2796 simple \u2796 simple \u2705 fine-grained Inference batching \u2796 partially \u2796 partially \u2705 Scale-to-zero \u274c \u274c \u2705 after 30s of inactivity) Transformers \u274c \u274c \u2705 Low-latency predictions \u274c \u274c \u2705 Multiple models \u274c \u274c \u2796 (python-based) Custom predictor required (python-only) \u2705 \u2705 \u274c Custom script # Depending on the model server and serving tool used in the deployment, you can provide your own python script to load the model and make predictions. Show supported custom predictors Serving tool Model server Custom predictor script Docker Flask \u2705 (required) TensorFlow Serving \u274c Kubernetes Flask \u2705 (required) TensorFlow Serving \u274c KServe Flask \u2705 (only required for artifacts with multiple models) TensorFlow Serving \u274c Environment variables # A number of different environment variables is available in the predictor to ease its implementation. Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment Transformer # Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Warning Transformers are only supported in KServe deployments. Inference logger # Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide Inference batcher # Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide . Resources # Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide . Conclusion # In this guide you learned how to configure a predictor.","title":"Predictor"},{"location":"user_guides/mlops/serving/predictor/#how-to-configure-a-predictor","text":"","title":"How To Configure A Predictor"},{"location":"user_guides/mlops/serving/predictor/#introduction","text":"In this guide, you will learn how to configure a predictor for a trained model. Warning This guide assumes that a model has already been trained and saved into the Model Registry. To learn how to create a model in the Model Registry, see Model Registry Guide Predictors are the main component of deployments. They are responsible for running a model server that loads a trained model, handles inference requests and returns predictions. They can be configured to use different model servers, serving tools, log specific inference data or scale differently. In each predictor, you can configure the following components: Model server Serving tool Custom script Transformer Inference Logger Inference Batcher Resources","title":"Introduction"},{"location":"user_guides/mlops/serving/predictor/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/predictor/#step-1-create-a-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form.","title":"Step 1: Create a deployment"},{"location":"user_guides/mlops/serving/predictor/#step-2-simplified-deployment-form","text":"A simplified creation form will appear, including the most common deployment fields among all the configuration possible. These fields include the model server (a) and custom script (b) (for python models). If you want to use your own predictor script , click on Select file and navigate through the file system to find it, or click on Upload file to upload the predictor script now. CHANGE IMAGE List of created API Keys Other configuration such as the serving tool, is part of the advanced configuration of a deployment. To navigate to the advanced creation form, click on Advanced configuration .","title":"Step 2: Simplified deployment form"},{"location":"user_guides/mlops/serving/predictor/#step-3-advanced-deployment-form","text":"Here, you change the serving tool for your deployment by enabling or disabling the KServe checkbox. CHANGE IMAGE List of created API Keys Additionally, you can adjust the default values of the rest of components: Predictor components Transformer Inference logger Inference batcher Resources Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model.","title":"Step 3: Advanced deployment form"},{"location":"user_guides/mlops/serving/predictor/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/predictor/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Hopsworks Model Registry handle mr = project . get_model_registry () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/predictor/#step-2-optional-implement-predictor-script","text":"Python class Predict ( object ): def __init__ ( self ): \"\"\" Initialization code goes here: - Download the model artifact - Load the model \"\"\" pass def predict ( self , inputs ): \"\"\" Serve predictions using the trained model\"\"\" pass Jupyter magic In a jupyter notebook, you can add %%writefile my_predictor.py at the top of the cell to save it as a local file.","title":"Step 2 (Optional): Implement predictor script"},{"location":"user_guides/mlops/serving/predictor/#step-3-optional-upload-the-script-to-your-project","text":"You can also use the UI to upload your predictor script. See above uploaded_file_path = dataset_api . upload ( \"my_predictor.py\" , \"Resources\" , overwrite = True ) predictor_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path )","title":"Step 3 (Optional): Upload the script to your project"},{"location":"user_guides/mlops/serving/predictor/#step-4-define-predictor","text":"my_model = mr . get_model ( \"my_model\" , version = 1 ) my_predictor = ms . create_predictor ( my_model , # optional model_server = \"PYTHON\" , serving_tool = \"KSERVE\" , script_file = predictor_script_path )","title":"Step 4: Define predictor"},{"location":"user_guides/mlops/serving/predictor/#step-3-create-a-deployment-with-the-predictor","text":"my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor ) my_deployment . save ()","title":"Step 3: Create a deployment with the predictor"},{"location":"user_guides/mlops/serving/predictor/#api-reference","text":"Predictor","title":"API Reference"},{"location":"user_guides/mlops/serving/predictor/#model-server","text":"Hopsworks Model Serving currently supports deploying models with a Flask server for python-based models or TensorFlow Serving for TensorFlow / Keras models. Support for TorchServe for running PyTorch models is coming soon. Today, you can deploy PyTorch models as python-based models. Show supported model servers Model Server Supported ML Frameworks Flask \u2705 python-based (scikit-learn, xgboost, pytorch...) TensorFlow Serving \u2705 keras, tensorflow TorchServe \u274c pytorch","title":"Model Server"},{"location":"user_guides/mlops/serving/predictor/#serving-tool","text":"In Hopsworks, model servers can be deployed in three different ways: directly on Docker, on Kubernetes deployments or using KServe inference services. Although the same models can be deployed in either of our two serving tools (Python or KServe), the use of KServe is highly recommended. The following is a comparitive table showing the features supported by each of them. Show serving tools comparison Feature / requirement Docker Kubernetes (enterprise) KServe (enterprise) Autoscaling (scale-out) \u274c \u2705 \u2705 Resource allocation \u2796 fixed \u2796 min. resources \u2705 min / max. resources Inference logging \u2796 simple \u2796 simple \u2705 fine-grained Inference batching \u2796 partially \u2796 partially \u2705 Scale-to-zero \u274c \u274c \u2705 after 30s of inactivity) Transformers \u274c \u274c \u2705 Low-latency predictions \u274c \u274c \u2705 Multiple models \u274c \u274c \u2796 (python-based) Custom predictor required (python-only) \u2705 \u2705 \u274c","title":"Serving tool"},{"location":"user_guides/mlops/serving/predictor/#custom-script","text":"Depending on the model server and serving tool used in the deployment, you can provide your own python script to load the model and make predictions. Show supported custom predictors Serving tool Model server Custom predictor script Docker Flask \u2705 (required) TensorFlow Serving \u274c Kubernetes Flask \u2705 (required) TensorFlow Serving \u274c KServe Flask \u2705 (only required for artifacts with multiple models) TensorFlow Serving \u274c","title":"Custom script"},{"location":"user_guides/mlops/serving/predictor/#environment-variables","text":"A number of different environment variables is available in the predictor to ease its implementation. Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment","title":"Environment variables"},{"location":"user_guides/mlops/serving/predictor/#transformer","text":"Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Warning Transformers are only supported in KServe deployments.","title":"Transformer"},{"location":"user_guides/mlops/serving/predictor/#inference-logger","text":"Inference loggers are deployment components that log inference requests into a Kafka topic for later analysis. To learn about the different logging modes, see the Inference Logger Guide","title":"Inference logger"},{"location":"user_guides/mlops/serving/predictor/#inference-batcher","text":"Inference batcher are deployment component that apply batching to the incoming inference requests for a better throughput-latency trade-off. To learn about the different configuration available for the inference batcher, see the Inference Batcher Guide .","title":"Inference batcher"},{"location":"user_guides/mlops/serving/predictor/#resources","text":"Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide .","title":"Resources"},{"location":"user_guides/mlops/serving/predictor/#conclusion","text":"In this guide you learned how to configure a predictor.","title":"Conclusion"},{"location":"user_guides/mlops/serving/resources/","text":"How To Allocate Resources For A Deployment # Introduction # GUI # CODE # API Reference #","title":"How To Allocate Resources For A Deployment"},{"location":"user_guides/mlops/serving/resources/#how-to-allocate-resources-for-a-deployment","text":"","title":"How To Allocate Resources For A Deployment"},{"location":"user_guides/mlops/serving/resources/#introduction","text":"","title":"Introduction"},{"location":"user_guides/mlops/serving/resources/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/resources/#code","text":"","title":"CODE"},{"location":"user_guides/mlops/serving/resources/#api-reference","text":"","title":"API Reference"},{"location":"user_guides/mlops/serving/transformer/","text":"How To Configure A Transformer # Introduction # In this guide, you will learn how to configure a transformer in a deployment. Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. They run on a built-in Flask server provided by Hopsworks and require a custom python script implementing the Transformer class . Warning Transformers are only supported in deployments using KServe as serving tool. A transformer has two configurable components: Custom script Resources See examples of transformer scripts in the serving example notebooks . GUI # Step 1: Create a deployment # If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form. Step 2: Simplified deployment form # A simplified creation form will appear including the most common deployment fields among all the configuration possible. Transformers are part of the advanced configuration of a deployment. To navigate to the advanced creation form, click on Advanced configuration . CHANGE IMAGE List of created API Keys Step 3: Advanced deployment form # Transformers require KServe as the serving platform for the deployment. Make sure that KServe is enabled for this deployment by activating the corresponding checkbox. CHANGE IMAGE List of created API Keys Then, if the transformer script is already located in Hopsworks, click on Select file and navigate through the file system to find your script. Otherwise, you can click on Upload file to upload the transformer script now. CHANGE IMAGE List of created API Keys At the end of the page, you can configure the resources to be allocated for the transformer, as well as the minimum and maximum number of replicas to be deployed. CHANGE IMAGE List of created API Keys Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model. Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Dataset API instance dataset_api = project . get_dataset_api () # get Hopsworks Model Serving handle ms = project . get_model_serving () Step 2: Implement transformer script # Python class Transformer ( object ): def __init__ ( self ): \"\"\" Initialization code goes here \"\"\" pass def preprocess ( self , inputs ): \"\"\" Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. \"\"\" return inputs def postprocess ( self , outputs ): \"\"\" Transform the predictions computed by the model before returning a response \"\"\" return outputs Jupyter magic In a jupyter notebook, you can add %%writefile my_transformer.py at the top of the cell to save it as a local file. Step 3: Upload the script to your project # You can also use the UI to upload your transformer script. See above uploaded_file_path = dataset_api . upload ( \"my_transformer.py\" , \"Resources\" , overwrite = True ) transformer_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path ) Step 4: Define a transformer # my_transformer = ms . create_transformer ( script_file = uploaded_file_path ) # or from hsml.transformer import Transformer my_transformer = Transformer ( script_file ) Step 5: Create a deployment with the transformer # my_predictor = ms . create_predictor ( transformer = my_transformer ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor , transformer = my_transformer ) my_deployment . save () API Reference # Transformer Resources # Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide . Environment variables # A number of different environment variables is available in the transformer to ease its implementation. Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment Conclusion # In this guide you learned how to configure a transformer.","title":"Transformer"},{"location":"user_guides/mlops/serving/transformer/#how-to-configure-a-transformer","text":"","title":"How To Configure A Transformer"},{"location":"user_guides/mlops/serving/transformer/#introduction","text":"In this guide, you will learn how to configure a transformer in a deployment. Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. They run on a built-in Flask server provided by Hopsworks and require a custom python script implementing the Transformer class . Warning Transformers are only supported in deployments using KServe as serving tool. A transformer has two configurable components: Custom script Resources See examples of transformer scripts in the serving example notebooks .","title":"Introduction"},{"location":"user_guides/mlops/serving/transformer/#gui","text":"","title":"GUI"},{"location":"user_guides/mlops/serving/transformer/#step-1-create-a-deployment","text":"If you have at least one model already trained and saved in the Model Registry, navigate to the deployments page by clicking on the Deployments tab on the navigation menu on the left. CHANGE IMAGE List of created API Keys Once in the deployments page, click on Create new deployment on the top-right corner to open the deployment creation form.","title":"Step 1: Create a deployment"},{"location":"user_guides/mlops/serving/transformer/#step-2-simplified-deployment-form","text":"A simplified creation form will appear including the most common deployment fields among all the configuration possible. Transformers are part of the advanced configuration of a deployment. To navigate to the advanced creation form, click on Advanced configuration . CHANGE IMAGE List of created API Keys","title":"Step 2: Simplified deployment form"},{"location":"user_guides/mlops/serving/transformer/#step-3-advanced-deployment-form","text":"Transformers require KServe as the serving platform for the deployment. Make sure that KServe is enabled for this deployment by activating the corresponding checkbox. CHANGE IMAGE List of created API Keys Then, if the transformer script is already located in Hopsworks, click on Select file and navigate through the file system to find your script. Otherwise, you can click on Upload file to upload the transformer script now. CHANGE IMAGE List of created API Keys At the end of the page, you can configure the resources to be allocated for the transformer, as well as the minimum and maximum number of replicas to be deployed. CHANGE IMAGE List of created API Keys Once you are done with the changes, click on Create deployment at the end of the page to create the deployment for your model.","title":"Step 3: Advanced deployment form"},{"location":"user_guides/mlops/serving/transformer/#code","text":"","title":"Code"},{"location":"user_guides/mlops/serving/transformer/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project ( \"my_project\" ) # get Dataset API instance dataset_api = project . get_dataset_api () # get Hopsworks Model Serving handle ms = project . get_model_serving ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/mlops/serving/transformer/#step-2-implement-transformer-script","text":"Python class Transformer ( object ): def __init__ ( self ): \"\"\" Initialization code goes here \"\"\" pass def preprocess ( self , inputs ): \"\"\" Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. \"\"\" return inputs def postprocess ( self , outputs ): \"\"\" Transform the predictions computed by the model before returning a response \"\"\" return outputs Jupyter magic In a jupyter notebook, you can add %%writefile my_transformer.py at the top of the cell to save it as a local file.","title":"Step 2: Implement transformer script"},{"location":"user_guides/mlops/serving/transformer/#step-3-upload-the-script-to-your-project","text":"You can also use the UI to upload your transformer script. See above uploaded_file_path = dataset_api . upload ( \"my_transformer.py\" , \"Resources\" , overwrite = True ) transformer_script_path = os . path . join ( \"/Projects\" , project . name , uploaded_file_path )","title":"Step 3: Upload the script to your project"},{"location":"user_guides/mlops/serving/transformer/#step-4-define-a-transformer","text":"my_transformer = ms . create_transformer ( script_file = uploaded_file_path ) # or from hsml.transformer import Transformer my_transformer = Transformer ( script_file )","title":"Step 4: Define a transformer"},{"location":"user_guides/mlops/serving/transformer/#step-5-create-a-deployment-with-the-transformer","text":"my_predictor = ms . create_predictor ( transformer = my_transformer ) my_deployment = my_predictor . deploy () # or my_deployment = ms . create_deployment ( my_predictor , transformer = my_transformer ) my_deployment . save ()","title":"Step 5: Create a deployment with the transformer"},{"location":"user_guides/mlops/serving/transformer/#api-reference","text":"Transformer","title":"API Reference"},{"location":"user_guides/mlops/serving/transformer/#resources","text":"Resources include the number of replicas for the deployment as well as the resources (i.e., memory, CPU, GPU) to be allocated per replica. To learn about the different combinations available, see the Resources Guide .","title":"Resources"},{"location":"user_guides/mlops/serving/transformer/#environment-variables","text":"A number of different environment variables is available in the transformer to ease its implementation. Show environment variables Name Description ARTIFACT_FILES_PATH Local path to the model artifact files DEPLOYMENT_NAME Name of the current deployment MODEL_NAME Name of the model being served by the current deployment MODEL_VERSION Version of the model being served by the current deployment ARTIFACT_VERSION Version of the model artifact being served by the current deployment","title":"Environment variables"},{"location":"user_guides/mlops/serving/transformer/#conclusion","text":"In this guide you learned how to configure a transformer.","title":"Conclusion"},{"location":"user_guides/mlops/vector_database/","text":"How To Use OpenSearch k-NN plugin # Introduction # The k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points. Use cases include recommendations (for example, an \u201cother songs you might like\u201d feature in a music application), image recognition, and fraud detection. Limited to internal Jobs and Notebooks Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster. Code # In this guide, you will learn how to create a simple recommendation application, using the k-NN plugin in OpenSearch. Step 1: Get the OpenSearch API # import hopsworks connection = hopsworks . connection () project = connection . get_project () opensearch_api = project . get_opensearch_api () Step 2: Configure the opensearch-py client # from opensearchpy import OpenSearch client = OpenSearch ( ** opensearch_api . get_default_py_config ()) Step 3: Create an index # Create an index to use by calling opensearch_api.get_project_index(..) . knn_index_name = opensearch_api . get_project_index ( \"demo_knn_index\" ) index_body = { \"settings\" : { \"knn\" : True , \"knn.algo_param.ef_search\" : 100 , }, \"mappings\" : { \"properties\" : { \"my_vector1\" : { \"type\" : \"knn_vector\" , \"dimension\" : 2 } } } } response = client . indices . create ( knn_index_name , body = index_body ) print ( response ) Step 4: Bulk ingestion of vectors # Ingest 10 vectors in a bulk fashion to the index. These vectors represent the list of vectors to calculate the similarity for. from opensearchpy.helpers import bulk import random actions = [ { \"_index\" : knn_index_name , \"_id\" : count , \"_source\" : { \"my_vector1\" : [ random . uniform ( 0 , 10 ), random . uniform ( 0 , 10 )], } } for count in range ( 0 , 10 ) ] bulk ( client , actions , ) Step 5: Score vector similarity # Score the vector [2.5, 3] and find the 3 most similar vectors. # Define the search request query = { \"size\" : 3 , \"query\" : { \"knn\" : { \"my_vector1\" : { \"vector\" : [ 2.5 , 3 ], \"k\" : 3 } } } } # Perform the similarity search response = client . search ( body = query , index = knn_index_name ) # Pretty print response import pprint pp = pprint . PrettyPrinter () pp . pprint ( response ) Output from the above script shows the score for each of the three most similar vectors that have been indexed. [4.798869166444522, 4.069064892468535] is the most similar vector to [2.5, 3] with a score of 0.1346312 . 2022 -05-30 09 :55:50,529 INFO: POST https://10.0.2.15:9200/my_project_demo_knn_index/_search [ status:200 request:0.017s ] { '_shards' : { 'failed' : 0 , 'skipped' : 0 , 'successful' : 1 , 'total' : 1 } , 'hits' : { 'hits' : [{ '_id' : '9' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .1346312, '_source' : { 'my_vector1' : [ 4 .798869166444522, 4 .069064892468535 ]} , '_type' : '_doc' } , { '_id' : '0' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .040784083, '_source' : { 'my_vector1' : [ 6 .267438489652193, 6 .0538134453735175 ]} , '_type' : '_doc' } , { '_id' : '7' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .03222388, '_source' : { 'my_vector1' : [ 7 .973873201006634, 2 .7361877621502115 ]} , '_type' : '_doc' }] , 'max_score' : 0 .1346312, 'total' : { 'relation' : 'eq' , 'value' : 3 }} , 'timed_out' : False, 'took' : 9 } API Reference # k-NN plugin OpenSearch Conclusion # In this guide you learned how to create a simple recommendation application.","title":"Vector Database"},{"location":"user_guides/mlops/vector_database/#how-to-use-opensearch-k-nn-plugin","text":"","title":"How To Use OpenSearch k-NN plugin"},{"location":"user_guides/mlops/vector_database/#introduction","text":"The k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. To determine the neighbors, you can specify the space (the distance function) you want to use to measure the distance between points. Use cases include recommendations (for example, an \u201cother songs you might like\u201d feature in a music application), image recognition, and fraud detection. Limited to internal Jobs and Notebooks Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster.","title":"Introduction"},{"location":"user_guides/mlops/vector_database/#code","text":"In this guide, you will learn how to create a simple recommendation application, using the k-NN plugin in OpenSearch.","title":"Code"},{"location":"user_guides/mlops/vector_database/#step-1-get-the-opensearch-api","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project () opensearch_api = project . get_opensearch_api ()","title":"Step 1: Get the OpenSearch API"},{"location":"user_guides/mlops/vector_database/#step-2-configure-the-opensearch-py-client","text":"from opensearchpy import OpenSearch client = OpenSearch ( ** opensearch_api . get_default_py_config ())","title":"Step 2: Configure the opensearch-py client"},{"location":"user_guides/mlops/vector_database/#step-3-create-an-index","text":"Create an index to use by calling opensearch_api.get_project_index(..) . knn_index_name = opensearch_api . get_project_index ( \"demo_knn_index\" ) index_body = { \"settings\" : { \"knn\" : True , \"knn.algo_param.ef_search\" : 100 , }, \"mappings\" : { \"properties\" : { \"my_vector1\" : { \"type\" : \"knn_vector\" , \"dimension\" : 2 } } } } response = client . indices . create ( knn_index_name , body = index_body ) print ( response )","title":"Step 3: Create an index"},{"location":"user_guides/mlops/vector_database/#step-4-bulk-ingestion-of-vectors","text":"Ingest 10 vectors in a bulk fashion to the index. These vectors represent the list of vectors to calculate the similarity for. from opensearchpy.helpers import bulk import random actions = [ { \"_index\" : knn_index_name , \"_id\" : count , \"_source\" : { \"my_vector1\" : [ random . uniform ( 0 , 10 ), random . uniform ( 0 , 10 )], } } for count in range ( 0 , 10 ) ] bulk ( client , actions , )","title":"Step 4: Bulk ingestion of vectors"},{"location":"user_guides/mlops/vector_database/#step-5-score-vector-similarity","text":"Score the vector [2.5, 3] and find the 3 most similar vectors. # Define the search request query = { \"size\" : 3 , \"query\" : { \"knn\" : { \"my_vector1\" : { \"vector\" : [ 2.5 , 3 ], \"k\" : 3 } } } } # Perform the similarity search response = client . search ( body = query , index = knn_index_name ) # Pretty print response import pprint pp = pprint . PrettyPrinter () pp . pprint ( response ) Output from the above script shows the score for each of the three most similar vectors that have been indexed. [4.798869166444522, 4.069064892468535] is the most similar vector to [2.5, 3] with a score of 0.1346312 . 2022 -05-30 09 :55:50,529 INFO: POST https://10.0.2.15:9200/my_project_demo_knn_index/_search [ status:200 request:0.017s ] { '_shards' : { 'failed' : 0 , 'skipped' : 0 , 'successful' : 1 , 'total' : 1 } , 'hits' : { 'hits' : [{ '_id' : '9' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .1346312, '_source' : { 'my_vector1' : [ 4 .798869166444522, 4 .069064892468535 ]} , '_type' : '_doc' } , { '_id' : '0' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .040784083, '_source' : { 'my_vector1' : [ 6 .267438489652193, 6 .0538134453735175 ]} , '_type' : '_doc' } , { '_id' : '7' , '_index' : 'my_project_demo_knn_index' , '_score' : 0 .03222388, '_source' : { 'my_vector1' : [ 7 .973873201006634, 2 .7361877621502115 ]} , '_type' : '_doc' }] , 'max_score' : 0 .1346312, 'total' : { 'relation' : 'eq' , 'value' : 3 }} , 'timed_out' : False, 'took' : 9 }","title":"Step 5: Score vector similarity"},{"location":"user_guides/mlops/vector_database/#api-reference","text":"k-NN plugin OpenSearch","title":"API Reference"},{"location":"user_guides/mlops/vector_database/#conclusion","text":"In this guide you learned how to create a simple recommendation application.","title":"Conclusion"},{"location":"user_guides/projects/dummy/","text":"","title":"Authentication"},{"location":"user_guides/projects/api_key/create_api_key/","text":"How To Create An API Key # Introduction # An API key allows a user or a program to make API calls without having to authenticate with a username and password. To access an endpoint using an API key, a client should send the access token using the ApiKey authentication scheme. The API Key can now be used when connecting to your Hopsworks instance using the hopsworks , hsfs or hsml python library or set in the ApiKey header for the REST API. GET /resource HTTP/1.1 Host: server.hopsworks.ai Authorization: ApiKey <api_key> UI # In this guide, you will learn how to create an API key. Step 1: Navigate to API Keys # In the Account Settings page you can find the API section showing a list of all API keys. List of API Keys Step 2: Create an API Key # Click New Api key , select the required scopes and create it by clicking Create Api Key . Create new API Key Conclusion # In this guide you learned how to create an API Key.","title":"Create API Key"},{"location":"user_guides/projects/api_key/create_api_key/#how-to-create-an-api-key","text":"","title":"How To Create An API Key"},{"location":"user_guides/projects/api_key/create_api_key/#introduction","text":"An API key allows a user or a program to make API calls without having to authenticate with a username and password. To access an endpoint using an API key, a client should send the access token using the ApiKey authentication scheme. The API Key can now be used when connecting to your Hopsworks instance using the hopsworks , hsfs or hsml python library or set in the ApiKey header for the REST API. GET /resource HTTP/1.1 Host: server.hopsworks.ai Authorization: ApiKey <api_key>","title":"Introduction"},{"location":"user_guides/projects/api_key/create_api_key/#ui","text":"In this guide, you will learn how to create an API key.","title":"UI"},{"location":"user_guides/projects/api_key/create_api_key/#step-1-navigate-to-api-keys","text":"In the Account Settings page you can find the API section showing a list of all API keys. List of API Keys","title":"Step 1: Navigate to API Keys"},{"location":"user_guides/projects/api_key/create_api_key/#step-2-create-an-api-key","text":"Click New Api key , select the required scopes and create it by clicking Create Api Key . Create new API Key","title":"Step 2: Create an API Key"},{"location":"user_guides/projects/api_key/create_api_key/#conclusion","text":"In this guide you learned how to create an API Key.","title":"Conclusion"},{"location":"user_guides/projects/git/clone_repo/","text":"How To Clone a Git Repository # Introduction # Repositories are cloned and managed within the scope of a project. The content of the repository will reside on the Hopsworks File System. The content of the repository can be edited from Jupyter notebooks and can for example be used to configure Jobs. Repositories can be managed from the Git section in the project settings. The Git overview in the project settings provides a list of repositories currently cloned within the project, the location of their content as well which branch and commit their HEAD is currently at. Beta The feature is currently in Beta and will be improved in the upcoming releases. Prerequisites # This guide requires that you have previously configured a Git Provider with your git credentials. UI # Step 1: Navigate to repositories # In the Project settings page you can find the Git section. This page lists all the cloned git repositories under Repositories , while operations performed on those repositories, e.g push / pull / commit are listed under Git Executions . Git repository overview Step 2: Clone a repository # To clone a new repository, click on the Clone repository button on the Git overview page. Git clone The clone dialog asks you to specify the URL of the repository to clone. The supported protocol is HTTPS. As an example, if the repository is hosted on Github, the URL should look like: https://github.com/logicalclocks/hops-examples.git . Then specify which branch you want to clone. By default the main branch will be used, however a different branch or commit can be specified by selecting Clone from a specific branch . You can select the folder, within your project, in which the repository should be cloned. By default, the repository is going to be cloned within the Resources dataset. However, by clicking on the location button, a different location can be selected. Finally, click on the Clone repository button to trigger the cloning of the repository. Step 3: Track progress of the clone # The progress of the git clone can be tracked under Git Executions . Track progress of clone Step 4: Browse repository files # In the File browser page you can now browse the files of the cloned repository, found on the path in Resources/hops-examples Browse repository files Step 5: Repository actions # The operation to perform on the cloned repository can be found in the dropdown as shown below. Repository actions Code # Step 1: Get the git API # This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () git_api = project . get_git_api () Step 2: Clone the repository # REPO_URL = \"https://github.com/logicalclocks/hops-examples.git\" # git repository HOPSWORKS_FOLDER = \"Resources\" # path in hopsworks filesystem to clone to PROVIDER = \"GitHub\" BRANCH = \"master\" # optional branch to clone examples_repo = git_api . clone ( REPO_URL , HOPSWORKS_FOLDER , PROVIDER , branch = BRANCH ) A notebook for managing git can be found here . Conclusion # In this guide you learned how to clone a Git repository. You can now start Jupyter from the cloned git repository path to work with the files.","title":"Clone Repository"},{"location":"user_guides/projects/git/clone_repo/#how-to-clone-a-git-repository","text":"","title":"How To Clone a Git Repository"},{"location":"user_guides/projects/git/clone_repo/#introduction","text":"Repositories are cloned and managed within the scope of a project. The content of the repository will reside on the Hopsworks File System. The content of the repository can be edited from Jupyter notebooks and can for example be used to configure Jobs. Repositories can be managed from the Git section in the project settings. The Git overview in the project settings provides a list of repositories currently cloned within the project, the location of their content as well which branch and commit their HEAD is currently at. Beta The feature is currently in Beta and will be improved in the upcoming releases.","title":"Introduction"},{"location":"user_guides/projects/git/clone_repo/#prerequisites","text":"This guide requires that you have previously configured a Git Provider with your git credentials.","title":"Prerequisites"},{"location":"user_guides/projects/git/clone_repo/#ui","text":"","title":"UI"},{"location":"user_guides/projects/git/clone_repo/#step-1-navigate-to-repositories","text":"In the Project settings page you can find the Git section. This page lists all the cloned git repositories under Repositories , while operations performed on those repositories, e.g push / pull / commit are listed under Git Executions . Git repository overview","title":"Step 1: Navigate to repositories"},{"location":"user_guides/projects/git/clone_repo/#step-2-clone-a-repository","text":"To clone a new repository, click on the Clone repository button on the Git overview page. Git clone The clone dialog asks you to specify the URL of the repository to clone. The supported protocol is HTTPS. As an example, if the repository is hosted on Github, the URL should look like: https://github.com/logicalclocks/hops-examples.git . Then specify which branch you want to clone. By default the main branch will be used, however a different branch or commit can be specified by selecting Clone from a specific branch . You can select the folder, within your project, in which the repository should be cloned. By default, the repository is going to be cloned within the Resources dataset. However, by clicking on the location button, a different location can be selected. Finally, click on the Clone repository button to trigger the cloning of the repository.","title":"Step 2: Clone a repository"},{"location":"user_guides/projects/git/clone_repo/#step-3-track-progress-of-the-clone","text":"The progress of the git clone can be tracked under Git Executions . Track progress of clone","title":"Step 3: Track progress of the clone"},{"location":"user_guides/projects/git/clone_repo/#step-4-browse-repository-files","text":"In the File browser page you can now browse the files of the cloned repository, found on the path in Resources/hops-examples Browse repository files","title":"Step 4: Browse repository files"},{"location":"user_guides/projects/git/clone_repo/#step-5-repository-actions","text":"The operation to perform on the cloned repository can be found in the dropdown as shown below. Repository actions","title":"Step 5: Repository actions"},{"location":"user_guides/projects/git/clone_repo/#code","text":"","title":"Code"},{"location":"user_guides/projects/git/clone_repo/#step-1-get-the-git-api","text":"This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () git_api = project . get_git_api ()","title":"Step 1: Get the git API"},{"location":"user_guides/projects/git/clone_repo/#step-2-clone-the-repository","text":"REPO_URL = \"https://github.com/logicalclocks/hops-examples.git\" # git repository HOPSWORKS_FOLDER = \"Resources\" # path in hopsworks filesystem to clone to PROVIDER = \"GitHub\" BRANCH = \"master\" # optional branch to clone examples_repo = git_api . clone ( REPO_URL , HOPSWORKS_FOLDER , PROVIDER , branch = BRANCH ) A notebook for managing git can be found here .","title":"Step 2: Clone the repository"},{"location":"user_guides/projects/git/clone_repo/#conclusion","text":"In this guide you learned how to clone a Git repository. You can now start Jupyter from the cloned git repository path to work with the files.","title":"Conclusion"},{"location":"user_guides/projects/git/configure_git_provider/","text":"How To Configure a Git Provider # Introduction # When you perform Git operations on Hopsworks that need to interact with the remote repository, Hopsworks relies on the Git HTTPS protocol to perform those operations. Authentication with the remote repository happens through a token generated by the Git repository hosting service (Github, Gitlab, Bitbucket). Beta The feature is currently in Beta and will be improved in the upcoming releases. Tokens are personal The tokens are personal to each user. When you perform operations on a repository, your token is going to be used, even though the repository might belong to a different user. UI # Documentation on how to generate a token for the supported Git hosting services is available here: Github Gitlab Bitbucket Step 1: Navigate to Git Providers # In the Account Settings page you can find the Git Providers section. The Git provider section displays which providers have been already configured and can be used to clone new repositories. Git provider configuration list Step 2: Configure a provider # Click on Edit Configuration to change a provider username or token, or to configure a new provider. Tick the checkbox next to the provider you want to configure and insert the username and the token to use for that provider. Git provider configuration Click Create Configuration to save the configuration. Step 3: Provider is configured # The configured provider should now be marked as configured. Git provider configured Code # Step 1: Get the git API # This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () git_api = project . get_git_api () Step 2: Configure git provider # PROVIDER = \"GitHub\" GITHUB_USER = \"my_user\" API_TOKEN = \"my_token\" git_api . set_provider ( PROVIDER , GITHUB_USER , API_TOKEN ) API Reference # GitProvider Conclusion # In this guide you learned how configure your git provider credentials. You can now use the credentials to clone a repository from the configured provider.","title":"Configure Git Provider"},{"location":"user_guides/projects/git/configure_git_provider/#how-to-configure-a-git-provider","text":"","title":"How To Configure a Git Provider"},{"location":"user_guides/projects/git/configure_git_provider/#introduction","text":"When you perform Git operations on Hopsworks that need to interact with the remote repository, Hopsworks relies on the Git HTTPS protocol to perform those operations. Authentication with the remote repository happens through a token generated by the Git repository hosting service (Github, Gitlab, Bitbucket). Beta The feature is currently in Beta and will be improved in the upcoming releases. Tokens are personal The tokens are personal to each user. When you perform operations on a repository, your token is going to be used, even though the repository might belong to a different user.","title":"Introduction"},{"location":"user_guides/projects/git/configure_git_provider/#ui","text":"Documentation on how to generate a token for the supported Git hosting services is available here: Github Gitlab Bitbucket","title":"UI"},{"location":"user_guides/projects/git/configure_git_provider/#step-1-navigate-to-git-providers","text":"In the Account Settings page you can find the Git Providers section. The Git provider section displays which providers have been already configured and can be used to clone new repositories. Git provider configuration list","title":"Step 1: Navigate to Git Providers"},{"location":"user_guides/projects/git/configure_git_provider/#step-2-configure-a-provider","text":"Click on Edit Configuration to change a provider username or token, or to configure a new provider. Tick the checkbox next to the provider you want to configure and insert the username and the token to use for that provider. Git provider configuration Click Create Configuration to save the configuration.","title":"Step 2: Configure a provider"},{"location":"user_guides/projects/git/configure_git_provider/#step-3-provider-is-configured","text":"The configured provider should now be marked as configured. Git provider configured","title":"Step 3: Provider is configured"},{"location":"user_guides/projects/git/configure_git_provider/#code","text":"","title":"Code"},{"location":"user_guides/projects/git/configure_git_provider/#step-1-get-the-git-api","text":"This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () git_api = project . get_git_api ()","title":"Step 1: Get the git API"},{"location":"user_guides/projects/git/configure_git_provider/#step-2-configure-git-provider","text":"PROVIDER = \"GitHub\" GITHUB_USER = \"my_user\" API_TOKEN = \"my_token\" git_api . set_provider ( PROVIDER , GITHUB_USER , API_TOKEN )","title":"Step 2: Configure git provider"},{"location":"user_guides/projects/git/configure_git_provider/#api-reference","text":"GitProvider","title":"API Reference"},{"location":"user_guides/projects/git/configure_git_provider/#conclusion","text":"In this guide you learned how configure your git provider credentials. You can now use the credentials to clone a repository from the configured provider.","title":"Conclusion"},{"location":"user_guides/projects/iam_role/iam_role_chaining/","text":"How To Use AWS IAM Roles on EC2 instances # Introduction # When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles can be configured in AWS and mapped to a project in Hopsworks. Prerequisites # Before you begin this guide you'll need the following: A Hopsworks cluster running on EC2. Role chaining setup in AWS. Configure role mappings in Hopsworks. For a guide on how to configure this see AWS IAM Role Chaining . UI # In this guide, you will learn how to use a mapped IAM role in your project. Step 1: Navigate to your project's IAM Role Chaining tab # In the Project Settings page you can find the IAM Role Chaining section showing a list of all IAM roles mapped to your project. Role Chaining Step 2: Use the IAM role # You can now use the IAM roles listed in your project when creating a storage connector with Temporary Credentials . Conclusion # In this guide you learned how to use IAM roles on a cluster deployed on an EC2 instances.","title":"AWS IAM Roles"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#how-to-use-aws-iam-roles-on-ec2-instances","text":"","title":"How To Use AWS IAM Roles on EC2 instances"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#introduction","text":"When deploying Hopsworks on EC2 instances you might need to assume different roles to access resources on AWS. These roles can be configured in AWS and mapped to a project in Hopsworks.","title":"Introduction"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#prerequisites","text":"Before you begin this guide you'll need the following: A Hopsworks cluster running on EC2. Role chaining setup in AWS. Configure role mappings in Hopsworks. For a guide on how to configure this see AWS IAM Role Chaining .","title":"Prerequisites"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#ui","text":"In this guide, you will learn how to use a mapped IAM role in your project.","title":"UI"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#step-1-navigate-to-your-projects-iam-role-chaining-tab","text":"In the Project Settings page you can find the IAM Role Chaining section showing a list of all IAM roles mapped to your project. Role Chaining","title":"Step 1: Navigate to your project's IAM Role Chaining tab"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#step-2-use-the-iam-role","text":"You can now use the IAM roles listed in your project when creating a storage connector with Temporary Credentials .","title":"Step 2: Use the IAM role"},{"location":"user_guides/projects/iam_role/iam_role_chaining/#conclusion","text":"In this guide you learned how to use IAM roles on a cluster deployed on an EC2 instances.","title":"Conclusion"},{"location":"user_guides/projects/jobs/alert/","text":"How To Add Alerts To Jobs # Introduction # An API key allows a user or a program to make API calls without having to authenticate with a username and password. To access an endpoint using an API key, a client should send the access token using the ApiKey authentication scheme. The API Key can now be used when connecting to your Hopsworks instance using the hopsworks , hsfs or hsml python library or set in the ApiKey header for the REST API. GET /resource HTTP/1.1 Host: server.hopsworks.ai Authorization: ApiKey <api_key> In this guide, you will learn how to create an API key. Step 1: Navigate to API Keys # In the Account Settings page you can find the API section showing a list of all API keys. List of created API Keys Step 2: Create an API Key # Click New Api key , select the required scopes and create it by clicking Create Api Key . Create API Key Conclusion # In this guide you learned how to create an API Key.","title":"Alerts"},{"location":"user_guides/projects/jobs/alert/#how-to-add-alerts-to-jobs","text":"","title":"How To Add Alerts To Jobs"},{"location":"user_guides/projects/jobs/alert/#introduction","text":"An API key allows a user or a program to make API calls without having to authenticate with a username and password. To access an endpoint using an API key, a client should send the access token using the ApiKey authentication scheme. The API Key can now be used when connecting to your Hopsworks instance using the hopsworks , hsfs or hsml python library or set in the ApiKey header for the REST API. GET /resource HTTP/1.1 Host: server.hopsworks.ai Authorization: ApiKey <api_key> In this guide, you will learn how to create an API key.","title":"Introduction"},{"location":"user_guides/projects/jobs/alert/#step-1-navigate-to-api-keys","text":"In the Account Settings page you can find the API section showing a list of all API keys. List of created API Keys","title":"Step 1: Navigate to API Keys"},{"location":"user_guides/projects/jobs/alert/#step-2-create-an-api-key","text":"Click New Api key , select the required scopes and create it by clicking Create Api Key . Create API Key","title":"Step 2: Create an API Key"},{"location":"user_guides/projects/jobs/alert/#conclusion","text":"In this guide you learned how to create an API Key.","title":"Conclusion"},{"location":"user_guides/projects/jobs/pyspark_job/","text":"How To Run A PySpark Job # Introduction # All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a PySpark job. The PySpark program can either be a .py script or a .ipynb file. Instantiate the SparkSession For a .py file, remember to instantiate the SparkSession i.e spark=SparkSession.builder.getOrCreate() For a .ipynb file, the SparkSession is already available as spark when the job is started. UI # Step 1: Jobs overview # The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview Step 2: Create new job dialog # To configure a job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog Step 3: Set the script # Next step is to select the program to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. Configure program Step 4: Set the job type # Next step is to set the job type to SPARK to indicate it should be executed as a spark job. Then specify advanced configuration or click Create New Job to create the job. Set the job type Step 5 (optional): Advanced configuration # Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Additional files or dependencies required for the Spark job can be configured. Additional archives : Number of cores to allocate for the Spark driver Additional jars : Number of MBs to allocate for the Spark driver Additional python dependencies : Number of cores to allocate for each Spark executor Additional files : Number of MBs to allocate for each Spark executor File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration Step 6: Execute the job # Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Start job execution Step 7: Application logs # To monitor logs while the execution is running, click Spark UI to open the Spark UI in a separate tab. Once the execution is finished, you can click on Logs to see the full logs for execution. Access Spark logs Code # Step 1: Upload the python program # This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.ipynb\" , \"Resources\" ) Step 2: Create SPARK job # In this snippet we get the JobsApi object to get the default job configuration for a SPARK job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () spark_config = jobs_api . get_configuration ( \"SPARK\" ) spark_config [ 'appPath' ] = uploaded_file_path job = jobs_api . create_job ( \"pyspark_job\" , spark_config ) Step 3: Execute the job # In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. execution = job . run ( await_termination = True ) out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ()) API Reference # Jobs Executions Conclusion # In this guide you learned how to create and run a PySpark job.","title":"Run PySpark Job"},{"location":"user_guides/projects/jobs/pyspark_job/#how-to-run-a-pyspark-job","text":"","title":"How To Run A PySpark Job"},{"location":"user_guides/projects/jobs/pyspark_job/#introduction","text":"All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a PySpark job. The PySpark program can either be a .py script or a .ipynb file. Instantiate the SparkSession For a .py file, remember to instantiate the SparkSession i.e spark=SparkSession.builder.getOrCreate() For a .ipynb file, the SparkSession is already available as spark when the job is started.","title":"Introduction"},{"location":"user_guides/projects/jobs/pyspark_job/#ui","text":"","title":"UI"},{"location":"user_guides/projects/jobs/pyspark_job/#step-1-jobs-overview","text":"The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview","title":"Step 1: Jobs overview"},{"location":"user_guides/projects/jobs/pyspark_job/#step-2-create-new-job-dialog","text":"To configure a job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog","title":"Step 2: Create new job dialog"},{"location":"user_guides/projects/jobs/pyspark_job/#step-3-set-the-script","text":"Next step is to select the program to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. Configure program","title":"Step 3: Set the script"},{"location":"user_guides/projects/jobs/pyspark_job/#step-4-set-the-job-type","text":"Next step is to set the job type to SPARK to indicate it should be executed as a spark job. Then specify advanced configuration or click Create New Job to create the job. Set the job type","title":"Step 4: Set the job type"},{"location":"user_guides/projects/jobs/pyspark_job/#step-5-optional-advanced-configuration","text":"Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Additional files or dependencies required for the Spark job can be configured. Additional archives : Number of cores to allocate for the Spark driver Additional jars : Number of MBs to allocate for the Spark driver Additional python dependencies : Number of cores to allocate for each Spark executor Additional files : Number of MBs to allocate for each Spark executor File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration","title":"Step 5 (optional): Advanced configuration"},{"location":"user_guides/projects/jobs/pyspark_job/#step-6-execute-the-job","text":"Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Start job execution","title":"Step 6: Execute the job"},{"location":"user_guides/projects/jobs/pyspark_job/#step-7-application-logs","text":"To monitor logs while the execution is running, click Spark UI to open the Spark UI in a separate tab. Once the execution is finished, you can click on Logs to see the full logs for execution. Access Spark logs","title":"Step 7: Application logs"},{"location":"user_guides/projects/jobs/pyspark_job/#code","text":"","title":"Code"},{"location":"user_guides/projects/jobs/pyspark_job/#step-1-upload-the-python-program","text":"This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.ipynb\" , \"Resources\" )","title":"Step 1: Upload the python program"},{"location":"user_guides/projects/jobs/pyspark_job/#step-2-create-spark-job","text":"In this snippet we get the JobsApi object to get the default job configuration for a SPARK job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () spark_config = jobs_api . get_configuration ( \"SPARK\" ) spark_config [ 'appPath' ] = uploaded_file_path job = jobs_api . create_job ( \"pyspark_job\" , spark_config )","title":"Step 2: Create SPARK job"},{"location":"user_guides/projects/jobs/pyspark_job/#step-3-execute-the-job","text":"In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. execution = job . run ( await_termination = True ) out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ())","title":"Step 3: Execute the job"},{"location":"user_guides/projects/jobs/pyspark_job/#api-reference","text":"Jobs Executions","title":"API Reference"},{"location":"user_guides/projects/jobs/pyspark_job/#conclusion","text":"In this guide you learned how to create and run a PySpark job.","title":"Conclusion"},{"location":"user_guides/projects/jobs/python_job/","text":"How To Run A Python Job # Introduction # All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a Python job. UI # Step 1: Jobs overview # The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview Step 2: Create new job dialog # By default, the dialog will create a Spark job. To instead configure a Python job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog Step 3: Set the script # Next step is to select the python script to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. Configure program Step 4: Set the job type # Next step is to set the job type to PYTHON to indicate it should be executed as a simple python script. Then click Create New Job to create the job. Set the job type Step 5 (optional): Additional configuration # It is possible to also set following configuration settings for a PYTHON job. Container memory : The amount of memory in MB to be allocated to the Python script Container cores : The number of cores to be allocated for the Python script Additional files : List of files that will be locally accessible by the application Set the job type Step 6: Execute the job # Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Once the execution is finished, click on Logs to see the logs for the execution. Start job execution Code # Step 1: Upload the python script # This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.py\" , \"Resources\" ) Step 2: Create PYTHON job # In this snippet we get the JobsApi object to get the default job configuration for a PYTHON job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () py_job_config = jobs_api . get_configuration ( \"PYTHON\" ) py_job_config [ 'appPath' ] = uploaded_file_path job = jobs_api . create_job ( \"py_job\" , py_job_config ) Step 3: Execute the job # In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. # Run the job execution = job . run ( await_termination = True ) # Download logs out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ()) API Reference # Jobs Executions Conclusion # In this guide you learned how to create and run a job.","title":"Run Python Job"},{"location":"user_guides/projects/jobs/python_job/#how-to-run-a-python-job","text":"","title":"How To Run A Python Job"},{"location":"user_guides/projects/jobs/python_job/#introduction","text":"All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a Python job.","title":"Introduction"},{"location":"user_guides/projects/jobs/python_job/#ui","text":"","title":"UI"},{"location":"user_guides/projects/jobs/python_job/#step-1-jobs-overview","text":"The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview","title":"Step 1: Jobs overview"},{"location":"user_guides/projects/jobs/python_job/#step-2-create-new-job-dialog","text":"By default, the dialog will create a Spark job. To instead configure a Python job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog","title":"Step 2: Create new job dialog"},{"location":"user_guides/projects/jobs/python_job/#step-3-set-the-script","text":"Next step is to select the python script to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. Configure program","title":"Step 3: Set the script"},{"location":"user_guides/projects/jobs/python_job/#step-4-set-the-job-type","text":"Next step is to set the job type to PYTHON to indicate it should be executed as a simple python script. Then click Create New Job to create the job. Set the job type","title":"Step 4: Set the job type"},{"location":"user_guides/projects/jobs/python_job/#step-5-optional-additional-configuration","text":"It is possible to also set following configuration settings for a PYTHON job. Container memory : The amount of memory in MB to be allocated to the Python script Container cores : The number of cores to be allocated for the Python script Additional files : List of files that will be locally accessible by the application Set the job type","title":"Step 5 (optional): Additional configuration"},{"location":"user_guides/projects/jobs/python_job/#step-6-execute-the-job","text":"Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Once the execution is finished, click on Logs to see the logs for the execution. Start job execution","title":"Step 6: Execute the job"},{"location":"user_guides/projects/jobs/python_job/#code","text":"","title":"Code"},{"location":"user_guides/projects/jobs/python_job/#step-1-upload-the-python-script","text":"This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.py\" , \"Resources\" )","title":"Step 1: Upload the python script"},{"location":"user_guides/projects/jobs/python_job/#step-2-create-python-job","text":"In this snippet we get the JobsApi object to get the default job configuration for a PYTHON job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () py_job_config = jobs_api . get_configuration ( \"PYTHON\" ) py_job_config [ 'appPath' ] = uploaded_file_path job = jobs_api . create_job ( \"py_job\" , py_job_config )","title":"Step 2: Create PYTHON job"},{"location":"user_guides/projects/jobs/python_job/#step-3-execute-the-job","text":"In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. # Run the job execution = job . run ( await_termination = True ) # Download logs out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ())","title":"Step 3: Execute the job"},{"location":"user_guides/projects/jobs/python_job/#api-reference","text":"Jobs Executions","title":"API Reference"},{"location":"user_guides/projects/jobs/python_job/#conclusion","text":"In this guide you learned how to create and run a job.","title":"Conclusion"},{"location":"user_guides/projects/jobs/spark_job/","text":"How To Run A Spark Job # Introduction # All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a Spark job. UI # Step 1: Jobs overview # The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview Step 2: Create new job dialog # To configure a job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog Step 3: Set the jar # Next step is to select the program to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. After that set the name for the job. Configure program Step 4: Set the job type # Next step is to set the job type to SPARK to indicate it should be executed as a spark job. Then specify additional configuration or click Create New Job to create the job. Set the job type Step 5: Set the main class # Next step is to set the main class for the application. Then specify advanced configuration or click Create New Job to create the job. Set the main class Step 6 (optional): Advanced configuration # Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Additional files or dependencies required for the Spark job can be configured. Additional archives : Number of cores to allocate for the Spark driver Additional jars : Number of MBs to allocate for the Spark driver Additional python dependencies : Number of cores to allocate for each Spark executor Additional files : Number of MBs to allocate for each Spark executor File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration Step 7: Execute the job # Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Start job execution Step 8: Application logs # To monitor logs while the execution is running, click Spark UI to open the Spark UI in a separate tab. Once the execution is finished, you can click on Logs to see the full logs for execution. Access Spark logs Code # Step 1: Upload the python program # This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.ipynb\" , \"Resources\" ) Step 2: Create SPARK job # In this snippet we get the JobsApi object to get the default job configuration for a SPARK job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () spark_config = jobs_api . get_configuration ( \"SPARK\" ) spark_config [ 'appPath' ] = uploaded_file_path spark_config [ 'mainClass' ] = 'org.apache.spark.examples.SparkPi' job = jobs_api . create_job ( \"pyspark_job\" , spark_config ) Step 3: Execute the job # In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. execution = job . run ( await_termination = True ) out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ()) API Reference # Jobs Executions Conclusion # In this guide you learned how to create and run a PySpark job.","title":"Run Spark Job"},{"location":"user_guides/projects/jobs/spark_job/#how-to-run-a-spark-job","text":"","title":"How To Run A Spark Job"},{"location":"user_guides/projects/jobs/spark_job/#introduction","text":"All members of a project in Hopsworks can launch the following types of applications through a project's Jobs service: Python ( Hopsworks Enterprise only ) Apache Spark Launching a job of any type is very similar process, what mostly differs between job types is the various configuration parameters each job type comes with. After following this guide you will be able to create a Spark job.","title":"Introduction"},{"location":"user_guides/projects/jobs/spark_job/#ui","text":"","title":"UI"},{"location":"user_guides/projects/jobs/spark_job/#step-1-jobs-overview","text":"The image below shows the Jobs overview page in Hopsworks and is accessed by clicking Jobs in the sidebar. Jobs overview","title":"Step 1: Jobs overview"},{"location":"user_guides/projects/jobs/spark_job/#step-2-create-new-job-dialog","text":"To configure a job, click Advanced options , which will open up the advanced configuration page for the job. Create new job dialog","title":"Step 2: Create new job dialog"},{"location":"user_guides/projects/jobs/spark_job/#step-3-set-the-jar","text":"Next step is to select the program to run. You can either select From project , if the file was previously uploaded to Hopsworks, or Upload new file which lets you select a file from your local filesystem as demonstrated below. After that set the name for the job. Configure program","title":"Step 3: Set the jar"},{"location":"user_guides/projects/jobs/spark_job/#step-4-set-the-job-type","text":"Next step is to set the job type to SPARK to indicate it should be executed as a spark job. Then specify additional configuration or click Create New Job to create the job. Set the job type","title":"Step 4: Set the job type"},{"location":"user_guides/projects/jobs/spark_job/#step-5-set-the-main-class","text":"Next step is to set the main class for the application. Then specify advanced configuration or click Create New Job to create the job. Set the main class","title":"Step 5: Set the main class"},{"location":"user_guides/projects/jobs/spark_job/#step-6-optional-advanced-configuration","text":"Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Additional files or dependencies required for the Spark job can be configured. Additional archives : Number of cores to allocate for the Spark driver Additional jars : Number of MBs to allocate for the Spark driver Additional python dependencies : Number of cores to allocate for each Spark executor Additional files : Number of MBs to allocate for each Spark executor File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration","title":"Step 6 (optional): Advanced configuration"},{"location":"user_guides/projects/jobs/spark_job/#step-7-execute-the-job","text":"Now click the Run button to start the execution of the job, and then click on Executions to see the list of all executions. Start job execution","title":"Step 7: Execute the job"},{"location":"user_guides/projects/jobs/spark_job/#step-8-application-logs","text":"To monitor logs while the execution is running, click Spark UI to open the Spark UI in a separate tab. Once the execution is finished, you can click on Logs to see the full logs for execution. Access Spark logs","title":"Step 8: Application logs"},{"location":"user_guides/projects/jobs/spark_job/#code","text":"","title":"Code"},{"location":"user_guides/projects/jobs/spark_job/#step-1-upload-the-python-program","text":"This snippet assumes the python script is in the current working directory and named script.py . It will upload the python script to run to the Resources dataset. import hopsworks connection = hopsworks . connection () project = connection . get_project () dataset_api = project . get_dataset_api () uploaded_file_path = dataset_api . upload ( \"script.ipynb\" , \"Resources\" )","title":"Step 1: Upload the python program"},{"location":"user_guides/projects/jobs/spark_job/#step-2-create-spark-job","text":"In this snippet we get the JobsApi object to get the default job configuration for a SPARK job, set the python script to run and create the Job object. jobs_api = project . get_jobs_api () spark_config = jobs_api . get_configuration ( \"SPARK\" ) spark_config [ 'appPath' ] = uploaded_file_path spark_config [ 'mainClass' ] = 'org.apache.spark.examples.SparkPi' job = jobs_api . create_job ( \"pyspark_job\" , spark_config )","title":"Step 2: Create SPARK job"},{"location":"user_guides/projects/jobs/spark_job/#step-3-execute-the-job","text":"In this snippet we execute the job synchronously, that is wait until it reaches a terminal state, and then download and print the logs. execution = job . run ( await_termination = True ) out , err = execution . download_logs () f_out = open ( out , \"r\" ) print ( f_out . read ()) f_err = open ( err , \"r\" ) print ( f_err . read ())","title":"Step 3: Execute the job"},{"location":"user_guides/projects/jobs/spark_job/#api-reference","text":"Jobs Executions","title":"API Reference"},{"location":"user_guides/projects/jobs/spark_job/#conclusion","text":"In this guide you learned how to create and run a PySpark job.","title":"Conclusion"},{"location":"user_guides/projects/jupyter/python_notebook/","text":"How To Run A Python Notebook # Introduction # Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop. Supports JupyterLab and the classic Jupyter front-end Configured with Python3, Spark, PySpark and SparkR kernels Step 1: Jupyter dashboard # The image below shows the Jupyter service page in Hopsworks and is accessed by clicking Jupyter in the sidebar. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below. Step 2 (Optional): Configure resources # Next step is to configure Jupyter, Click edit configuration to get to the configuration page and select Python . Container cores : Number of cores to allocate for the Jupyter instance Container memory : Number of MBs to allocate for the Jupyter instance Configured resource pool is shared by all running kernels. If a kernel crashes while executing a cell, try increasing the Container memory. Resource configuration for the Python kernel Click Save to save the new configuration. Step 3 (Optional): Configure max runtime and root path # Before starting the server there are two additional configurations that can be set next to the Run Jupyter button. The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting no limit . Configure maximum runtime The root path from which to start the Jupyter instance can be configured. By default it starts by setting the /Jupyter folder as the root. Configure root folder Step 4: Start Jupyter # Start the Jupyter instance by clicking the Run Jupyter button. Starting Jupyter and running a Python notebook Conclusion # In this guide you learned how to configure and run a Python application in Jupyter. You can now follow this guide to install a library that can be used in a notebook.","title":"Run Python Notebook"},{"location":"user_guides/projects/jupyter/python_notebook/#how-to-run-a-python-notebook","text":"","title":"How To Run A Python Notebook"},{"location":"user_guides/projects/jupyter/python_notebook/#introduction","text":"Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop. Supports JupyterLab and the classic Jupyter front-end Configured with Python3, Spark, PySpark and SparkR kernels","title":"Introduction"},{"location":"user_guides/projects/jupyter/python_notebook/#step-1-jupyter-dashboard","text":"The image below shows the Jupyter service page in Hopsworks and is accessed by clicking Jupyter in the sidebar. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below.","title":"Step 1: Jupyter dashboard"},{"location":"user_guides/projects/jupyter/python_notebook/#step-2-optional-configure-resources","text":"Next step is to configure Jupyter, Click edit configuration to get to the configuration page and select Python . Container cores : Number of cores to allocate for the Jupyter instance Container memory : Number of MBs to allocate for the Jupyter instance Configured resource pool is shared by all running kernels. If a kernel crashes while executing a cell, try increasing the Container memory. Resource configuration for the Python kernel Click Save to save the new configuration.","title":"Step 2 (Optional): Configure resources"},{"location":"user_guides/projects/jupyter/python_notebook/#step-3-optional-configure-max-runtime-and-root-path","text":"Before starting the server there are two additional configurations that can be set next to the Run Jupyter button. The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting no limit . Configure maximum runtime The root path from which to start the Jupyter instance can be configured. By default it starts by setting the /Jupyter folder as the root. Configure root folder","title":"Step 3 (Optional): Configure max runtime and root path"},{"location":"user_guides/projects/jupyter/python_notebook/#step-4-start-jupyter","text":"Start the Jupyter instance by clicking the Run Jupyter button. Starting Jupyter and running a Python notebook","title":"Step 4: Start Jupyter"},{"location":"user_guides/projects/jupyter/python_notebook/#conclusion","text":"In this guide you learned how to configure and run a Python application in Jupyter. You can now follow this guide to install a library that can be used in a notebook.","title":"Conclusion"},{"location":"user_guides/projects/jupyter/spark_notebook/","text":"How To Run A Spark Notebook # Introduction # Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop. Supports JupyterLab and the classic Jupyter front-end Configured with Python3, Spark, PySpark and SparkR kernels Step 1: Jupyter dashboard # The image below shows the Jupyter service page in Hopsworks and is accessed by clicking Jupyter in the sidebar. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below. Step 2 (Optional): Configure spark # Next step is to configure the Spark properties to be used in Jupyter, Click edit configuration to get to the configuration page and select Spark . Resource and compute # Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels Attach files or dependencies # Additional files or dependencies required for the Spark job can be configured. Additional archives : List of zip or .tgz files that will be locally accessible by the application Additional jars : List of .jar files to add to the CLASSPATH of the application Additional python dependencies : List of .py, .zip or .egg files that will be locally accessible by the application Additional files : List of files that will be locally accessible by the application File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration Click Save to save the new configuration. Step 3 (Optional): Configure max runtime and root path # Before starting the server there are two additional configurations that can be set next to the Run Jupyter button. The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting no limit . Configure maximum runtime The root path from which to start the Jupyter instance can be configured. By default it starts by setting the /Jupyter folder as the root. Configure root folder Step 4: Start Jupyter # Start the Jupyter instance by clicking the Run Jupyter button. Starting Jupyter and running a Spark notebook Step 5: Access Spark UI # Navigate back to Hopsworks and a Spark session will have appeared, click on the Spark UI button to go to the Spark UI. Access Spark UI and see application logs Conclusion # In this guide you learned how to configure and run a Spark application in Jupyter. You can now follow this guide to install a library that can be used in a notebook.","title":"Run Spark Notebook"},{"location":"user_guides/projects/jupyter/spark_notebook/#how-to-run-a-spark-notebook","text":"","title":"How To Run A Spark Notebook"},{"location":"user_guides/projects/jupyter/spark_notebook/#introduction","text":"Jupyter is provided as a service in Hopsworks, providing the same user experience and features as if run on your laptop. Supports JupyterLab and the classic Jupyter front-end Configured with Python3, Spark, PySpark and SparkR kernels","title":"Introduction"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-1-jupyter-dashboard","text":"The image below shows the Jupyter service page in Hopsworks and is accessed by clicking Jupyter in the sidebar. Jupyter dashboard in Hopsworks From this page, you can configure various options and settings to start Jupyter with as described in the sections below.","title":"Step 1: Jupyter dashboard"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-2-optional-configure-spark","text":"Next step is to configure the Spark properties to be used in Jupyter, Click edit configuration to get to the configuration page and select Spark .","title":"Step 2 (Optional): Configure spark"},{"location":"user_guides/projects/jupyter/spark_notebook/#resource-and-compute","text":"Resource allocation for the Spark driver and executors can be configured, also the number of executors and whether dynamic execution should be enabled. Driver memory : Number of cores to allocate for the Spark driver Driver virtual cores : Number of MBs to allocate for the Spark driver Executor memory : Number of cores to allocate for each Spark executor Executor virtual cores : Number of MBs to allocate for each Spark executor Dynamic/Static : Run the Spark application in static or dynamic allocation mode (see spark docs for details). Resource configuration for the Spark kernels","title":"Resource and compute"},{"location":"user_guides/projects/jupyter/spark_notebook/#attach-files-or-dependencies","text":"Additional files or dependencies required for the Spark job can be configured. Additional archives : List of zip or .tgz files that will be locally accessible by the application Additional jars : List of .jar files to add to the CLASSPATH of the application Additional python dependencies : List of .py, .zip or .egg files that will be locally accessible by the application Additional files : List of files that will be locally accessible by the application File configuration for the Spark kernels Line-separates properties to be set for the Spark application. For example, changing the configuration variables for the Kryo Serializer or setting environment variables for the driver, you can set the properties as shown below. Additional Spark configuration Click Save to save the new configuration.","title":"Attach files or dependencies"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-3-optional-configure-max-runtime-and-root-path","text":"Before starting the server there are two additional configurations that can be set next to the Run Jupyter button. The runtime of the Jupyter instance can be configured, this is useful to ensure that idle instances will not be hanging around and keep allocating resources. If a limited runtime is not desirable, this can be disabled by setting no limit . Configure maximum runtime The root path from which to start the Jupyter instance can be configured. By default it starts by setting the /Jupyter folder as the root. Configure root folder","title":"Step 3 (Optional): Configure max runtime and root path"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-4-start-jupyter","text":"Start the Jupyter instance by clicking the Run Jupyter button. Starting Jupyter and running a Spark notebook","title":"Step 4: Start Jupyter"},{"location":"user_guides/projects/jupyter/spark_notebook/#step-5-access-spark-ui","text":"Navigate back to Hopsworks and a Spark session will have appeared, click on the Spark UI button to go to the Spark UI. Access Spark UI and see application logs","title":"Step 5: Access Spark UI"},{"location":"user_guides/projects/jupyter/spark_notebook/#conclusion","text":"In this guide you learned how to configure and run a Spark application in Jupyter. You can now follow this guide to install a library that can be used in a notebook.","title":"Conclusion"},{"location":"user_guides/projects/kafka/consume_messages/","text":"How To Consume Message From A Topic # Introduction # A Consumer is a process which reads messages from a kafka topic. Prerequisites # This guide requires that you have previously produced messages to a kafka topic. Code # In this guide, you will learn how to consume messages from a kafka topic. Step 1: Get the Kafka API # import hopsworks connection = hopsworks . connection () project = connection . get_project () kafka_api = project . get_kafka_api () Step 2: Configure confluent-kafka client # consumer_config = kafka_api . get_default_config () consumer_config [ 'default.topic.config' ] = { 'auto.offset.reset' : 'earliest' } from confluent_kafka import Consumer consumer = Consumer ( consumer_config ) Step 3: Consume messages from a topic # # Subscribe to topic consumer . subscribe ([ \"my_topic\" ]) for i in range ( 0 , 10 ): msg = consumer . poll ( timeout = 10.0 ) print ( msg . value ()) API Reference # KafkaTopic Conclusion # In this guide you learned how to consume messages from a Kafka Topic.","title":"Consume messages"},{"location":"user_guides/projects/kafka/consume_messages/#how-to-consume-message-from-a-topic","text":"","title":"How To Consume Message From A Topic"},{"location":"user_guides/projects/kafka/consume_messages/#introduction","text":"A Consumer is a process which reads messages from a kafka topic.","title":"Introduction"},{"location":"user_guides/projects/kafka/consume_messages/#prerequisites","text":"This guide requires that you have previously produced messages to a kafka topic.","title":"Prerequisites"},{"location":"user_guides/projects/kafka/consume_messages/#code","text":"In this guide, you will learn how to consume messages from a kafka topic.","title":"Code"},{"location":"user_guides/projects/kafka/consume_messages/#step-1-get-the-kafka-api","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project () kafka_api = project . get_kafka_api ()","title":"Step 1: Get the Kafka API"},{"location":"user_guides/projects/kafka/consume_messages/#step-2-configure-confluent-kafka-client","text":"consumer_config = kafka_api . get_default_config () consumer_config [ 'default.topic.config' ] = { 'auto.offset.reset' : 'earliest' } from confluent_kafka import Consumer consumer = Consumer ( consumer_config )","title":"Step 2: Configure confluent-kafka client"},{"location":"user_guides/projects/kafka/consume_messages/#step-3-consume-messages-from-a-topic","text":"# Subscribe to topic consumer . subscribe ([ \"my_topic\" ]) for i in range ( 0 , 10 ): msg = consumer . poll ( timeout = 10.0 ) print ( msg . value ())","title":"Step 3: Consume messages from a topic"},{"location":"user_guides/projects/kafka/consume_messages/#api-reference","text":"KafkaTopic","title":"API Reference"},{"location":"user_guides/projects/kafka/consume_messages/#conclusion","text":"In this guide you learned how to consume messages from a Kafka Topic.","title":"Conclusion"},{"location":"user_guides/projects/kafka/create_schema/","text":"How To Create A Kafka Schema # Introduction # Code # In this guide, you will learn how to create a Kafka Avro Schema in the Hopsworks Schema Registry. Step 1: Get the Kafka API # import hopsworks connection = hopsworks . connection () project = connection . get_project () kafka_api = project . get_kafka_api () Step 2: Define the schema # Define the Avro Schema, see types for the format of the schema. schema = { \"type\" : \"record\" , \"name\" : \"tutorial\" , \"fields\" : [ { \"name\" : \"id\" , \"type\" : \"int\" }, { \"name\" : \"data\" , \"type\" : \"string\" } ] } Step 3: Create the schema # Create the schema in the Schema Registry. SCHEMA_NAME = \"schema_example\" my_schema = kafka_api . create_schema ( SCHEMA_NAME , schema ) API Reference # KafkaSchema Conclusion # In this guide you learned how to create a Kafka Schema.","title":"Create Schema"},{"location":"user_guides/projects/kafka/create_schema/#how-to-create-a-kafka-schema","text":"","title":"How To Create A Kafka Schema"},{"location":"user_guides/projects/kafka/create_schema/#introduction","text":"","title":"Introduction"},{"location":"user_guides/projects/kafka/create_schema/#code","text":"In this guide, you will learn how to create a Kafka Avro Schema in the Hopsworks Schema Registry.","title":"Code"},{"location":"user_guides/projects/kafka/create_schema/#step-1-get-the-kafka-api","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project () kafka_api = project . get_kafka_api ()","title":"Step 1: Get the Kafka API"},{"location":"user_guides/projects/kafka/create_schema/#step-2-define-the-schema","text":"Define the Avro Schema, see types for the format of the schema. schema = { \"type\" : \"record\" , \"name\" : \"tutorial\" , \"fields\" : [ { \"name\" : \"id\" , \"type\" : \"int\" }, { \"name\" : \"data\" , \"type\" : \"string\" } ] }","title":"Step 2: Define the schema"},{"location":"user_guides/projects/kafka/create_schema/#step-3-create-the-schema","text":"Create the schema in the Schema Registry. SCHEMA_NAME = \"schema_example\" my_schema = kafka_api . create_schema ( SCHEMA_NAME , schema )","title":"Step 3: Create the schema"},{"location":"user_guides/projects/kafka/create_schema/#api-reference","text":"KafkaSchema","title":"API Reference"},{"location":"user_guides/projects/kafka/create_schema/#conclusion","text":"In this guide you learned how to create a Kafka Schema.","title":"Conclusion"},{"location":"user_guides/projects/kafka/create_topic/","text":"How To Create A Kafka Topic # Introduction # A Topic is a queue to which records are stored and published. Producer applications write data to topics and consumer applications read from topics. Prerequisites # This guide requires that you have previously created a Kafka Schema to be used for the topic. Code # In this guide, you will learn how to create a Kafka Topic. Step 1: Get the Kafka API # import hopsworks connection = hopsworks . connection () project = connection . get_project () kafka_api = project . get_kafka_api () Step 2: Define the schema # TOPIC_NAME = \"topic_example\" SCHEMA_NAME = \"schema_example\" my_topic = kafka_api . create_topic ( TOPIC_NAME , SCHEMA_NAME , 1 , replicas = 1 , partitions = 1 ) API Reference # KafkaTopic Conclusion # In this guide you learned how to create a Kafka Topic.","title":"Create Topic"},{"location":"user_guides/projects/kafka/create_topic/#how-to-create-a-kafka-topic","text":"","title":"How To Create A Kafka Topic"},{"location":"user_guides/projects/kafka/create_topic/#introduction","text":"A Topic is a queue to which records are stored and published. Producer applications write data to topics and consumer applications read from topics.","title":"Introduction"},{"location":"user_guides/projects/kafka/create_topic/#prerequisites","text":"This guide requires that you have previously created a Kafka Schema to be used for the topic.","title":"Prerequisites"},{"location":"user_guides/projects/kafka/create_topic/#code","text":"In this guide, you will learn how to create a Kafka Topic.","title":"Code"},{"location":"user_guides/projects/kafka/create_topic/#step-1-get-the-kafka-api","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project () kafka_api = project . get_kafka_api ()","title":"Step 1: Get the Kafka API"},{"location":"user_guides/projects/kafka/create_topic/#step-2-define-the-schema","text":"TOPIC_NAME = \"topic_example\" SCHEMA_NAME = \"schema_example\" my_topic = kafka_api . create_topic ( TOPIC_NAME , SCHEMA_NAME , 1 , replicas = 1 , partitions = 1 )","title":"Step 2: Define the schema"},{"location":"user_guides/projects/kafka/create_topic/#api-reference","text":"KafkaTopic","title":"API Reference"},{"location":"user_guides/projects/kafka/create_topic/#conclusion","text":"In this guide you learned how to create a Kafka Topic.","title":"Conclusion"},{"location":"user_guides/projects/kafka/produce_messages/","text":"How To Produce To A Topic # Introduction # A Producer is a process which produces messages to a kafka topic. Prerequisites # This guide requires that you have previously created a Kafka Topic . Code # In this guide, you will learn how to produce messages to a kafka topic. Step 1: Get the Kafka API # import hopsworks connection = hopsworks . connection () project = connection . get_project () kafka_api = project . get_kafka_api () Step 2: Configure confluent-kafka client # producer_config = kafka_api . get_default_config () from confluent_kafka import Producer producer = Producer ( producer_config ) Step 3: Produce messages to topic # import uuid import json # Send a few messages for i in range ( 0 , 10 ): producer . produce ( \"my_topic\" , json . dumps ({ \"id\" : i , \"data\" : str ( uuid . uuid1 ())}), \"key\" ) # Trigger the sending of all messages to the brokers, 10 sec timeout producer . flush ( 10 ) API Reference # KafkaTopic Conclusion # In this guide you learned how to produce messages to a Kafka Topic. Next step is to create a Consumer to read the messages from the topic.","title":"Produce messages"},{"location":"user_guides/projects/kafka/produce_messages/#how-to-produce-to-a-topic","text":"","title":"How To Produce To A Topic"},{"location":"user_guides/projects/kafka/produce_messages/#introduction","text":"A Producer is a process which produces messages to a kafka topic.","title":"Introduction"},{"location":"user_guides/projects/kafka/produce_messages/#prerequisites","text":"This guide requires that you have previously created a Kafka Topic .","title":"Prerequisites"},{"location":"user_guides/projects/kafka/produce_messages/#code","text":"In this guide, you will learn how to produce messages to a kafka topic.","title":"Code"},{"location":"user_guides/projects/kafka/produce_messages/#step-1-get-the-kafka-api","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project () kafka_api = project . get_kafka_api ()","title":"Step 1: Get the Kafka API"},{"location":"user_guides/projects/kafka/produce_messages/#step-2-configure-confluent-kafka-client","text":"producer_config = kafka_api . get_default_config () from confluent_kafka import Producer producer = Producer ( producer_config )","title":"Step 2: Configure confluent-kafka client"},{"location":"user_guides/projects/kafka/produce_messages/#step-3-produce-messages-to-topic","text":"import uuid import json # Send a few messages for i in range ( 0 , 10 ): producer . produce ( \"my_topic\" , json . dumps ({ \"id\" : i , \"data\" : str ( uuid . uuid1 ())}), \"key\" ) # Trigger the sending of all messages to the brokers, 10 sec timeout producer . flush ( 10 )","title":"Step 3: Produce messages to topic"},{"location":"user_guides/projects/kafka/produce_messages/#api-reference","text":"KafkaTopic","title":"API Reference"},{"location":"user_guides/projects/kafka/produce_messages/#conclusion","text":"In this guide you learned how to produce messages to a Kafka Topic. Next step is to create a Consumer to read the messages from the topic.","title":"Conclusion"},{"location":"user_guides/projects/opensearch/connect/","text":"How To Connect To OpenSearch # Introduction # Text here Limited to internal Jobs and Notebooks Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster. Code # In this guide, you will learn how to connect to the OpenSearch cluster using an opensearch-py client. Step 1: Get the OpenSearch API # import hopsworks connection = hopsworks . connection () project = connection . get_project () opensearch_api = project . get_opensearch_api () Step 2: Configure the opensearch-py client # from opensearchpy import OpenSearch client = OpenSearch ( ** opensearch_api . get_default_py_config ()) API Reference # OpenSearch Conclusion # In this guide you learned how to connect to the OpenSearch cluster. You can now use the client to interact directly with the OpenSearch cluster, such as vector database .","title":"Connect"},{"location":"user_guides/projects/opensearch/connect/#how-to-connect-to-opensearch","text":"","title":"How To Connect To OpenSearch"},{"location":"user_guides/projects/opensearch/connect/#introduction","text":"Text here Limited to internal Jobs and Notebooks Currently it's only possible to configure the opensearch-py client in a job or jupyter notebook running inside the Hopsworks cluster.","title":"Introduction"},{"location":"user_guides/projects/opensearch/connect/#code","text":"In this guide, you will learn how to connect to the OpenSearch cluster using an opensearch-py client.","title":"Code"},{"location":"user_guides/projects/opensearch/connect/#step-1-get-the-opensearch-api","text":"import hopsworks connection = hopsworks . connection () project = connection . get_project () opensearch_api = project . get_opensearch_api ()","title":"Step 1: Get the OpenSearch API"},{"location":"user_guides/projects/opensearch/connect/#step-2-configure-the-opensearch-py-client","text":"from opensearchpy import OpenSearch client = OpenSearch ( ** opensearch_api . get_default_py_config ())","title":"Step 2: Configure the opensearch-py client"},{"location":"user_guides/projects/opensearch/connect/#api-reference","text":"OpenSearch","title":"API Reference"},{"location":"user_guides/projects/opensearch/connect/#conclusion","text":"In this guide you learned how to connect to the OpenSearch cluster. You can now use the client to interact directly with the OpenSearch cluster, such as vector database .","title":"Conclusion"},{"location":"user_guides/projects/project/add_members/","text":"How To Add Members To A Project # Introduction # In this guide, you will learn how to add new members to your project. Step 1: Members list # In the Project settings page you can find the General section, which lists the members of the project. List of project members Step 2: Add new member # Next click Add members and a dialog where users can be invited. Select the users to invite. Add new member dialog Step 3: Member invited # The invited user will now appear in the list of members and will have access to the project. List of project members Conclusion # In this guide you learned how to add a new member.","title":"Add Members"},{"location":"user_guides/projects/project/add_members/#how-to-add-members-to-a-project","text":"","title":"How To Add Members To A Project"},{"location":"user_guides/projects/project/add_members/#introduction","text":"In this guide, you will learn how to add new members to your project.","title":"Introduction"},{"location":"user_guides/projects/project/add_members/#step-1-members-list","text":"In the Project settings page you can find the General section, which lists the members of the project. List of project members","title":"Step 1: Members list"},{"location":"user_guides/projects/project/add_members/#step-2-add-new-member","text":"Next click Add members and a dialog where users can be invited. Select the users to invite. Add new member dialog","title":"Step 2: Add new member"},{"location":"user_guides/projects/project/add_members/#step-3-member-invited","text":"The invited user will now appear in the list of members and will have access to the project. List of project members","title":"Step 3: Member invited"},{"location":"user_guides/projects/project/add_members/#conclusion","text":"In this guide you learned how to add a new member.","title":"Conclusion"},{"location":"user_guides/projects/project/create_project/","text":"How To Create A Project # Introduction # In this guide, you will learn how to create a new project. Project name validation rules A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There is also a number of reserved project names that can not be used. GUI # Step 1: Create a project # If you log in to the platform and do not have any projects, you are presented with the following view. To run the Feature Store tour click Run a demo project , to create a new project click Create new project . For this guide click Create new project to continue. Landing page Step 2: Project creation form # In the creation form in which you enter the project name, an optional description and set of members to invite to the project. Project creation form Step 3: Project creation # Then wait for the project creation process to finish. List of created API Keys Step 4: Project overview # Once the project is created the overview page for it will appear. List of created API Keys Code # Step 1: Connect to Hopsworks # import hopsworks connection = hopsworks . connection () Step 2: Create project # project = connection . create_project ( \"my_project\" ) API Reference # Projects Reserved project names # PROJECTS, HOPS-SYSTEM, HOPSWORKS, INFORMATION_SCHEMA, AIRFLOW, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO, PERFORMANCE_SCHEMA, SQOOP, SYS, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO, PERFORMANCE_SCHEMA, SQOOP, SYS, BIGINT, BINARY, BOOLEAN, BOTH, BY, CASE, CAST, CHAR, COLUMN, CONF, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE, CURRENT_TIMESTAMP, CURSOR, DATABASE, DATE, DECIMAL, DELETE, DESCRIBE, DISTINCT, DOUBLE, DROP, ELSE, END, EXCHANGE, EXISTS, EXTENDED, EXTERNAL, FALSE, FETCH, FLOAT, FOLLOWING, FOR, FROM, FULL, FUNCTION, GRANT, GROUP, GROUPING, HAVING, IF, IMPORT, IN, INNER, INSERT, INT, INTERSECT, INTERVAL, INTO, IS, JOIN, LATERAL, LEFT, LESS, LIKE, LOCAL, MACRO, MAP, MORE, NONE, NOT, NULL, OF, ON, OR, ORDER, OUT, OUTER, OVER, PARTIALSCAN, PARTITION, PERCENT, PRECEDING, PRESERVE, PROCEDURE, RANGE, READS, REDUCE, REVOKE, RIGHT, ROLLUP, ROW, ROWS, SELECT, SET, SMALLINT, TABLE, TABLESAMPLE, THEN, TIMESTAMP, TO, TRANSFORM, TRIGGER, TRUE, TRUNCATE, UNBOUNDED, UNION, UNIQUEJOIN, UPDATE, USER, USING, UTC_TMESTAMP, VALUES, VARCHAR, WHEN, WHERE, WINDOW, WITH, COMMIT, ONLY, REGEXP, RLIKE, ROLLBACK, START, CACHE, CONSTRAINT, FOREIGN, PRIMARY, REFERENCES, DAYOFWEEK, EXTRACT, FLOOR, INTEGER, PRECISION, VIEWS, TIME, NUMERIC, SYNC, BASE, PYTHON37, FILEBEAT. And any word containing _FEATURESTORE. Conclusion # In this guide you learned how to create a project.","title":"Create Project"},{"location":"user_guides/projects/project/create_project/#how-to-create-a-project","text":"","title":"How To Create A Project"},{"location":"user_guides/projects/project/create_project/#introduction","text":"In this guide, you will learn how to create a new project. Project name validation rules A valid project name can only contain characters a-z, A-Z, 0-9 and special characters \u2018_\u2019 and \u2018.\u2019 but not \u2018__\u2019 (double underscore). There is also a number of reserved project names that can not be used.","title":"Introduction"},{"location":"user_guides/projects/project/create_project/#gui","text":"","title":"GUI"},{"location":"user_guides/projects/project/create_project/#step-1-create-a-project","text":"If you log in to the platform and do not have any projects, you are presented with the following view. To run the Feature Store tour click Run a demo project , to create a new project click Create new project . For this guide click Create new project to continue. Landing page","title":"Step 1: Create a project"},{"location":"user_guides/projects/project/create_project/#step-2-project-creation-form","text":"In the creation form in which you enter the project name, an optional description and set of members to invite to the project. Project creation form","title":"Step 2: Project creation form"},{"location":"user_guides/projects/project/create_project/#step-3-project-creation","text":"Then wait for the project creation process to finish. List of created API Keys","title":"Step 3: Project creation"},{"location":"user_guides/projects/project/create_project/#step-4-project-overview","text":"Once the project is created the overview page for it will appear. List of created API Keys","title":"Step 4: Project overview"},{"location":"user_guides/projects/project/create_project/#code","text":"","title":"Code"},{"location":"user_guides/projects/project/create_project/#step-1-connect-to-hopsworks","text":"import hopsworks connection = hopsworks . connection ()","title":"Step 1: Connect to Hopsworks"},{"location":"user_guides/projects/project/create_project/#step-2-create-project","text":"project = connection . create_project ( \"my_project\" )","title":"Step 2: Create project"},{"location":"user_guides/projects/project/create_project/#api-reference","text":"Projects","title":"API Reference"},{"location":"user_guides/projects/project/create_project/#reserved-project-names","text":"PROJECTS, HOPS-SYSTEM, HOPSWORKS, INFORMATION_SCHEMA, AIRFLOW, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO, PERFORMANCE_SCHEMA, SQOOP, SYS, GLASSFISH_TIMERS, GRAFANA, HOPS, METASTORE, MYSQL, NDBINFO, PERFORMANCE_SCHEMA, SQOOP, SYS, BIGINT, BINARY, BOOLEAN, BOTH, BY, CASE, CAST, CHAR, COLUMN, CONF, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE, CURRENT_TIMESTAMP, CURSOR, DATABASE, DATE, DECIMAL, DELETE, DESCRIBE, DISTINCT, DOUBLE, DROP, ELSE, END, EXCHANGE, EXISTS, EXTENDED, EXTERNAL, FALSE, FETCH, FLOAT, FOLLOWING, FOR, FROM, FULL, FUNCTION, GRANT, GROUP, GROUPING, HAVING, IF, IMPORT, IN, INNER, INSERT, INT, INTERSECT, INTERVAL, INTO, IS, JOIN, LATERAL, LEFT, LESS, LIKE, LOCAL, MACRO, MAP, MORE, NONE, NOT, NULL, OF, ON, OR, ORDER, OUT, OUTER, OVER, PARTIALSCAN, PARTITION, PERCENT, PRECEDING, PRESERVE, PROCEDURE, RANGE, READS, REDUCE, REVOKE, RIGHT, ROLLUP, ROW, ROWS, SELECT, SET, SMALLINT, TABLE, TABLESAMPLE, THEN, TIMESTAMP, TO, TRANSFORM, TRIGGER, TRUE, TRUNCATE, UNBOUNDED, UNION, UNIQUEJOIN, UPDATE, USER, USING, UTC_TMESTAMP, VALUES, VARCHAR, WHEN, WHERE, WINDOW, WITH, COMMIT, ONLY, REGEXP, RLIKE, ROLLBACK, START, CACHE, CONSTRAINT, FOREIGN, PRIMARY, REFERENCES, DAYOFWEEK, EXTRACT, FLOOR, INTEGER, PRECISION, VIEWS, TIME, NUMERIC, SYNC, BASE, PYTHON37, FILEBEAT. And any word containing _FEATURESTORE.","title":"Reserved project names"},{"location":"user_guides/projects/project/create_project/#conclusion","text":"In this guide you learned how to create a project.","title":"Conclusion"},{"location":"user_guides/projects/python/python_env_export/","text":"How To Export Python Environment # Introduction # The python environment in a project can be exported to an environment.yml file. It can be useful to export it and then recreate it outside of Hopsworks, or just have a snapshot of all the installed libraries and their versions. In this guide, you will learn how to export the python environment for a project. Step 1: Go to environment # Under the Project settings section you can find the Python libraries setting. Step 2: Click Export env # An existing Anaconda environment can be exported as a yml file, clicking the Export env will download the environment.yml file in your browser. Export environment Conclusion # In this guide you learned how to export your python environment.","title":"Export environment"},{"location":"user_guides/projects/python/python_env_export/#how-to-export-python-environment","text":"","title":"How To Export Python Environment"},{"location":"user_guides/projects/python/python_env_export/#introduction","text":"The python environment in a project can be exported to an environment.yml file. It can be useful to export it and then recreate it outside of Hopsworks, or just have a snapshot of all the installed libraries and their versions. In this guide, you will learn how to export the python environment for a project.","title":"Introduction"},{"location":"user_guides/projects/python/python_env_export/#step-1-go-to-environment","text":"Under the Project settings section you can find the Python libraries setting.","title":"Step 1: Go to environment"},{"location":"user_guides/projects/python/python_env_export/#step-2-click-export-env","text":"An existing Anaconda environment can be exported as a yml file, clicking the Export env will download the environment.yml file in your browser. Export environment","title":"Step 2: Click Export env"},{"location":"user_guides/projects/python/python_env_export/#conclusion","text":"In this guide you learned how to export your python environment.","title":"Conclusion"},{"location":"user_guides/projects/python/python_env_recreate/","text":"How To Recreate Python Environment # Introduction # Sometimes it may be desirable to recreate the python environment to start from the same state the python environment was created with. In this guide, you will learn how to recreate the python environment. Keep in mind There may be Jobs or Jupyter notebooks that depend on additional libraries that have been installed. It is recommended to first export the environment to save a snapshot of all libraries currently installed and their versions. Step 1: Remove the environment # Under the Project settings section you can find the Python libraries setting. First click Remove env . Remove environment Step 2: Create new environment # After removing the environment, simply recreate it by clicking Create Environment . Create environment Conclusion # In this guide you learned how to recreate your python environment.","title":"Recreate environment"},{"location":"user_guides/projects/python/python_env_recreate/#how-to-recreate-python-environment","text":"","title":"How To Recreate Python Environment"},{"location":"user_guides/projects/python/python_env_recreate/#introduction","text":"Sometimes it may be desirable to recreate the python environment to start from the same state the python environment was created with. In this guide, you will learn how to recreate the python environment. Keep in mind There may be Jobs or Jupyter notebooks that depend on additional libraries that have been installed. It is recommended to first export the environment to save a snapshot of all libraries currently installed and their versions.","title":"Introduction"},{"location":"user_guides/projects/python/python_env_recreate/#step-1-remove-the-environment","text":"Under the Project settings section you can find the Python libraries setting. First click Remove env . Remove environment","title":"Step 1: Remove the environment"},{"location":"user_guides/projects/python/python_env_recreate/#step-2-create-new-environment","text":"After removing the environment, simply recreate it by clicking Create Environment . Create environment","title":"Step 2: Create new environment"},{"location":"user_guides/projects/python/python_env_recreate/#conclusion","text":"In this guide you learned how to recreate your python environment.","title":"Conclusion"},{"location":"user_guides/projects/python/python_git/","text":"How To Install Python Libraries # Introduction # The prepackaged python environment in Hopsworks contains a large number of libraries for data engineering, machine learning and more general data science development. But in some cases users want to install additional packages for their applications. In this guide, you will learn how to install Python packages using these different options. PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip Under the Project settings section you can find the Python libraries setting. Step 1 (optional): Name and version # Enter the name and, optionally, the desired version to install. Installing library by name and version Step 2 (optional): Search # Enter the search term and select a library and version to install. Installing library using search Step 3 (optional): Distribution (.whl, .egg..) # Install a python package by uploading the corresponding package file and selecting it in the file browser. Installing library from file Step 4 (optional): Git hosted # To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo Conclusion # In this guide you learned how to install python libraries. Now you can use the library in a Jupyter notebook or a Job .","title":"How To Install Python Libraries"},{"location":"user_guides/projects/python/python_git/#how-to-install-python-libraries","text":"","title":"How To Install Python Libraries"},{"location":"user_guides/projects/python/python_git/#introduction","text":"The prepackaged python environment in Hopsworks contains a large number of libraries for data engineering, machine learning and more general data science development. But in some cases users want to install additional packages for their applications. In this guide, you will learn how to install Python packages using these different options. PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip Under the Project settings section you can find the Python libraries setting.","title":"Introduction"},{"location":"user_guides/projects/python/python_git/#step-1-optional-name-and-version","text":"Enter the name and, optionally, the desired version to install. Installing library by name and version","title":"Step 1 (optional): Name and version"},{"location":"user_guides/projects/python/python_git/#step-2-optional-search","text":"Enter the search term and select a library and version to install. Installing library using search","title":"Step 2 (optional): Search"},{"location":"user_guides/projects/python/python_git/#step-3-optional-distribution-whl-egg","text":"Install a python package by uploading the corresponding package file and selecting it in the file browser. Installing library from file","title":"Step 3 (optional): Distribution (.whl, .egg..)"},{"location":"user_guides/projects/python/python_git/#step-4-optional-git-hosted","text":"To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo","title":"Step 4 (optional): Git hosted"},{"location":"user_guides/projects/python/python_git/#conclusion","text":"In this guide you learned how to install python libraries. Now you can use the library in a Jupyter notebook or a Job .","title":"Conclusion"},{"location":"user_guides/projects/python/python_install/","text":"How To Install Python Libraries # Introduction # The prepackaged python environment in Hopsworks contains a large number of libraries for data engineering, machine learning and more general data science development. But in some cases users want to install additional packages for their applications. In this guide, you will learn how to install Python packages using these different options. PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip Under the Project settings section you can find the Python libraries setting. Step 1 (optional): Name and version # Enter the name and, optionally, the desired version to install. Installing library by name and version Step 2 (optional): Search # Enter the search term and select a library and version to install. Installing library using search Step 3 (optional): Distribution (.whl, .egg..) # Install a python package by uploading the corresponding package file and selecting it in the file browser. Installing library from file Step 4 (optional): Git hosted # To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo Conclusion # In this guide you learned how to install python libraries. Now you can use the library in a Jupyter notebook or a Job .","title":"distribution"},{"location":"user_guides/projects/python/python_install/#how-to-install-python-libraries","text":"","title":"How To Install Python Libraries"},{"location":"user_guides/projects/python/python_install/#introduction","text":"The prepackaged python environment in Hopsworks contains a large number of libraries for data engineering, machine learning and more general data science development. But in some cases users want to install additional packages for their applications. In this guide, you will learn how to install Python packages using these different options. PyPi, using pip package manager A conda channel, using conda package manager Packages saved in certain file formats, currently we support .whl or .egg A public or private git repository A requirements.txt file to install multiple libraries at the same time using pip An environment.yml file to install multiple libraries at the same time using conda and pip Under the Project settings section you can find the Python libraries setting.","title":"Introduction"},{"location":"user_guides/projects/python/python_install/#step-1-optional-name-and-version","text":"Enter the name and, optionally, the desired version to install. Installing library by name and version","title":"Step 1 (optional): Name and version"},{"location":"user_guides/projects/python/python_install/#step-2-optional-search","text":"Enter the search term and select a library and version to install. Installing library using search","title":"Step 2 (optional): Search"},{"location":"user_guides/projects/python/python_install/#step-3-optional-distribution-whl-egg","text":"Install a python package by uploading the corresponding package file and selecting it in the file browser. Installing library from file","title":"Step 3 (optional): Distribution (.whl, .egg..)"},{"location":"user_guides/projects/python/python_install/#step-4-optional-git-hosted","text":"To install from a git repository simply provide the repository URL. The URL you should provide is the same as you would enter on the command line using pip install git+{repo_url} . In the case of a private git repository, also select whether it is a GitHub or GitLab repository and the preconfigured access token for the repository. Note : If you are installing from a git repository which is not GitHub or GitLab simply supply the access token in the URL. Keep in mind that in this case the access token may be visible in logs for other users in the same project to see. Installing library from git repo","title":"Step 4 (optional): Git hosted"},{"location":"user_guides/projects/python/python_install/#conclusion","text":"In this guide you learned how to install python libraries. Now you can use the library in a Jupyter notebook or a Job .","title":"Conclusion"},{"location":"user_guides/projects/secrets/create_secret/","text":"How To Create A Secret # Introduction # A Secret is a key-value pair used to store encrypted information accessible only to the owner of the secret. Also if you wish to, you can share the same secret API key with all the members of a Project. UI # Step 1: Navigate to Secrets # In the Account Settings page you can find the Secrets section showing a list of all secrets. List of secrets Step 2: Create a Secret # Click New Secret to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value. If the secret should be private to this user, select Private , to share the secret with all members of a project select Project and enter the project name. Create new secret dialog Step 3: Secret created # Click New Secret to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value. If the secret should be private to this user, select Private , to share the secret with all members of a project select Project and enter the project name. Secret is now created Code # Step 1: Get secrets API # import hopsworks connection = hopsworks . connection () secrets_api = connection . get_secrets_api () Step 2: Create secret # secret = secrets_api . create_secret ( \"my_secret\" , \"Fk3MoPlQXCQvPo\" ) API Reference # Secrets Conclusion # In this guide you learned how to create a secret.","title":"Create Secret"},{"location":"user_guides/projects/secrets/create_secret/#how-to-create-a-secret","text":"","title":"How To Create A Secret"},{"location":"user_guides/projects/secrets/create_secret/#introduction","text":"A Secret is a key-value pair used to store encrypted information accessible only to the owner of the secret. Also if you wish to, you can share the same secret API key with all the members of a Project.","title":"Introduction"},{"location":"user_guides/projects/secrets/create_secret/#ui","text":"","title":"UI"},{"location":"user_guides/projects/secrets/create_secret/#step-1-navigate-to-secrets","text":"In the Account Settings page you can find the Secrets section showing a list of all secrets. List of secrets","title":"Step 1: Navigate to Secrets"},{"location":"user_guides/projects/secrets/create_secret/#step-2-create-a-secret","text":"Click New Secret to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value. If the secret should be private to this user, select Private , to share the secret with all members of a project select Project and enter the project name. Create new secret dialog","title":"Step 2: Create a Secret"},{"location":"user_guides/projects/secrets/create_secret/#step-3-secret-created","text":"Click New Secret to bring up the dialog for secret creation. Enter a name for the secret to be used for lookup, and the secret value. If the secret should be private to this user, select Private , to share the secret with all members of a project select Project and enter the project name. Secret is now created","title":"Step 3: Secret created"},{"location":"user_guides/projects/secrets/create_secret/#code","text":"","title":"Code"},{"location":"user_guides/projects/secrets/create_secret/#step-1-get-secrets-api","text":"import hopsworks connection = hopsworks . connection () secrets_api = connection . get_secrets_api ()","title":"Step 1: Get secrets API"},{"location":"user_guides/projects/secrets/create_secret/#step-2-create-secret","text":"secret = secrets_api . create_secret ( \"my_secret\" , \"Fk3MoPlQXCQvPo\" )","title":"Step 2: Create secret"},{"location":"user_guides/projects/secrets/create_secret/#api-reference","text":"Secrets","title":"API Reference"},{"location":"user_guides/projects/secrets/create_secret/#conclusion","text":"In this guide you learned how to create a secret.","title":"Conclusion"}]}